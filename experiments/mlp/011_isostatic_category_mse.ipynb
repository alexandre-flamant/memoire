{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638db9e71feb8f9a",
   "metadata": {},
   "source": [
    "# Imports, config and logging server"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "# Standard library\n",
    "import ast\n",
    "from copy import deepcopy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import mlflow\n",
    "from mlflow.pytorch import load_model\n",
    "\n",
    "import torchmetrics.functional.regression as R\n",
    "\n",
    "# Project-specific imports\n",
    "from default import *\n",
    "from config import DIRECTORY, PORT\n",
    "from tools import MLFlowSession\n",
    "from models.architecture import MLP\n",
    "from models.processing import StandardScaler\n",
    "from dataset import FixedPrattTrussDatasetThreeTargets\n",
    "\n",
    "# Set random state and device\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "# Set working directory and figure directory\n",
    "%cd -q {PROJECT_HOME}\n",
    "figure_dir = os.path.join(os.getcwd(), '022025_experiment/figures')\n",
    "\n",
    "# Configure MLFlow server\n",
    "DIRECTORY = DIRECTORY['011']\n",
    "PORT = PORT['011']\n",
    "server = MLFlowSession(PORT=PORT, DIRECTORY=DIRECTORY)\n",
    "server.start()\n",
    "\n",
    "# Set dataset dirs\n",
    "bisupported = False\n",
    "n_input_dims = 64 if bisupported else 65\n",
    "n_output_dims = 3\n",
    "\n",
    "loss = 'mse'\n",
    "\n",
    "train_dataset_dir = r\"data/dataset/pratt_truss_bridge_isostatic/1_category_ea\"\n",
    "test_dataset_dir = r\"data/dataset/pratt_truss_bridge_isostatic/test\"\n",
    "\n",
    "_validation_ds = FixedPrattTrussDatasetThreeTargets(f\"{test_dataset_dir}/1_category_8192.hdf5\")"
   ],
   "id": "66f2a04d3129a7d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training procedure",
   "id": "b94b4dd275d98c63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_step(model, batch, input_scaler, target_scaler, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    input, target, u, q, w, r = batch\n",
    "    input, target = input.to(device), target.to(device)\n",
    "\n",
    "    z_input = input_scaler.transform(input)\n",
    "    z_target = target_scaler.transform(target)\n",
    "\n",
    "    z_target_pred = model(z_input)\n",
    "    target_pred = target_scaler.inverse_transform(z_target_pred)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(z_target_pred, z_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    metrics = compute_metrics(model, target_pred, z_target_pred, target, z_target)\n",
    "    metrics['loss'] = loss.item()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def validation(model, batch, input_scaler, target_scaler, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input, target, u, q, w, r = batch\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        z_input = input_scaler.transform(input)\n",
    "        z_target = target_scaler.transform(target)\n",
    "\n",
    "        z_target_pred = model(z_input)\n",
    "        target_pred = target_scaler.inverse_transform(z_target_pred)\n",
    "\n",
    "        loss = criterion(z_target_pred, z_target)\n",
    "\n",
    "    metrics = compute_metrics(model, target_pred, z_target_pred, target, z_target)\n",
    "    metrics['loss'] = loss.item()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(model, target_pred, z_target_pred, target, z_target):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        r2 = R.r2_score(z_target_pred, z_target)\n",
    "        if r2.isinf():  # Switch to 64 bits in case of overflow\n",
    "            r2 = R.r2_score(target_pred.cpu().to(torch.float64), target.cpu().to(torch.float64))\n",
    "        mape = R.mean_absolute_percentage_error(target_pred, target)\n",
    "        rmse = R.mean_squared_error(target_pred * 1e-6, target * 1e-6, squared=False)\n",
    "        if rmse.isinf():  # Switch to 64 bits in case of overflow\n",
    "            rmse = R.mean_squared_error(target_pred.cpu().to(torch.float64), target.cpu().to(torch.float64),\n",
    "                                        squared=False)\n",
    "\n",
    "    return {'r2': r2, 'mape': mape, 'rmse_MN': rmse}\n",
    "\n",
    "\n",
    "def log_epoch(train_metrics, val_metrics, epoch):\n",
    "    metrics = dict()\n",
    "    metrics.update({f'train_{k}': v for k, v in train_metrics.items()})\n",
    "    metrics.update({f'val_{k}': v for k, v in val_metrics.items()})\n",
    "\n",
    "    mlflow.log_metrics(metrics, step=epoch)\n",
    "\n",
    "\n",
    "def log_model(name, model, signature, metadata=None):\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model,\n",
    "        artifact_path=name,\n",
    "        signature=signature,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "\n",
    "def console_log(epoch, train_metrics, val_metrics):\n",
    "    print(f\">> Epoch {epoch + 1:4d}\", end='  ')\n",
    "    print(f\"TRAIN\", end='   ')\n",
    "    metric_names = {k for k in train_metrics.keys() if k != 'loss'}\n",
    "    metric_names = ['loss'] + sorted(metric_names)\n",
    "    for k in metric_names:\n",
    "        v = train_metrics[k]\n",
    "        print(f\"{k}: {v: 1.4f}\", end='   ')\n",
    "\n",
    "    print(\"  ||  \", end='')\n",
    "    print(f\"VALIDATION\", end='   ')\n",
    "    metric_names = {k for k in val_metrics.keys() if k != 'loss'}\n",
    "    metric_names = ['loss'] + sorted(metric_names)\n",
    "    for k in metric_names:\n",
    "        v = val_metrics[k]\n",
    "        print(f\"{k}: {v: 1.4f}\", end='   ')\n",
    "    print()\n",
    "\n",
    "\n",
    "def train(model, train_dataset, val_dataset, train_batch_size, val_batch_size, n_epoch, optimizer, criterion,\n",
    "          log_step=10):\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    train_dl = DataLoader(train_dataset, train_batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_dataset, val_batch_size, shuffle=False)\n",
    "\n",
    "    input_scaler = StandardScaler(train_dataset[0][0].__len__()).to(device)\n",
    "    target_scaler = StandardScaler(train_dataset[0][1].__len__()).to(device)\n",
    "\n",
    "    # Train the scaler\n",
    "    input, target = None, None\n",
    "    for batch in train_dl:\n",
    "        input, target, u, q, w, r = batch\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        input_scaler.partial_fit(input)\n",
    "        target_scaler.partial_fit(target)\n",
    "\n",
    "    signature = mlflow.models.infer_signature(input.cpu().detach().numpy(),\n",
    "                                              input_scaler.transform(input).cpu().detach().numpy())\n",
    "    log_model('input_scaler', input_scaler, signature=signature)\n",
    "\n",
    "    signature = mlflow.models.infer_signature(target.cpu().detach().numpy(),\n",
    "                                              target_scaler.transform(target).cpu().detach().numpy())\n",
    "    log_model('target_scaler', target_scaler, signature=signature)\n",
    "\n",
    "    best_val_metric = {\n",
    "        'mape': {model: None, 'value': np.inf, 'epoch': -1},\n",
    "        'rmse_MN': {model: None, 'value': np.inf, 'epoch': -1},\n",
    "        'loss': {model: None, 'value': np.inf, 'epoch': -1},\n",
    "        'r2': {model: None, 'value': -np.inf, 'epoch': -1}\n",
    "    }\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        train_metrics = {}\n",
    "        val_metrics = {}\n",
    "        for batch in train_dl:\n",
    "            train_metrics_epoch = train_step(model, batch, input_scaler, target_scaler, optimizer, criterion)\n",
    "            for k, m in train_metrics_epoch.items():\n",
    "                if k not in train_metrics: train_metrics[k] = []\n",
    "                train_metrics[k].append(m)\n",
    "\n",
    "        for batch in val_dl:\n",
    "            val_metrics_epoch = validation(model, batch, input_scaler, target_scaler, criterion)\n",
    "            for k, m in val_metrics_epoch.items():\n",
    "                if k not in val_metrics: val_metrics[k] = []\n",
    "                val_metrics[k].append(m)\n",
    "\n",
    "        # Compute the mean on GPU  -> Faster for batch\n",
    "        train_metrics = {name: torch.tensor(metrics, device=device, dtype=torch.float32).mean() for name, metrics in\n",
    "                         train_metrics.items()}\n",
    "        val_metrics = {name: torch.tensor(metrics, device=device, dtype=torch.float32).mean() for name, metrics in\n",
    "                       val_metrics.items()}\n",
    "\n",
    "        log_epoch(train_metrics, val_metrics, epoch + 1)\n",
    "\n",
    "        negative_metrics = {'r2'}  # Set of metrics which are better when higher\n",
    "\n",
    "        for k, v in val_metrics.items():\n",
    "            f = 1 if k not in negative_metrics else -1\n",
    "            if f * best_val_metric[k]['value'] >= f * v:\n",
    "                best_val_metric[k] = {'model': deepcopy(model), 'value': v.item(), 'epoch': epoch + 1}\n",
    "\n",
    "        if (log_step < 0): continue\n",
    "        if (epoch % log_step == 0):\n",
    "            console_log(epoch + 1, train_metrics, val_metrics)\n",
    "\n",
    "    input, target, u, q, w, r = train_dl.__iter__().__next__()\n",
    "    signature = mlflow.models.infer_signature(input.cpu().detach().numpy(), target.cpu().detach().numpy())\n",
    "    for k, v in best_val_metric.items():\n",
    "        metric = v['value']\n",
    "        epoch = v['epoch']\n",
    "        log_model(f\"{k}_model\", v['model'], signature=signature, metadata={'metric': metric, 'epoch': epoch + 1})\n",
    "\n",
    "\n",
    "def trial_routine(run_name, train_routine, mlp_params, train_dataset, val_dataset, train_batch_size,\n",
    "                  val_batch_size, n_epoch, optimizer, lr, criterion, fold, log_step=10, log_params=None):\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Model initialization\n",
    "        model = MLP(**mlp_params)\n",
    "\n",
    "        # Model information logging\n",
    "        capacity = sum(np.prod(p.size()) for p in filter(lambda p: p.requires_grad, model.parameters()))\n",
    "        if log_params is not None:\n",
    "            mlflow.log_params(log_params)\n",
    "        mlflow.log_params(mlp_params)\n",
    "        mlflow.log_params({\n",
    "            'n_hidden_layers': len(mlp_params['hidden_dims']),\n",
    "            'hidden_layers_size': mlp_params['hidden_dims'][0],\n",
    "            'capacity': capacity,\n",
    "            'n_epoch': n_epoch,\n",
    "            'val_batch_size': val_batch_size,\n",
    "            'train_batch_size': train_batch_size,\n",
    "            'train_size': train_dataset.__len__(),\n",
    "            'val_size': val_dataset.__len__(),\n",
    "            'k-fold': fold,\n",
    "            'optimizer': optimizer.__name__,\n",
    "            'learning_rate': f\"{lr:.1e}\",\n",
    "            'criterion': criterion.__name__,\n",
    "        })\n",
    "\n",
    "        # Run the training with the configuration\n",
    "        train_routine(model, train_dataset, val_dataset,\n",
    "                      train_batch_size, val_batch_size,\n",
    "                      n_epoch, optimizer(model.parameters(), lr=lr), criterion(), log_step=log_step)"
   ],
   "id": "8d7d17538f1a25e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot(results, configs, metrics, labels, logs=None, xlim=None, ylim=None, figsize=(10, 10)):\n",
    "    if not isinstance(metrics, list):\n",
    "        metrics = [metrics]\n",
    "    if logs is None:\n",
    "        logs = [True] * len(metrics)\n",
    "    elif not isinstance(logs, list):\n",
    "        logs = [logs] * len(metrics)\n",
    "\n",
    "    cmap = mpl.colormaps['tab10']\n",
    "    c = cmap(np.linspace(0, 1, len(configs)))\n",
    "\n",
    "    if len(metrics) == 1:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figsize)\n",
    "        axs = [ax]\n",
    "    else:\n",
    "        fig, axs = plt.subplots(nrows=int(np.ceil(len(metrics) / 2)), ncols=2, figsize=figsize)\n",
    "        axs = [ax for row in axs for ax in row]\n",
    "\n",
    "    for i, (metric, log) in enumerate(zip(metrics, logs)):\n",
    "        ax = axs[i]\n",
    "        # Plot the data but collect the handles for the legend\n",
    "        config_handles = []\n",
    "        for i, conf in enumerate(configs):\n",
    "            # Store only the validation line handles for the legend\n",
    "            l = len(results[conf][f'train_{metric}'])\n",
    "            h = ax.plot(np.arange(l), results[conf][f'train_{metric}'], alpha=.5, ls='--', lw=1, c=c[i])[0]\n",
    "            h2 = ax.plot(np.arange(l), results[conf][f'val_{metric}'], ls='-', lw=1, c=c[i])[0]\n",
    "            config_handles.append(h2)\n",
    "\n",
    "        # Create custom handles for the line style legend\n",
    "        line_style_handles = [\n",
    "            Line2D([0], [0], color='black', lw=1, ls='--', alpha=0.5, label='Training'),\n",
    "            Line2D([0], [0], color='black', lw=1, ls='-', label='Validation'),\n",
    "            Line2D([0], [0], color='black', alpha=0, lw=1, ls='-')\n",
    "        ]\n",
    "\n",
    "        # Get the current position and size of the axis\n",
    "        box = ax.get_position()\n",
    "        # Reduce the width of the axis to make room for the legend\n",
    "        ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "        # Combine both sets of handles and labels\n",
    "        all_handles = line_style_handles + config_handles\n",
    "        all_labels = ['Training', 'Validation', ''] + labels\n",
    "\n",
    "        # Create a single legend with both line styles and configurations\n",
    "        plt.figlegend(all_handles, all_labels, loc='center left', bbox_to_anchor=(1, .5),\n",
    "                      title=\"Legend\")\n",
    "\n",
    "        ax.set_title(f\"Evolution of {metric} during training\")\n",
    "        if log:\n",
    "            ax.set_yscale('log')\n",
    "        ax.set_ylabel(f\"{metric} [/]\")\n",
    "        ax.set_xlabel(\"epoch [/]\")\n",
    "\n",
    "        if xlim: ax.set_xlim(*xlim)\n",
    "        if ylim: ax.set_ylim(*ylim)\n",
    "\n",
    "    fig.tight_layout(h_pad=2, w_pad=7.5)\n",
    "    # Adjust right padding to make room for the legend\n",
    "    plt.show()"
   ],
   "id": "15464ff8ac549310"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyper-parameter tuning",
   "id": "d822d0bdf811fd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "TRAIN_BATCH_SIZE = 512\n",
    "VAL_BATCH_SIZE = 512\n",
    "kfold = 10\n",
    "\n",
    "N_EPOCH = 200\n",
    "LR = 5e-4\n",
    "\n",
    "log_step = -1\n",
    "\n",
    "ds = FixedPrattTrussDatasetThreeTargets(f\"{train_dataset_dir}/train_4096.hdf5\")\n",
    "mlflow.set_tracking_uri(uri=server.url())"
   ],
   "id": "9ea7feb2909af47a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model capacity",
   "id": "f68c8c2c6d8364d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "mlflow.set_experiment(\"capacity\")\n",
    "\n",
    "n_layers_values = [2, 3, 4, 5]\n",
    "n_neurons_values = [65, 70, 80, 100, 120]\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = []\n",
    "    for n_layers in n_layers_values:\n",
    "        for n_neurons in n_neurons_values:\n",
    "            for fold, (train_idx, val_idx) in enumerate(KFold(n_splits=kfold, shuffle=True).split(ds)):\n",
    "                hidden_dims = [n_neurons for _ in range(n_layers)]\n",
    "                train_dataset, validation_dataset = Subset(ds, train_idx), Subset(ds, val_idx)\n",
    "                future = executor.submit(trial_routine, None, train,\n",
    "                                         {\n",
    "                                             'input_dim': n_input_dims,\n",
    "                                             'hidden_dims': hidden_dims,\n",
    "                                             'output_dim': n_output_dims,\n",
    "                                             'activation': \"relu\",\n",
    "                                             'activation_params': None,\n",
    "                                             'dropout': 0.0,\n",
    "                                             'batch_norm': False,\n",
    "                                             'layer_norm': False,\n",
    "                                             'normalization_params': None,\n",
    "                                             'output_activation': None,\n",
    "                                             'output_activation_params': None,\n",
    "                                         },\n",
    "                                         train_dataset, validation_dataset, TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
    "                                         N_EPOCH,\n",
    "                                         torch.optim.Adam, LR, nn.MSELoss, kfold, log_step, {'loss': loss})\n",
    "                futures.append(future)\n",
    "\n",
    "    # Ensure all processes complete execution\n",
    "    for future in futures:\n",
    "        future.result()"
   ],
   "id": "b26bb4f73fd71c97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experiment = mlflow.get_experiment_by_name(\"capacity\")\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "cols = [c for c in runs_df if c.startswith('metrics.')]\n",
    "metrics_names = [col[col.index('.') + 1:] for col in cols]\n",
    "\n",
    "layers_combinations = set()\n",
    "for n_layers in runs_df['params.n_hidden_layers'].unique():\n",
    "    for layer_size in runs_df['params.hidden_layers_size'].unique():\n",
    "        layers_combinations.add((n_layers, layer_size))\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "results = {k1: {k2: [] for k2 in metrics_names} for k1 in layers_combinations}\n",
    "for n, size in layers_combinations:\n",
    "    run_ids = runs_df[\n",
    "        (runs_df['params.loss'] == loss) &\n",
    "        (runs_df['params.n_hidden_layers'] == n) &\n",
    "        (runs_df['params.hidden_layers_size'] == size)\n",
    "        ]['run_id']\n",
    "    for run_id in run_ids:\n",
    "        for metric_name in metrics_names:\n",
    "            results[(n, size)][metric_name].append(\n",
    "                [metric.value for metric in client.get_metric_history(run_id, metric_name)])\n",
    "\n",
    "for k in results.keys():\n",
    "    for metric in results[k].keys():\n",
    "        results[k][metric] = np.vstack(results[k][metric]).mean(axis=0)"
   ],
   "id": "609d322f3cd69353"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Your existing code for preparing the data\n",
    "configs_by_val_mape = sorted([(np.min(v['val_mape']), k) for k, v in results.items()])\n",
    "configs = configs_by_val_mape\n",
    "\n",
    "configs = sorted(configs)\n",
    "configs = [x[1] for x in configs]\n",
    "\n",
    "labels = [str([int(x) for x in cfg]) for cfg in configs]\n",
    "\n",
    "plot(results, configs, ['loss', 'mape', 'r2', 'rmse_MN'], labels, logs=[True, True, False, True])"
   ],
   "id": "f4cb4f1604bacc66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pareto front technique\n",
    "best_mape = configs_by_val_mape[0][0]\n",
    "candidates = [x[1] for x in filter(lambda x: x[0] <= best_mape * 1.02, configs_by_val_mape)]\n",
    "\n",
    "filtered_results = {k: results[k] for k in candidates}\n",
    "\n",
    "configs_by_val_rmse = sorted([(np.min(v['val_rmse_MN']), k) for k, v in filtered_results.items()])\n",
    "best_candidate = [int(x) for x in configs_by_val_rmse[0][1]]\n",
    "best_hidden_dims = [best_candidate[1]] * best_candidate[0]\n",
    "\n",
    "print(f\"Best candidate is: {best_candidate} according to the pareto front.\")"
   ],
   "id": "1ad10330c3d3ddfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "to_delete = [\n",
    "    'n_layers_values', 'n_neurons_values', 'executor', 'futures', 'n_layers', 'n_neurons',\n",
    "    'fold', 'train_idx', 'val_idx', 'hidden_dims', 'train_dataset', 'validation_dataset',\n",
    "    'future', 'experiment', 'runs_df', 'cols', 'metrics_names', 'layers_combinations',\n",
    "    'client', 'results', 'n', 'size', 'run_ids', 'run_id', 'metric_name', 'k',\n",
    "    'configs_by_val_mape', 'configs', 'labels', 'best_mape', 'candidates',\n",
    "    'filtered_results', 'configs_by_val_rmse', 'best_candidate'\n",
    "]\n",
    "\n",
    "for var in to_delete:\n",
    "    try:\n",
    "        globals().pop(var, None)\n",
    "    except KeyError:\n",
    "        continue\n"
   ],
   "id": "594d2c55d3ec0692"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Activation function",
   "id": "8c2815b36c683b5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "mlflow.set_experiment(\"activation_function\")\n",
    "\n",
    "activation_values = ['relu', 'gelu', 'tanh', 'sigmoid',\n",
    "                     'leaky_relu', 'leaky_relu', 'leaky_relu', 'leaky_relu']\n",
    "activation_params_values = [None, None, None, None,\n",
    "                            {'negative_slope': 5e-1}, {'negative_slope': 1e-1}, {'negative_slope': 5e-2},\n",
    "                            {'negative_slope': 1e-2}]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = []\n",
    "    for activation, activation_params in zip(activation_values, activation_params_values):\n",
    "        for fold, (train_idx, val_idx) in enumerate(KFold(n_splits=kfold, shuffle=True).split(ds)):\n",
    "            train_dataset, validation_dataset = Subset(ds, train_idx), Subset(ds, val_idx)\n",
    "            future = executor.submit(trial_routine, None, train,\n",
    "                                     {\n",
    "                                         'input_dim': n_input_dims,\n",
    "                                         'hidden_dims': best_hidden_dims,\n",
    "                                         'output_dim': n_output_dims,\n",
    "                                         'activation': activation,\n",
    "                                         'activation_params': activation_params,\n",
    "                                         'dropout': 0.0,\n",
    "                                         'batch_norm': False,\n",
    "                                         'layer_norm': False,\n",
    "                                         'normalization_params': None,\n",
    "                                         'output_activation': None,\n",
    "                                         'output_activation_params': None,\n",
    "                                     },\n",
    "                                     train_dataset, validation_dataset, TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
    "                                     N_EPOCH,\n",
    "                                     torch.optim.Adam, LR, nn.MSELoss, kfold, log_step, {'loss': loss})\n",
    "            futures.append(future)\n",
    "\n",
    "    # Ensure all processes complete execution\n",
    "    for future in futures:\n",
    "        future.result()"
   ],
   "id": "905bdbf759853840"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experiment = mlflow.get_experiment_by_name(\"activation_function\")\n",
    "\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "cols = [c for c in runs_df if c.startswith('metrics.')]\n",
    "metrics_names = [col[col.index('.') + 1:] for col in cols]\n",
    "\n",
    "activations_combinations = set()\n",
    "for m in runs_df['params.activation'].unique():\n",
    "    for n in runs_df[runs_df['params.activation'] == m]['params.activation_params'].unique():\n",
    "        activations_combinations.add((m, n))\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "results = {k1: {k2: [] for k2 in metrics_names} for k1 in activations_combinations}\n",
    "for k in activations_combinations:\n",
    "    (act, params) = k\n",
    "    run_ids = runs_df[\n",
    "        (runs_df['params.loss'] == loss) &\n",
    "        (runs_df['params.activation'] == act) &\n",
    "        (runs_df['params.activation_params'] == params)\n",
    "        ]['run_id']\n",
    "    for run_id in run_ids:\n",
    "        for metric_name in metrics_names:\n",
    "            results[k][metric_name].append([m.value for m in client.get_metric_history(run_id, metric_name)])\n",
    "\n",
    "for k in results.keys():\n",
    "    for metric in results[k].keys():\n",
    "        results[k][metric] = np.vstack(results[k][metric]).mean(axis=0)"
   ],
   "id": "2b3ba45b5632209f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "configs_by_val_mape = sorted([(np.min(v['val_mape']), k) for k, v in results.items()])\n",
    "\n",
    "configs = [x[1] for x in configs_by_val_mape]\n",
    "\n",
    "labels = [f\"[{cfg[0]}]\" if cfg[1] == 'None' else f\"[{cfg[0]}, {cfg[1]}]\" for cfg in configs]\n",
    "\n",
    "plot(results, configs, ['loss', 'mape', 'r2', 'rmse_MN'], labels, logs=[True, True, False, True])"
   ],
   "id": "c3623f3f0c491f01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pareto front technique\n",
    "best_mape = configs_by_val_mape[0][0]\n",
    "candidates = [x[1] for x in filter(lambda x: x[0] <= best_mape * 1.02, configs_by_val_mape)]\n",
    "\n",
    "filtered_results = {k: results[k] for k in candidates}\n",
    "\n",
    "configs_by_val_rmse = sorted([(np.min(v['val_rmse_MN']), k) for k, v in filtered_results.items()])\n",
    "\n",
    "best_candidate = configs_by_val_rmse[0][1]\n",
    "best_activation = best_candidate[0]\n",
    "best_activation_params = ast.literal_eval(best_candidate[1])\n",
    "\n",
    "print(f\"Best candidate is: {best_candidate} according to the pareto front.\")"
   ],
   "id": "83b91a7a74a4f55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "to_delete = [\n",
    "    'activation_values', 'activation_params_values', 'executor', 'futures',\n",
    "    'activation', 'activation_params', 'fold', 'train_idx', 'val_idx',\n",
    "    'train_dataset', 'validation_dataset', 'future', 'experiment', 'runs_df',\n",
    "    'cols', 'metrics_names', 'activations_combinations', 'client', 'results',\n",
    "    'k', 'act', 'params', 'run_ids', 'run_id', 'metric_name', 'configs_by_val_mape',\n",
    "    'configs', 'labels', 'best_mape', 'candidates', 'filtered_results',\n",
    "    'configs_by_val_rmse', 'best_candidate'\n",
    "]\n",
    "\n",
    "for var in to_delete:\n",
    "    try:\n",
    "        del globals()[var]\n",
    "    except KeyError:\n",
    "        pass"
   ],
   "id": "1ed34ffac908592d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Learning rate",
   "id": "759058df10cdae53"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "%%capture\n",
    "mlflow.set_experiment(\"learning_rate\")\n",
    "\n",
    "lr_values = sorted(np.hstack([f * np.logspace(-4, -3, 2) for f in [1, 2.5, 5, 7.5]]))\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = []\n",
    "    for LR in lr_values:\n",
    "        for fold, (train_idx, val_idx) in enumerate(KFold(n_splits=kfold, shuffle=True).split(ds)):\n",
    "            train_dataset, validation_dataset = Subset(ds, train_idx), Subset(ds, val_idx)\n",
    "            future = executor.submit(trial_routine, None, train,\n",
    "                                     {\n",
    "                                         'input_dim': n_input_dims,\n",
    "                                         'hidden_dims': best_hidden_dims,\n",
    "                                         'output_dim': n_output_dims,\n",
    "                                         'activation': best_activation,\n",
    "                                         'activation_params': best_activation_params,\n",
    "                                         'dropout': 0.0,\n",
    "                                         'batch_norm': False,\n",
    "                                         'layer_norm': False,\n",
    "                                         'normalization_params': None,\n",
    "                                         'output_activation': None,\n",
    "                                         'output_activation_params': None,\n",
    "                                     },\n",
    "                                     train_dataset, validation_dataset, TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
    "                                     N_EPOCH,\n",
    "                                     torch.optim.Adam, LR, nn.MSELoss, kfold, log_step, {'loss': loss})\n",
    "            futures.append(future)\n",
    "\n",
    "    # Ensure all processes complete execution\n",
    "    for future in futures:\n",
    "        future.result()"
   ],
   "id": "631785ed9e3cacdb"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "experiment = mlflow.get_experiment_by_name(\"learning_rate\")\n",
    "\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "cols = [c for c in runs_df if c.startswith('metrics.')]\n",
    "metrics_names = [col[col.index('.') + 1:] for col in cols]\n",
    "\n",
    "learning_rates = set(runs_df['params.learning_rate'].unique())\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "results = {k1: {k2: [] for k2 in metrics_names} for k1 in learning_rates}\n",
    "for lr in learning_rates:\n",
    "    run_ids = runs_df[\n",
    "        (runs_df['params.loss'] == loss) &\n",
    "        (runs_df['params.learning_rate'] == lr)\n",
    "        ]['run_id']\n",
    "    for run_id in run_ids:\n",
    "        for metric_name in metrics_names:\n",
    "            results[lr][metric_name].append([metric.value for metric in client.get_metric_history(run_id, metric_name)])\n",
    "\n",
    "for k in results.keys():\n",
    "    for metric in results[k].keys():\n",
    "        results[k][metric] = np.vstack(results[k][metric]).mean(axis=0)"
   ],
   "id": "f1fd2ad81d1f6a08"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "configs_by_val_mape = sorted([(np.min(v['val_mape']), k) for k, v in results.items()])\n",
    "configs = [x[1] for x in configs_by_val_mape]\n",
    "\n",
    "labels = [f\"{float(c):.1e}\" for c in configs]\n",
    "\n",
    "plot(results, configs, ['loss', 'mape', 'r2', 'rmse_MN'], labels, logs=[True, True, False, True])"
   ],
   "id": "26dfb3c34b525ba8"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# Pareto front technique\n",
    "best_mape = configs_by_val_mape[0][0]\n",
    "candidates = [x[1] for x in filter(lambda x: x[0] <= best_mape * 1.02, configs_by_val_mape)]\n",
    "\n",
    "filtered_results = {k: results[k] for k in candidates}\n",
    "\n",
    "configs_by_val_rmse = sorted([(np.min(v['val_rmse_MN']), k) for k, v in filtered_results.items()])\n",
    "\n",
    "best_candidate = configs_by_val_rmse[0][1]\n",
    "best_learning_rate = float(best_candidate)\n",
    "\n",
    "print(f\"Best candidate is: {best_candidate} according to the pareto front.\")"
   ],
   "id": "dd1951623497aeb1"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "to_delete = [\n",
    "    'lr_values', 'executor', 'futures', 'LR', 'fold', 'train_idx', 'val_idx',\n",
    "    'train_dataset', 'validation_dataset', 'future', 'experiment', 'runs_df',\n",
    "    'cols', 'metrics_names', 'learning_rates', 'client', 'results',\n",
    "    'lr', 'run_ids', 'run_id', 'metric_name', 'k', 'configs_by_val_mape',\n",
    "    'configs', 'labels', 'best_mape', 'candidates', 'filtered_results',\n",
    "    'configs_by_val_rmse', 'best_candidate'\n",
    "]\n",
    "\n",
    "for var in to_delete:\n",
    "    try:\n",
    "        del globals()[var]\n",
    "    except KeyError:\n",
    "        pass"
   ],
   "id": "aced255603be3a50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "best_learning_rate = 1e-3",
   "id": "5b87aebe1aad7ce7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training final model",
   "id": "b925d6273839e233"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "4b0176b4d8b79c81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "mlflow.set_experiment(\"training\")\n",
    "\n",
    "VAL_BATCH_SIZE = 8192\n",
    "N_EPOCH = 1_000\n",
    "log_step = -1\n",
    "\n",
    "sizes = [2 ** i for i in range(8, 15)]\n",
    "batch_size_values = [512 for _ in sizes]\n",
    "dataset_path_values = [f\"{train_dataset_dir}/train_{n}.hdf5\" for n in sizes]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = []\n",
    "    for i, (dataset_path, TRAIN_BATCH_SIZE) in enumerate(zip(dataset_path_values, batch_size_values)):\n",
    "        size = sizes[i]\n",
    "        train_dataset = FixedPrattTrussDatasetThreeTargets(dataset_path)\n",
    "        validation_dataset = _validation_ds\n",
    "        future = executor.submit(trial_routine, None, train,\n",
    "                                 {\n",
    "                                     'input_dim': n_input_dims,\n",
    "                                     'hidden_dims': best_hidden_dims,\n",
    "                                     'output_dim': n_output_dims,\n",
    "                                     'activation': best_activation,\n",
    "                                     'activation_params': best_activation_params,\n",
    "                                     'dropout': 0.0,\n",
    "                                     'batch_norm': False,\n",
    "                                     'layer_norm': False,\n",
    "                                     'normalization_params': None,\n",
    "                                     'output_activation': None,\n",
    "                                     'output_activation_params': None\n",
    "                                 },\n",
    "                                 train_dataset, validation_dataset, TRAIN_BATCH_SIZE, VAL_BATCH_SIZE,\n",
    "                                 N_EPOCH,\n",
    "                                 torch.optim.Adam, best_learning_rate, nn.MSELoss,\n",
    "                                 -1, log_step, {'loss': loss, 'noise': 0.})\n",
    "        futures.append(future)\n",
    "\n",
    "    # Ensure all processes complete execution\n",
    "    for future in futures:\n",
    "        future.result()"
   ],
   "id": "f9afd9629e812550"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experiment = mlflow.get_experiment_by_name(\"training\")\n",
    "\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "cols = [c for c in runs_df if c.startswith('metrics.')]\n",
    "metrics_names = [col[col.index('.') + 1:] for col in cols]\n",
    "\n",
    "sizes = set()\n",
    "for size in runs_df['params.train_size'].unique():\n",
    "    sizes.add(size)\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "results = {k1: {k2: [] for k2 in metrics_names} for k1 in sizes}\n",
    "for size in sizes:\n",
    "    run_ids = runs_df[\n",
    "        (runs_df['params.loss'] == loss) &\n",
    "        (runs_df['params.train_size'] == size)\n",
    "        ]['run_id']\n",
    "    for run_id in run_ids:\n",
    "        for metric_name in metrics_names:\n",
    "            results[size][metric_name].append([m.value for m in client.get_metric_history(run_id, metric_name)])\n",
    "\n",
    "for k in results.keys():\n",
    "    for metric in results[k].keys():\n",
    "        results[k][metric] = np.vstack(results[k][metric]).mean(axis=0)"
   ],
   "id": "de25338dc5451bf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "configs = sorted([(np.min(v['val_mape']), k) for k, v in results.items()])\n",
    "configs = [x[1] for x in configs]\n",
    "\n",
    "labels = [f\"{int(size)}\" for size in configs]\n",
    "\n",
    "plot(results, configs, ['loss', 'mape', 'r2', 'rmse_MN'], labels, logs=[True, True, False, True])"
   ],
   "id": "7af0c8ab1bae50db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Noise sensitivity",
   "id": "f58c2b317235d424"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "for set_path, set_name in zip(\n",
    "    [f\"{test_dataset_dir}/0_uniform_8192.hdf5\",\n",
    "     f\"{test_dataset_dir}/1_category_8192.hdf5\"],\n",
    "    ['uniform_ea', 'category_ea']\n",
    "):\n",
    "    mlflow.set_tracking_uri(uri=MLFLOW_URI(port=PORT))\n",
    "    experiment = mlflow.get_experiment_by_name(\"training\")\n",
    "    runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "    runs_df = runs_df[runs_df['params.loss'] == loss]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for f in np.linspace(0, .1, 6):\n",
    "        validation_ds = FixedPrattTrussDatasetThreeTargets(\n",
    "            set_path,\n",
    "            f_noise_length=None,\n",
    "            f_noise_loads=lambda size: 1. + np.random.exponential(f / 2, size=size) * (\n",
    "                    (np.random.binomial(1, .5, size=size) * 2.) - 1),\n",
    "            f_noise_strain=lambda size: 1. + np.random.exponential(f / 2, size=size) * (\n",
    "                    (np.random.binomial(1, .5, size=size) * 2.) - 1),\n",
    "            f_noise_displacement=lambda size: 1. + np.random.exponential(f / 2, size=size) * (\n",
    "                    (np.random.binomial(1, .5, size=size) * 2.) - 1),\n",
    "            bisupported=bisupported\n",
    "        )\n",
    "\n",
    "        dl = DataLoader(validation_ds, batch_size=8192)\n",
    "\n",
    "        for i in range(len(runs_df[['artifact_uri', 'params.train_size']])):\n",
    "            artifact_uri = runs_df.iloc[i]['artifact_uri']\n",
    "            size = runs_df.iloc[i]['params.train_size']\n",
    "\n",
    "            uri = f\"{artifact_uri}/input_scaler/\"\n",
    "            input_scaler = load_model(uri)\n",
    "\n",
    "            uri = f\"{artifact_uri}/target_scaler/\"\n",
    "            target_scaler = load_model(uri)\n",
    "\n",
    "            uri = f\"{artifact_uri}/mape_model/\"\n",
    "            model = load_model(uri)\n",
    "\n",
    "            for batch in dl:\n",
    "                metrics = validation(model, batch, input_scaler, target_scaler, F.mse_loss)\n",
    "            results.append((size, f, metrics))\n",
    "\n",
    "    mlflow.set_tracking_uri(uri=MLFLOW_URI(port=PORT))\n",
    "    mlflow.set_experiment(\"noise_sensitivity\")\n",
    "\n",
    "    df = pd.DataFrame([[results[i][0], results[i][1],\n",
    "                        results[i][2]['r2'].item(),\n",
    "                        results[i][2]['mape'].item(),\n",
    "                        results[i][2]['rmse_MN'].item(),\n",
    "                        results[i][2]['loss']]\n",
    "                       for i in range(len(results))], columns=['train_size', 'noise', 'r2', 'mape', 'rmse_MN', 'loss'])\n",
    "    df.sort_values(by=['train_size', 'noise'], axis=0, ignore_index=True, inplace=True)\n",
    "\n",
    "    for size in df.train_size.unique():\n",
    "        with mlflow.start_run():\n",
    "            df_2 = df[df['train_size'] == size]\n",
    "            # Model information logging\n",
    "            mlflow.log_params({\n",
    "                'train_size': size,\n",
    "                'loss': loss,\n",
    "                'test_set': set_name\n",
    "            })\n",
    "            for i in range(len(df_2)):\n",
    "                mlflow.log_metrics(\n",
    "                    dict(df_2.iloc[i][1:]),\n",
    "                    step=i\n",
    "                )"
   ],
   "id": "c7a84290e518b902"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Closing the server",
   "id": "1e63d28497cd610a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "server.terminate()",
   "id": "ede93df77d0f5be9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
