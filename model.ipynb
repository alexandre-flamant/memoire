{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e7d4241-b558-4721-8e47-754a771591e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openseespy              import opensees as osp # Opensees interface\n",
    "\n",
    "osp.wipe()\n",
    "osp.model('basic', '-ndm', 2, '-ndf', 2)\n",
    "# Nodes\n",
    "osp.node(0, 0, 1)\n",
    "osp.node(1, 0, 0)\n",
    "osp.node(2, 1, 1)\n",
    "\n",
    "# Element\n",
    "osp.uniaxialMaterial(\"Elastic\", 0, 500)\n",
    "osp.element(\"Truss\", 0, *(0, 2), 200, 0)\n",
    "osp.element(\"Truss\", 1, *(1, 2), 200, 0)\n",
    "\n",
    "# Support\n",
    "osp.fix(0, True, True)\n",
    "osp.fix(1, True, True)\n",
    "\n",
    "# Load\n",
    "osp.timeSeries('Constant', 1)  # Define a constant time series for loading\n",
    "osp.pattern(\"Plain\", 1, 1)      # Define a plain load pattern\n",
    "osp.load(2, *(0, -1))\n",
    "\n",
    "# Define the solution procedure in OpenSees\n",
    "osp.system(\"FullGeneral\")                   # Define the system of equations\n",
    "osp.numberer(\"RCM\")                     # Define the numbering algorithm (Reverse Cuthill-McKee)\n",
    "osp.constraints(\"Plain\")                # Define the constraint handler\n",
    "osp.integrator(\"LoadControl\", 1.0)       # Define the integrator for the analysis (Load Control with step size 1.0)\n",
    "osp.algorithm(\"Linear\")                  # Define the solution algorithm (Linear)\n",
    "osp.analysis(\"Static\")                   # Define the type of analysis (Static)\n",
    "\n",
    "# Perform the analysis\n",
    "osp.analyze(1)  # Analyze one load step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c1e144df-6a05-4627-b52a-1d442622c533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.35355339e+05 3.53553391e+04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.53553391e+04 3.53553391e+04]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb629668-d242-4d89-abba-86fb28156a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Libraries and setup\n",
    "import json              # Save and load JSON files\n",
    "import numpy      as np  # Math library\n",
    "import pandas     as pd  # Dataframe library\n",
    "import pprint            # Aesthetic of output\n",
    "import re                # RegEx\n",
    "import sklearn           # Machine learning framework\n",
    "import torch             # Neural network framework\n",
    "import tqdm              # Loading bar\n",
    "\n",
    "from datetime                import datetime        # Tools for time formatting\n",
    "from openseespy              import opensees as osp # Opensees interface\n",
    "from torch                   import nn              # Neural network parts\n",
    "from torch                   import tensor          # Tensor\n",
    "from torch.utils.data        import DataLoader      # Dataset to batch\n",
    "from torch.utils.data        import Dataset         # Dataset class\n",
    "from torch.utils.data        import random_split    # Split Dataset to Subsets\n",
    "from torch.utils.tensorboard import SummaryWriter   # Writer\n",
    "from sklearn.preprocessing   import StandardScaler  # Scaler\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(linewidth=np.inf)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a517a43-3c48-4cb8-92f1-dbb2e8b6796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear10BarsTrussDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset containing trusses relevant data.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, nrows=None):\n",
    "        self.df = pd.read_csv(filename, nrows=nrows)\n",
    "        \n",
    "        self.data = self.df.drop(['n_cells',\n",
    "                                  'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'x5', 'y5', 'x6', 'y6',\n",
    "                                  'fix_x1', 'fix_y1', 'fix_x2', 'fix_y2', 'fix_x3', 'fix_y3',\n",
    "                                  'fix_x4', 'fix_y4', 'fix_x5', 'fix_y5', 'fix_x6', 'fix_y6', \n",
    "                                  'P_x1', 'P_y1', 'P_x2', 'P_y2', 'P_x3', 'P_y3', \n",
    "                                  'P_x4','P_x5', 'P_y5', 'P_x6'], axis=1)\n",
    "        \n",
    "        # Change units\n",
    "        # Area in cm^2\n",
    "        exp = re.compile('A_[0..9]*')\n",
    "        areas_col = [col for col in self.df.columns if exp.match(col)]\n",
    "        self.df[areas_col] *= 1.0e4\n",
    "        \n",
    "        # Young modulus in GPa\n",
    "        exp = re.compile('E_[0..9]*')\n",
    "        youngs_col = [col for col in self.df.columns if exp.match(col)]\n",
    "        self.df[youngs_col] *= 1.0e-9\n",
    "        \n",
    "        # Forces in kN\n",
    "        exp = re.compile('P_[x,y][0..9]*')\n",
    "        forces_col = [col for col in self.df.columns if exp.match(col)]\n",
    "        self.df[forces_col] *= 1.0e-3\n",
    "        \n",
    "        # Identify data\n",
    "        self.target = self.df[['E_1', 'E_2', 'E_3', 'E_4', 'E_5', 'E_6', 'E_7', 'E_8', 'E_9', 'E_10']]\n",
    "        self.data = self.data.drop(self.target.columns, axis=1)\n",
    "        \n",
    "        self.data_features = self.data.columns\n",
    "        self.target_features = self.target.columns\n",
    "        \n",
    "        self.target = tensor(self.target.to_numpy())\n",
    "        self.data = tensor(self.data.to_numpy())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.target.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx], self.df.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ead3aa4-a4db-48eb-9dee-c713c098c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Feed Forward Network\n",
    "    \"\"\"\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super(SimpleFNN, self).__init__()\n",
    "        self.layer_1 = nn.Linear(size_in, 100, dtype=torch.double)\n",
    "        self.activ_1 = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(100, size_out, dtype=torch.double)\n",
    "        self.activ_2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.activ_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.activ_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4a86520-571b-44e5-a8de-997837327d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '20/10/2024 11:40:14',\n",
      " 'model': 'LinearTrussGenerator',\n",
      " 'model_arguments': {'distributions': {'length': {'parameters': [4.0, 10.0], 'type': 'UNIFORM'},\n",
      "                                       'load': {'parameters': [-1000000.0, -1000.0], 'type': 'UNIFORM'}},\n",
      "                     'name': \"Linear cantilever's cross pattern beam\",\n",
      "                     'parameters': {'areas': {'default': {'parameters': [0.001, 0.1], 'type': 'UNIFORM_CONST'}},\n",
      "                                    'cell_height': {'default': {'parameters': ['length'], 'type': 'DISTRIBUTION'}},\n",
      "                                    'cell_length': {'default': {'parameters': ['length'], 'type': 'DISTRIBUTION'}},\n",
      "                                    'cell_number': {'default': {'parameters': [2], 'type': 'CONSTANT'}},\n",
      "                                    'loads': {'4-y': {'parameters': ['load'], 'type': 'DISTRIBUTION'},\n",
      "                                              '6-y': {'parameters': ['load'], 'type': 'DISTRIBUTION'},\n",
      "                                              'default': {'parameters': [0.0], 'type': 'CONSTANT'}},\n",
      "                                    'supports': {'1-x': {'parameters': [1], 'type': 'CONSTANT'},\n",
      "                                                 '1-y': {'parameters': [1], 'type': 'CONSTANT'},\n",
      "                                                 '2-x': {'parameters': [1], 'type': 'CONSTANT'},\n",
      "                                                 '2-y': {'parameters': [1], 'type': 'CONSTANT'},\n",
      "                                                 'default': {'parameters': [0], 'type': 'CONSTANT'}},\n",
      "                                    'youngs': {'default': {'parameters': [50000000000.0, 100000000000.0], 'type': 'UNIFORM_CONST'}}}},\n",
      " 'size': 100000}\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "# Reading dataset\n",
    "N_EPOCH = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_dir = \"./dataset_general_100_000/\"\n",
    "with open(f\"{data_dir}/info.json\") as f:\n",
    "    _info = json.load(f)\n",
    "pprint.pprint(_info, width=150)\n",
    "\n",
    "ds = Linear10BarsTrussDataset(f\"{data_dir}/data.csv\", nrows=100_000)\n",
    "train_ds, test_ds, eval_ds = random_split(ds, [.7,.2,.1])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "eval_dl = DataLoader(eval_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e3a6c54-8762-436b-907e-c5a27ca44ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_cells</th>\n",
       "      <th>cell_height</th>\n",
       "      <th>cell_length</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>...</th>\n",
       "      <th>N_1</th>\n",
       "      <th>N_2</th>\n",
       "      <th>N_3</th>\n",
       "      <th>N_4</th>\n",
       "      <th>N_5</th>\n",
       "      <th>N_6</th>\n",
       "      <th>N_7</th>\n",
       "      <th>N_8</th>\n",
       "      <th>N_9</th>\n",
       "      <th>N_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>8.684038</td>\n",
       "      <td>8.684038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.342019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.342019</td>\n",
       "      <td>8.684038</td>\n",
       "      <td>4.342019</td>\n",
       "      <td>8.684038</td>\n",
       "      <td>...</td>\n",
       "      <td>1238769.20</td>\n",
       "      <td>254422.060</td>\n",
       "      <td>-1297548.60</td>\n",
       "      <td>-379657.400</td>\n",
       "      <td>225032.390</td>\n",
       "      <td>254422.060</td>\n",
       "      <td>938287.060</td>\n",
       "      <td>-855160.50</td>\n",
       "      <td>536916.700</td>\n",
       "      <td>-359807.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8.975055</td>\n",
       "      <td>8.975055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.487527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.487527</td>\n",
       "      <td>8.975055</td>\n",
       "      <td>4.487527</td>\n",
       "      <td>8.975055</td>\n",
       "      <td>...</td>\n",
       "      <td>1706282.20</td>\n",
       "      <td>350441.250</td>\n",
       "      <td>-1787245.00</td>\n",
       "      <td>-522940.560</td>\n",
       "      <td>309959.880</td>\n",
       "      <td>350441.250</td>\n",
       "      <td>1292397.600</td>\n",
       "      <td>-1177899.10</td>\n",
       "      <td>739549.600</td>\n",
       "      <td>-495598.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6.651061</td>\n",
       "      <td>6.651061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.325531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.325531</td>\n",
       "      <td>6.651061</td>\n",
       "      <td>3.325531</td>\n",
       "      <td>6.651061</td>\n",
       "      <td>...</td>\n",
       "      <td>820711.30</td>\n",
       "      <td>168560.080</td>\n",
       "      <td>-859653.90</td>\n",
       "      <td>-251531.200</td>\n",
       "      <td>149088.800</td>\n",
       "      <td>168560.080</td>\n",
       "      <td>621635.400</td>\n",
       "      <td>-566562.25</td>\n",
       "      <td>355718.840</td>\n",
       "      <td>-238379.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4.829182</td>\n",
       "      <td>4.829182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.414591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.414591</td>\n",
       "      <td>4.829182</td>\n",
       "      <td>2.414591</td>\n",
       "      <td>4.829182</td>\n",
       "      <td>...</td>\n",
       "      <td>1261558.90</td>\n",
       "      <td>259102.660</td>\n",
       "      <td>-1321419.60</td>\n",
       "      <td>-386641.970</td>\n",
       "      <td>229172.310</td>\n",
       "      <td>259102.660</td>\n",
       "      <td>955548.700</td>\n",
       "      <td>-870892.94</td>\n",
       "      <td>546794.300</td>\n",
       "      <td>-366426.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4.596314</td>\n",
       "      <td>4.596314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.298157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.298157</td>\n",
       "      <td>4.596314</td>\n",
       "      <td>2.298157</td>\n",
       "      <td>4.596314</td>\n",
       "      <td>...</td>\n",
       "      <td>290618.22</td>\n",
       "      <td>59688.016</td>\n",
       "      <td>-304407.97</td>\n",
       "      <td>-89068.530</td>\n",
       "      <td>52793.133</td>\n",
       "      <td>59688.016</td>\n",
       "      <td>220124.360</td>\n",
       "      <td>-200622.69</td>\n",
       "      <td>125961.920</td>\n",
       "      <td>-84411.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>7.067292</td>\n",
       "      <td>7.067292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.533646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.533646</td>\n",
       "      <td>7.067292</td>\n",
       "      <td>3.533646</td>\n",
       "      <td>7.067292</td>\n",
       "      <td>...</td>\n",
       "      <td>293969.90</td>\n",
       "      <td>60376.400</td>\n",
       "      <td>-307918.72</td>\n",
       "      <td>-90095.760</td>\n",
       "      <td>53401.996</td>\n",
       "      <td>60376.400</td>\n",
       "      <td>222663.060</td>\n",
       "      <td>-202936.47</td>\n",
       "      <td>127414.640</td>\n",
       "      <td>-85385.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>9.169996</td>\n",
       "      <td>9.169996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.584998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.584998</td>\n",
       "      <td>9.169996</td>\n",
       "      <td>4.584998</td>\n",
       "      <td>9.169996</td>\n",
       "      <td>...</td>\n",
       "      <td>1938310.60</td>\n",
       "      <td>398095.900</td>\n",
       "      <td>-2030283.00</td>\n",
       "      <td>-594052.500</td>\n",
       "      <td>352109.700</td>\n",
       "      <td>398095.900</td>\n",
       "      <td>1468144.000</td>\n",
       "      <td>-1338075.40</td>\n",
       "      <td>840117.100</td>\n",
       "      <td>-562992.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>9.806996</td>\n",
       "      <td>9.806996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.903498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.903498</td>\n",
       "      <td>9.806996</td>\n",
       "      <td>4.903498</td>\n",
       "      <td>9.806996</td>\n",
       "      <td>...</td>\n",
       "      <td>309090.70</td>\n",
       "      <td>63481.950</td>\n",
       "      <td>-323756.97</td>\n",
       "      <td>-94729.960</td>\n",
       "      <td>56148.805</td>\n",
       "      <td>63481.950</td>\n",
       "      <td>234116.060</td>\n",
       "      <td>-213374.80</td>\n",
       "      <td>133968.400</td>\n",
       "      <td>-89777.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>4.362812</td>\n",
       "      <td>4.362812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.181406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.181406</td>\n",
       "      <td>4.362812</td>\n",
       "      <td>2.181406</td>\n",
       "      <td>4.362812</td>\n",
       "      <td>...</td>\n",
       "      <td>1271342.00</td>\n",
       "      <td>261111.920</td>\n",
       "      <td>-1331666.90</td>\n",
       "      <td>-389640.280</td>\n",
       "      <td>230949.480</td>\n",
       "      <td>261111.920</td>\n",
       "      <td>962958.750</td>\n",
       "      <td>-877646.44</td>\n",
       "      <td>551034.560</td>\n",
       "      <td>-369268.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>5.145792</td>\n",
       "      <td>5.145792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.572896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.572896</td>\n",
       "      <td>5.145792</td>\n",
       "      <td>2.572896</td>\n",
       "      <td>5.145792</td>\n",
       "      <td>...</td>\n",
       "      <td>163811.42</td>\n",
       "      <td>33644.070</td>\n",
       "      <td>-171584.23</td>\n",
       "      <td>-50204.848</td>\n",
       "      <td>29757.662</td>\n",
       "      <td>33644.070</td>\n",
       "      <td>124076.484</td>\n",
       "      <td>-113084.06</td>\n",
       "      <td>71000.375</td>\n",
       "      <td>-47579.900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_cells  cell_height  cell_length   x1        y1   x2        y2        x3  \\\n",
       "0        2     8.684038     8.684038  0.0  4.342019  0.0 -4.342019  8.684038   \n",
       "1        2     8.975055     8.975055  0.0  4.487527  0.0 -4.487527  8.975055   \n",
       "2        2     6.651061     6.651061  0.0  3.325531  0.0 -3.325531  6.651061   \n",
       "3        2     4.829182     4.829182  0.0  2.414591  0.0 -2.414591  4.829182   \n",
       "4        2     4.596314     4.596314  0.0  2.298157  0.0 -2.298157  4.596314   \n",
       "5        2     7.067292     7.067292  0.0  3.533646  0.0 -3.533646  7.067292   \n",
       "6        2     9.169996     9.169996  0.0  4.584998  0.0 -4.584998  9.169996   \n",
       "7        2     9.806996     9.806996  0.0  4.903498  0.0 -4.903498  9.806996   \n",
       "8        2     4.362812     4.362812  0.0  2.181406  0.0 -2.181406  4.362812   \n",
       "9        2     5.145792     5.145792  0.0  2.572896  0.0 -2.572896  5.145792   \n",
       "\n",
       "         y3        x4  ...         N_1         N_2         N_3         N_4  \\\n",
       "0  4.342019  8.684038  ...  1238769.20  254422.060 -1297548.60 -379657.400   \n",
       "1  4.487527  8.975055  ...  1706282.20  350441.250 -1787245.00 -522940.560   \n",
       "2  3.325531  6.651061  ...   820711.30  168560.080  -859653.90 -251531.200   \n",
       "3  2.414591  4.829182  ...  1261558.90  259102.660 -1321419.60 -386641.970   \n",
       "4  2.298157  4.596314  ...   290618.22   59688.016  -304407.97  -89068.530   \n",
       "5  3.533646  7.067292  ...   293969.90   60376.400  -307918.72  -90095.760   \n",
       "6  4.584998  9.169996  ...  1938310.60  398095.900 -2030283.00 -594052.500   \n",
       "7  4.903498  9.806996  ...   309090.70   63481.950  -323756.97  -94729.960   \n",
       "8  2.181406  4.362812  ...  1271342.00  261111.920 -1331666.90 -389640.280   \n",
       "9  2.572896  5.145792  ...   163811.42   33644.070  -171584.23  -50204.848   \n",
       "\n",
       "          N_5         N_6          N_7         N_8         N_9        N_10  \n",
       "0  225032.390  254422.060   938287.060  -855160.50  536916.700 -359807.120  \n",
       "1  309959.880  350441.250  1292397.600 -1177899.10  739549.600 -495598.750  \n",
       "2  149088.800  168560.080   621635.400  -566562.25  355718.840 -238379.950  \n",
       "3  229172.310  259102.660   955548.700  -870892.94  546794.300 -366426.500  \n",
       "4   52793.133   59688.016   220124.360  -200622.69  125961.920  -84411.600  \n",
       "5   53401.996   60376.400   222663.060  -202936.47  127414.640  -85385.125  \n",
       "6  352109.700  398095.900  1468144.000 -1338075.40  840117.100 -562992.600  \n",
       "7   56148.805   63481.950   234116.060  -213374.80  133968.400  -89777.030  \n",
       "8  230949.480  261111.920   962958.750  -877646.44  551034.560 -369268.030  \n",
       "9   29757.662   33644.070   124076.484  -113084.06   71000.375  -47579.900  \n",
       "\n",
       "[10 rows x 81 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,_,df = ds[0:10]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0dfc7dc1-245b-4dcf-b808-9f6b170e6482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6, 2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_residual(df):\n",
    "    n_cells = df['n_cells']\n",
    "    n_bars  = 5*n_cells\n",
    "    n_nodes = 2*(n_cells + 1)\n",
    "    \n",
    "    # Get the nodes\n",
    "    exp_x = re.compile('x[0..9]*')\n",
    "    exp_y = re.compile('y[0..9]*')\n",
    "    \n",
    "    x_cols = [col for col in df.columns if exp_x.match(col)]\n",
    "    y_cols = [col for col in df.columns if exp_y.match(col)]\n",
    "    \n",
    "    x = [df.iloc[i][x_cols].to_numpy() for i in df.index]\n",
    "    y = [df.iloc[i][y_cols].to_numpy() for i in df.index]\n",
    "    \n",
    "    nodes = np.stack((x,y), axis=2)\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "nodes = compute_residual(df)\n",
    "nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d66d367-b326-4d57-8cdf-d32fd7133a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and\n",
    "model = SimpleFNN(len(ds.data_features), len(ds.target_features))\n",
    "model.to(device)\n",
    "\n",
    "loss_fn  = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6458257-414c-4e6f-99a5-fa8a8d649501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train scalers\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "for x, y in train_dl:\n",
    "    x_scaler.partial_fit(x)\n",
    "    y_scaler.partial_fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8015eeb-8d0c-43e8-9f16-c3b6b3c42034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 1.0672617891241887\n",
      "  batch 200 loss: 1.0037822532476888\n",
      "  batch 300 loss: 1.0140957894402662\n",
      "  batch 400 loss: 0.9697347056287203\n",
      "  batch 500 loss: 0.9956144703234452\n",
      "  batch 600 loss: 0.9875832379665869\n",
      "  batch 700 loss: 0.9437476293772227\n",
      "  batch 800 loss: 0.9500484896568991\n",
      "  batch 900 loss: 0.8901252799573329\n",
      "  batch 1000 loss: 0.8761901750983107\n",
      "  batch 1094 loss: 0.8617039838665395\n",
      "LOSS train 0.8617039838665395 valid 0.8461031369544714\n",
      "MAE train 11.369495102532236 valid 11.242415594876578\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.8313130605814671\n",
      "  batch 200 loss: 0.8345266745210086\n",
      "  batch 300 loss: 0.8302688006760041\n",
      "  batch 400 loss: 0.8146787133131999\n",
      "  batch 500 loss: 0.7809165768670955\n",
      "  batch 600 loss: 0.7697037548227584\n",
      "  batch 700 loss: 0.7839069336504422\n",
      "  batch 800 loss: 0.7439119638460793\n",
      "  batch 900 loss: 0.7483282525313176\n",
      "  batch 1000 loss: 0.7664178276022219\n",
      "  batch 1094 loss: 0.7355735289701867\n",
      "LOSS train 0.7355735289701867 valid 0.736297404627205\n",
      "MAE train 10.20385184120407 valid 10.198049297524587\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 0.7423204912802465\n",
      "  batch 200 loss: 0.7460232481594592\n",
      "  batch 300 loss: 0.7248028708296509\n",
      "  batch 400 loss: 0.7145212507335933\n",
      "  batch 500 loss: 0.7171873656398715\n",
      "  batch 600 loss: 0.7101076732661207\n",
      "  batch 700 loss: 0.7367363196469328\n",
      "  batch 800 loss: 0.7350727124184891\n",
      "  batch 900 loss: 0.7246706298649745\n",
      "  batch 1000 loss: 0.7453919845109648\n",
      "  batch 1094 loss: 0.7157230773816665\n",
      "LOSS train 0.7157230773816665 valid 0.7107786895404165\n",
      "MAE train 9.914458966161632 valid 9.935465128391554\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 0.7306823952901753\n",
      "  batch 200 loss: 0.7042297869382971\n",
      "  batch 300 loss: 0.7132406459868297\n",
      "  batch 400 loss: 0.6967058700840691\n",
      "  batch 500 loss: 0.7038286466032448\n",
      "  batch 600 loss: 0.703182948077218\n",
      "  batch 700 loss: 0.7187465452013017\n",
      "  batch 800 loss: 0.7037792041982744\n",
      "  batch 900 loss: 0.7091993164444849\n",
      "  batch 1000 loss: 0.6989826896287303\n",
      "  batch 1094 loss: 0.7006010383211753\n",
      "LOSS train 0.7006010383211753 valid 0.689324209333846\n",
      "MAE train 9.765036799712362 valid 9.661279755263642\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 0.6800239293389168\n",
      "  batch 200 loss: 0.704045622059969\n",
      "  batch 300 loss: 0.6951802166040415\n",
      "  batch 400 loss: 0.7018934423014478\n",
      "  batch 500 loss: 0.7026624846807092\n",
      "  batch 600 loss: 0.6989144760152137\n",
      "  batch 700 loss: 0.7091885497132664\n",
      "  batch 800 loss: 0.7009011945376961\n",
      "  batch 900 loss: 0.6885453846542323\n",
      "  batch 1000 loss: 0.6907890327668518\n",
      "  batch 1094 loss: 0.6775309515572835\n",
      "LOSS train 0.6775309515572835 valid 0.6784080943825309\n",
      "MAE train 9.497224147582646 valid 9.499830103354311\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 0.7031448065680691\n",
      "  batch 200 loss: 0.6889005005116628\n",
      "  batch 300 loss: 0.7008396300599996\n",
      "  batch 400 loss: 0.6823074621788144\n",
      "  batch 500 loss: 0.6790299099801237\n",
      "  batch 600 loss: 0.6808873844035837\n",
      "  batch 700 loss: 0.6880425253000745\n",
      "  batch 800 loss: 0.6845602110083675\n",
      "  batch 900 loss: 0.6869937835421857\n",
      "  batch 1000 loss: 0.691573931152466\n",
      "  batch 1094 loss: 0.6695057364866989\n",
      "LOSS train 0.6695057364866989 valid 0.6756291648971742\n",
      "MAE train 9.420225081374879 valid 9.491719566932131\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 0.6864131544904744\n",
      "  batch 200 loss: 0.6987694427077339\n",
      "  batch 300 loss: 0.6834690357335014\n",
      "  batch 400 loss: 0.6841528984601659\n",
      "  batch 500 loss: 0.6860423681334066\n",
      "  batch 600 loss: 0.6920329232441966\n",
      "  batch 700 loss: 0.6592313336381408\n",
      "  batch 800 loss: 0.6764557872417016\n",
      "  batch 900 loss: 0.672119917738598\n",
      "  batch 1000 loss: 0.6816661910061663\n",
      "  batch 1094 loss: 0.6729792325179228\n",
      "LOSS train 0.6729792325179228 valid 0.669543537011751\n",
      "MAE train 9.449104107095886 valid 9.399290227261124\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 0.6816106796489606\n",
      "  batch 200 loss: 0.6866292460463381\n",
      "  batch 300 loss: 0.6614887630421229\n",
      "  batch 400 loss: 0.6687107887368977\n",
      "  batch 500 loss: 0.6780022070599051\n",
      "  batch 600 loss: 0.6710423521289154\n",
      "  batch 700 loss: 0.6662786108139376\n",
      "  batch 800 loss: 0.6893039769897104\n",
      "  batch 900 loss: 0.6851451797259358\n",
      "  batch 1000 loss: 0.6692413305344396\n",
      "  batch 1094 loss: 0.6835622839823106\n",
      "LOSS train 0.6835622839823106 valid 0.662713986557707\n",
      "MAE train 9.50872597593479 valid 9.324977759863385\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 0.6771368304452932\n",
      "  batch 200 loss: 0.6658941327595542\n",
      "  batch 300 loss: 0.6799242623277459\n",
      "  batch 400 loss: 0.6675603909450133\n",
      "  batch 500 loss: 0.6709120453577505\n",
      "  batch 600 loss: 0.6752063135406078\n",
      "  batch 700 loss: 0.6693385411602688\n",
      "  batch 800 loss: 0.6833946848802905\n",
      "  batch 900 loss: 0.6724362504254426\n",
      "  batch 1000 loss: 0.6727104554632725\n",
      "  batch 1094 loss: 0.658662169681865\n",
      "LOSS train 0.658662169681865 valid 0.6649128190958384\n",
      "MAE train 9.305051518233528 valid 9.361570557526665\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 0.6692490270859744\n",
      "  batch 200 loss: 0.6890703312184869\n",
      "  batch 300 loss: 0.6546194924408586\n",
      "  batch 400 loss: 0.6573128347941521\n",
      "  batch 500 loss: 0.6728986622607153\n",
      "  batch 600 loss: 0.6715281565109419\n",
      "  batch 700 loss: 0.6927294254400816\n",
      "  batch 800 loss: 0.6662848057243344\n",
      "  batch 900 loss: 0.6770069444348802\n",
      "  batch 1000 loss: 0.6606740201604174\n",
      "  batch 1094 loss: 0.6401685465632673\n",
      "LOSS train 0.6401685465632673 valid 0.6567588545843537\n",
      "MAE train 9.153877054321043 valid 9.260602373178008\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 0.6657975630855346\n",
      "  batch 200 loss: 0.6709374571573298\n",
      "  batch 300 loss: 0.6680555075750166\n",
      "  batch 400 loss: 0.6517430910927439\n",
      "  batch 500 loss: 0.674303736012506\n",
      "  batch 600 loss: 0.6567358177723296\n",
      "  batch 700 loss: 0.6794866533887084\n",
      "  batch 800 loss: 0.6512361887597745\n",
      "  batch 900 loss: 0.6818267512359084\n",
      "  batch 1000 loss: 0.6560857264007213\n",
      "  batch 1094 loss: 0.6653816457685336\n",
      "LOSS train 0.6653816457685336 valid 0.6630102874982398\n",
      "MAE train 9.358114563429375 valid 9.314154336194253\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 0.6802809448928566\n",
      "  batch 200 loss: 0.6501009763814758\n",
      "  batch 300 loss: 0.6575678493689836\n",
      "  batch 400 loss: 0.6549717210973813\n",
      "  batch 500 loss: 0.6566156454341642\n",
      "  batch 600 loss: 0.6627359865137413\n",
      "  batch 700 loss: 0.6407071141599349\n",
      "  batch 800 loss: 0.6658980206228436\n",
      "  batch 900 loss: 0.672536932175587\n",
      "  batch 1000 loss: 0.6614040020877529\n",
      "  batch 1094 loss: 0.6794458216679283\n",
      "LOSS train 0.6794458216679283 valid 0.6502019645091525\n",
      "MAE train 9.411891077286308 valid 9.219403485469934\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 0.6622398928837048\n",
      "  batch 200 loss: 0.6720862269447478\n",
      "  batch 300 loss: 0.6584598140738401\n",
      "  batch 400 loss: 0.6534864056163362\n",
      "  batch 500 loss: 0.6628619016351738\n",
      "  batch 600 loss: 0.6811848995166891\n",
      "  batch 700 loss: 0.6477811403322246\n",
      "  batch 800 loss: 0.6480376770242238\n",
      "  batch 900 loss: 0.6600911122952126\n",
      "  batch 1000 loss: 0.653489495484559\n",
      "  batch 1094 loss: 0.662396140560113\n",
      "LOSS train 0.662396140560113 valid 0.6496557959996065\n",
      "MAE train 9.26660757484075 valid 9.193450912112004\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 0.6585643676485151\n",
      "  batch 200 loss: 0.6534350960870622\n",
      "  batch 300 loss: 0.6390505347666791\n",
      "  batch 400 loss: 0.657867578980947\n",
      "  batch 500 loss: 0.656789932680216\n",
      "  batch 600 loss: 0.6752709776066772\n",
      "  batch 700 loss: 0.6681123784070799\n",
      "  batch 800 loss: 0.6561854293704625\n",
      "  batch 900 loss: 0.6492186557792826\n",
      "  batch 1000 loss: 0.6412090886717489\n",
      "  batch 1094 loss: 0.6683272896396854\n",
      "LOSS train 0.6683272896396854 valid 0.6507710345349647\n",
      "MAE train 9.343650024204456 valid 9.21348646438301\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 0.6442324405684253\n",
      "  batch 200 loss: 0.6497206164202842\n",
      "  batch 300 loss: 0.6515045877324535\n",
      "  batch 400 loss: 0.651800232358883\n",
      "  batch 500 loss: 0.6844502832282118\n",
      "  batch 600 loss: 0.6478594766245896\n",
      "  batch 700 loss: 0.6593100081452576\n",
      "  batch 800 loss: 0.6502101927941766\n",
      "  batch 900 loss: 0.6468775762283109\n",
      "  batch 1000 loss: 0.6449579466493545\n",
      "  batch 1094 loss: 0.6570563050856636\n",
      "LOSS train 0.6570563050856636 valid 0.6395914334722763\n",
      "MAE train 9.163681475681255 valid 9.039517118244639\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 0.6420658975941629\n",
      "  batch 200 loss: 0.6586750717454939\n",
      "  batch 300 loss: 0.6521884846479652\n",
      "  batch 400 loss: 0.6482959422356697\n",
      "  batch 500 loss: 0.6462895654701049\n",
      "  batch 600 loss: 0.6677195697019223\n",
      "  batch 700 loss: 0.636913545917099\n",
      "  batch 800 loss: 0.6566248405258396\n",
      "  batch 900 loss: 0.6414748030765548\n",
      "  batch 1000 loss: 0.6481377534370735\n",
      "  batch 1094 loss: 0.640191130356178\n",
      "LOSS train 0.640191130356178 valid 0.6390210077594702\n",
      "MAE train 9.024247862536537 valid 9.027260538359071\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 0.6399274417547481\n",
      "  batch 200 loss: 0.6515245676740213\n",
      "  batch 300 loss: 0.6442426837808126\n",
      "  batch 400 loss: 0.6624594485588902\n",
      "  batch 500 loss: 0.6451235447022303\n",
      "  batch 600 loss: 0.6529134537200143\n",
      "  batch 700 loss: 0.638060699810451\n",
      "  batch 800 loss: 0.6442053466569714\n",
      "  batch 900 loss: 0.6511443435651728\n",
      "  batch 1000 loss: 0.634477140320825\n",
      "  batch 1094 loss: 0.6387273326510816\n",
      "LOSS train 0.6387273326510816 valid 0.6303036128562409\n",
      "MAE train 8.99042360474594 valid 8.919708566044763\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 0.6391752105595141\n",
      "  batch 200 loss: 0.6518792333473615\n",
      "  batch 300 loss: 0.6589490740806155\n",
      "  batch 400 loss: 0.6479566342337865\n",
      "  batch 500 loss: 0.6336614002056138\n",
      "  batch 600 loss: 0.642166574990128\n",
      "  batch 700 loss: 0.62549919234938\n",
      "  batch 800 loss: 0.6435947774240526\n",
      "  batch 900 loss: 0.6248125470581747\n",
      "  batch 1000 loss: 0.631820758713758\n",
      "  batch 1094 loss: 0.6305347252561594\n",
      "LOSS train 0.6305347252561594 valid 0.6249448390783562\n",
      "MAE train 8.94906910186627 valid 8.885475817726135\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 0.6207269989224304\n",
      "  batch 200 loss: 0.6450119210970685\n",
      "  batch 300 loss: 0.6238297221593665\n",
      "  batch 400 loss: 0.6337915908567875\n",
      "  batch 500 loss: 0.6382650677788279\n",
      "  batch 600 loss: 0.641605969018798\n",
      "  batch 700 loss: 0.6262461456983964\n",
      "  batch 800 loss: 0.6220593251285806\n",
      "  batch 900 loss: 0.6294761790071103\n",
      "  batch 1000 loss: 0.6388496948548199\n",
      "  batch 1094 loss: 0.6131325671259427\n",
      "LOSS train 0.6131325671259427 valid 0.6153348028080178\n",
      "MAE train 8.734498476133044 valid 8.753084442038578\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 0.6360237625827391\n",
      "  batch 200 loss: 0.6300401325418092\n",
      "  batch 300 loss: 0.6131760875358376\n",
      "  batch 400 loss: 0.630014502361538\n",
      "  batch 500 loss: 0.6180536677311871\n",
      "  batch 600 loss: 0.625939938329602\n",
      "  batch 700 loss: 0.6133109502132622\n",
      "  batch 800 loss: 0.6316835388936406\n",
      "  batch 900 loss: 0.6315615921625721\n",
      "  batch 1000 loss: 0.6358687506694616\n",
      "  batch 1094 loss: 0.6226632927375757\n",
      "LOSS train 0.6226632927375757 valid 0.6311813501038219\n",
      "MAE train 8.840208654798777 valid 8.935708087578625\n",
      "EPOCH 21:\n",
      "  batch 100 loss: 0.6267422410560057\n",
      "  batch 200 loss: 0.5979573488912305\n",
      "  batch 300 loss: 0.6346680956419924\n",
      "  batch 400 loss: 0.6407056316400782\n",
      "  batch 500 loss: 0.6146239171697314\n",
      "  batch 600 loss: 0.6104337561956943\n",
      "  batch 700 loss: 0.6323525381786401\n",
      "  batch 800 loss: 0.6556282694860505\n",
      "  batch 900 loss: 0.6213788279465156\n",
      "  batch 1000 loss: 0.6078441082046934\n",
      "  batch 1094 loss: 0.6321982689576441\n",
      "LOSS train 0.6321982689576441 valid 0.6139955557236573\n",
      "MAE train 8.882999462581676 valid 8.779229329548143\n",
      "EPOCH 22:\n",
      "  batch 100 loss: 0.6309986994432205\n",
      "  batch 200 loss: 0.6108013770025156\n",
      "  batch 300 loss: 0.6322936494931856\n",
      "  batch 400 loss: 0.6272783069275466\n",
      "  batch 500 loss: 0.6183019413914421\n",
      "  batch 600 loss: 0.6273987648023742\n",
      "  batch 700 loss: 0.6181186818469214\n",
      "  batch 800 loss: 0.6272052997808136\n",
      "  batch 900 loss: 0.6103607248007645\n",
      "  batch 1000 loss: 0.6163731408701322\n",
      "  batch 1094 loss: 0.6142342905873357\n",
      "LOSS train 0.6142342905873357 valid 0.6103639083957803\n",
      "MAE train 8.730968698955785 valid 8.690709196526175\n",
      "EPOCH 23:\n",
      "  batch 100 loss: 0.5995923495950289\n",
      "  batch 200 loss: 0.6350795509555645\n",
      "  batch 300 loss: 0.6115632432213369\n",
      "  batch 400 loss: 0.6299203213440573\n",
      "  batch 500 loss: 0.6294415104092708\n",
      "  batch 600 loss: 0.6246228824019879\n",
      "  batch 700 loss: 0.6158207930881775\n",
      "  batch 800 loss: 0.6094166313685833\n",
      "  batch 900 loss: 0.620322633343566\n",
      "  batch 1000 loss: 0.620457137567097\n",
      "  batch 1094 loss: 0.6233957940796916\n",
      "LOSS train 0.6233957940796916 valid 0.6125881811830359\n",
      "MAE train 8.835813820189992 valid 8.770319116833342\n",
      "EPOCH 24:\n",
      "  batch 100 loss: 0.6119698003221942\n",
      "  batch 200 loss: 0.6221100795303387\n",
      "  batch 300 loss: 0.608434732990142\n",
      "  batch 400 loss: 0.6339541966641816\n",
      "  batch 500 loss: 0.622718956788391\n",
      "  batch 600 loss: 0.6184901575414716\n",
      "  batch 700 loss: 0.6222514259109992\n",
      "  batch 800 loss: 0.6050846167033022\n",
      "  batch 900 loss: 0.6243520869633614\n",
      "  batch 1000 loss: 0.6221169189168521\n",
      "  batch 1094 loss: 0.6124813246816336\n",
      "LOSS train 0.6124813246816336 valid 0.6058283667187323\n",
      "MAE train 8.707556399236392 valid 8.651374146032174\n",
      "EPOCH 25:\n",
      "  batch 100 loss: 0.6092442232466911\n",
      "  batch 200 loss: 0.6263769336500233\n",
      "  batch 300 loss: 0.6144809156459299\n",
      "  batch 400 loss: 0.6252552419073583\n",
      "  batch 500 loss: 0.6282048496449741\n",
      "  batch 600 loss: 0.606240511828779\n",
      "  batch 700 loss: 0.6208308881796831\n",
      "  batch 800 loss: 0.630734087523647\n",
      "  batch 900 loss: 0.6180994386705593\n",
      "  batch 1000 loss: 0.602941422600686\n",
      "  batch 1094 loss: 0.6270373387184222\n",
      "LOSS train 0.6270373387184222 valid 0.6084190310819566\n",
      "MAE train 8.812793043725963 valid 8.684958959098724\n",
      "EPOCH 26:\n",
      "  batch 100 loss: 0.622528526005689\n",
      "  batch 200 loss: 0.6155033427257073\n",
      "  batch 300 loss: 0.6233335987589039\n",
      "  batch 400 loss: 0.5973727213852333\n",
      "  batch 500 loss: 0.6175691652068015\n",
      "  batch 600 loss: 0.6426716462599106\n",
      "  batch 700 loss: 0.6211108939623917\n",
      "  batch 800 loss: 0.6138206141643632\n",
      "  batch 900 loss: 0.6147035621961655\n",
      "  batch 1000 loss: 0.6124335252044844\n",
      "  batch 1094 loss: 0.606917014219565\n",
      "LOSS train 0.606917014219565 valid 0.602967857705785\n",
      "MAE train 8.669441853326925 valid 8.613402190810694\n",
      "EPOCH 27:\n",
      "  batch 100 loss: 0.6112591827192557\n",
      "  batch 200 loss: 0.608468327290207\n",
      "  batch 300 loss: 0.6172975924290294\n",
      "  batch 400 loss: 0.6345329118376319\n",
      "  batch 500 loss: 0.6099548123096931\n",
      "  batch 600 loss: 0.5954702976915941\n",
      "  batch 700 loss: 0.6269406048135019\n",
      "  batch 800 loss: 0.6178019181996168\n",
      "  batch 900 loss: 0.631313847289755\n",
      "  batch 1000 loss: 0.6022454110782823\n",
      "  batch 1094 loss: 0.6145695883230977\n",
      "LOSS train 0.6145695883230977 valid 0.6061821608267385\n",
      "MAE train 8.716882733838888 valid 8.652658268133694\n",
      "EPOCH 28:\n",
      "  batch 100 loss: 0.6137251414113757\n",
      "  batch 200 loss: 0.6034570128776308\n",
      "  batch 300 loss: 0.6122393559947649\n",
      "  batch 400 loss: 0.6335855020973665\n",
      "  batch 500 loss: 0.6148011585032385\n",
      "  batch 600 loss: 0.6149242242544006\n",
      "  batch 700 loss: 0.6212138622126029\n",
      "  batch 800 loss: 0.6071933712608246\n",
      "  batch 900 loss: 0.5978489595445075\n",
      "  batch 1000 loss: 0.615921763868487\n",
      "  batch 1094 loss: 0.6251410447283342\n",
      "LOSS train 0.6251410447283342 valid 0.6023006310601718\n",
      "MAE train 8.817407151165492 valid 8.583541372202047\n",
      "EPOCH 29:\n",
      "  batch 100 loss: 0.6204697062747634\n",
      "  batch 200 loss: 0.6060745357819611\n",
      "  batch 300 loss: 0.6092653672890505\n",
      "  batch 400 loss: 0.6227295840298988\n",
      "  batch 500 loss: 0.6328558458584179\n",
      "  batch 600 loss: 0.6091106151713246\n",
      "  batch 700 loss: 0.5947730963686363\n",
      "  batch 800 loss: 0.6197589465135739\n",
      "  batch 900 loss: 0.6077787104367953\n",
      "  batch 1000 loss: 0.6200621483121409\n",
      "  batch 1094 loss: 0.609304744892564\n",
      "LOSS train 0.609304744892564 valid 0.630805098735481\n",
      "MAE train 8.672151523297362 valid 8.948328487995445\n",
      "EPOCH 30:\n",
      "  batch 100 loss: 0.6125736608762062\n",
      "  batch 200 loss: 0.6005557276415634\n",
      "  batch 300 loss: 0.614989015111004\n",
      "  batch 400 loss: 0.6115378612398774\n",
      "  batch 500 loss: 0.6117856531573787\n",
      "  batch 600 loss: 0.6142740211460511\n",
      "  batch 700 loss: 0.6289420309361985\n",
      "  batch 800 loss: 0.6138336480679157\n",
      "  batch 900 loss: 0.6201223013141287\n",
      "  batch 1000 loss: 0.5997784838904389\n",
      "  batch 1094 loss: 0.6310834606550956\n",
      "LOSS train 0.6310834606550956 valid 0.6114998078128105\n",
      "MAE train 8.919047427350367 valid 8.784362124785025\n",
      "EPOCH 31:\n",
      "  batch 100 loss: 0.6012279313976753\n",
      "  batch 200 loss: 0.6181365160573743\n",
      "  batch 300 loss: 0.6215437761651481\n",
      "  batch 400 loss: 0.6047399856099138\n",
      "  batch 500 loss: 0.6003201804593071\n",
      "  batch 600 loss: 0.6239915764460376\n",
      "  batch 700 loss: 0.6099161484800959\n",
      "  batch 800 loss: 0.6076020557777665\n",
      "  batch 900 loss: 0.6319167654485718\n",
      "  batch 1000 loss: 0.6209770396750848\n",
      "  batch 1094 loss: 0.6066450174486065\n",
      "LOSS train 0.6066450174486065 valid 0.6008642120219118\n",
      "MAE train 8.609599110476923 valid 8.60358041997291\n",
      "EPOCH 32:\n",
      "  batch 100 loss: 0.6061764687400779\n",
      "  batch 200 loss: 0.606246269698998\n",
      "  batch 300 loss: 0.6137983547798446\n",
      "  batch 400 loss: 0.5940946712110897\n",
      "  batch 500 loss: 0.6170778049745035\n",
      "  batch 600 loss: 0.6126343326914917\n",
      "  batch 700 loss: 0.6158030015142628\n",
      "  batch 800 loss: 0.6311473342296483\n",
      "  batch 900 loss: 0.5964315407097218\n",
      "  batch 1000 loss: 0.6337248323205994\n",
      "  batch 1094 loss: 0.6142559914616934\n",
      "LOSS train 0.6142559914616934 valid 0.6033893608346986\n",
      "MAE train 8.6953266558077 valid 8.615252359711567\n",
      "EPOCH 33:\n",
      "  batch 100 loss: 0.6091488622752849\n",
      "  batch 200 loss: 0.6132743565763816\n",
      "  batch 300 loss: 0.6116625788596385\n",
      "  batch 400 loss: 0.6197106018726056\n",
      "  batch 500 loss: 0.6143948923242594\n",
      "  batch 600 loss: 0.622493608581791\n",
      "  batch 700 loss: 0.5994647647173018\n",
      "  batch 800 loss: 0.6231889303580666\n",
      "  batch 900 loss: 0.6045766600018176\n",
      "  batch 1000 loss: 0.6137071163818996\n",
      "  batch 1094 loss: 0.6119698663418707\n",
      "LOSS train 0.6119698663418707 valid 0.6021035046427788\n",
      "MAE train 8.739321305873528 valid 8.613235445076752\n",
      "EPOCH 34:\n",
      "  batch 100 loss: 0.613331755297353\n",
      "  batch 200 loss: 0.6054527735108273\n",
      "  batch 300 loss: 0.6169017532902324\n",
      "  batch 400 loss: 0.6009795009490603\n",
      "  batch 500 loss: 0.5978569280937215\n",
      "  batch 600 loss: 0.6350411947721892\n",
      "  batch 700 loss: 0.6210863477532969\n",
      "  batch 800 loss: 0.6289831495310132\n",
      "  batch 900 loss: 0.6227131029987448\n",
      "  batch 1000 loss: 0.6126758061590829\n",
      "  batch 1094 loss: 0.5964561134765809\n",
      "LOSS train 0.5964561134765809 valid 0.6033069745163263\n",
      "MAE train 8.527292184663594 valid 8.625589301509423\n",
      "EPOCH 35:\n",
      "  batch 100 loss: 0.6086290288401589\n",
      "  batch 200 loss: 0.617227131107823\n",
      "  batch 300 loss: 0.6159739863138828\n",
      "  batch 400 loss: 0.6264669157682718\n",
      "  batch 500 loss: 0.61376962394524\n",
      "  batch 600 loss: 0.6127182216677928\n",
      "  batch 700 loss: 0.6008952965140187\n",
      "  batch 800 loss: 0.6080194598907532\n",
      "  batch 900 loss: 0.6051794465243855\n",
      "  batch 1000 loss: 0.6212385164095711\n",
      "  batch 1094 loss: 0.6141036835548038\n",
      "LOSS train 0.6141036835548038 valid 0.5979285349627251\n",
      "MAE train 8.718426387569608 valid 8.542480040409759\n",
      "EPOCH 36:\n",
      "  batch 100 loss: 0.6090778621064105\n",
      "  batch 200 loss: 0.6114814197932911\n",
      "  batch 300 loss: 0.6132746492982936\n",
      "  batch 400 loss: 0.6097342234887876\n",
      "  batch 500 loss: 0.620187335452058\n",
      "  batch 600 loss: 0.6063509842998834\n",
      "  batch 700 loss: 0.6160696919351765\n",
      "  batch 800 loss: 0.607386681563746\n",
      "  batch 900 loss: 0.6242622437428015\n",
      "  batch 1000 loss: 0.611940970967527\n",
      "  batch 1094 loss: 0.6111119726505954\n",
      "LOSS train 0.6111119726505954 valid 0.6012496658652096\n",
      "MAE train 8.697401330854987 valid 8.582242026584877\n",
      "EPOCH 37:\n",
      "  batch 100 loss: 0.6180697566722263\n",
      "  batch 200 loss: 0.6244186865551713\n",
      "  batch 300 loss: 0.6191668346722136\n",
      "  batch 400 loss: 0.6129567104028532\n",
      "  batch 500 loss: 0.6202706685256097\n",
      "  batch 600 loss: 0.6093647679979401\n",
      "  batch 700 loss: 0.6093712483830278\n",
      "  batch 800 loss: 0.6107956207868589\n",
      "  batch 900 loss: 0.5955398329204963\n",
      "  batch 1000 loss: 0.618960518938383\n",
      "  batch 1094 loss: 0.5929916883540227\n",
      "LOSS train 0.5929916883540227 valid 0.6025083470919202\n",
      "MAE train 8.565778833194832 valid 8.593288750133594\n",
      "EPOCH 38:\n",
      "  batch 100 loss: 0.6214406767574061\n",
      "  batch 200 loss: 0.6045159900184636\n",
      "  batch 300 loss: 0.6188641553325749\n",
      "  batch 400 loss: 0.6064877847981139\n",
      "  batch 500 loss: 0.6003269183692659\n",
      "  batch 600 loss: 0.607093537676644\n",
      "  batch 700 loss: 0.6029913515401429\n",
      "  batch 800 loss: 0.6307473233102527\n",
      "  batch 900 loss: 0.6075886613557695\n",
      "  batch 1000 loss: 0.6121818290009458\n",
      "  batch 1094 loss: 0.6175736056772257\n",
      "LOSS train 0.6175736056772257 valid 0.6004201244916986\n",
      "MAE train 8.717340435007657 valid 8.577506890224777\n",
      "EPOCH 39:\n",
      "  batch 100 loss: 0.6071114014400633\n",
      "  batch 200 loss: 0.6118803550278569\n",
      "  batch 300 loss: 0.6237464035936141\n",
      "  batch 400 loss: 0.6278612267329287\n",
      "  batch 500 loss: 0.6248367196665073\n",
      "  batch 600 loss: 0.6140510657358459\n",
      "  batch 700 loss: 0.6065250838028029\n",
      "  batch 800 loss: 0.6118171531432196\n",
      "  batch 900 loss: 0.6080474170420604\n",
      "  batch 1000 loss: 0.6037733087133337\n",
      "  batch 1094 loss: 0.6015300748898311\n",
      "LOSS train 0.6015300748898311 valid 0.6001431507981987\n",
      "MAE train 8.573381126350943 valid 8.562642000101134\n",
      "EPOCH 40:\n",
      "  batch 100 loss: 0.593120231819077\n",
      "  batch 200 loss: 0.6204789969499217\n",
      "  batch 300 loss: 0.6031904298910665\n",
      "  batch 400 loss: 0.6105087440432237\n",
      "  batch 500 loss: 0.6085921799219589\n",
      "  batch 600 loss: 0.6196032493921446\n",
      "  batch 700 loss: 0.5984557181365789\n",
      "  batch 800 loss: 0.6166707511449319\n",
      "  batch 900 loss: 0.6160598224404975\n",
      "  batch 1000 loss: 0.615192611004949\n",
      "  batch 1094 loss: 0.6221634380930492\n",
      "LOSS train 0.6221634380930492 valid 0.6001274839543747\n",
      "MAE train 8.717847854013453 valid 8.581687514229285\n",
      "EPOCH 41:\n",
      "  batch 100 loss: 0.6212160927455187\n",
      "  batch 200 loss: 0.6082938876005621\n",
      "  batch 300 loss: 0.6129183165682557\n",
      "  batch 400 loss: 0.6018376657986942\n",
      "  batch 500 loss: 0.6156340420369475\n",
      "  batch 600 loss: 0.6167005908102463\n",
      "  batch 700 loss: 0.6097028694089618\n",
      "  batch 800 loss: 0.6033202062397237\n",
      "  batch 900 loss: 0.6017174455050419\n",
      "  batch 1000 loss: 0.6107376579655892\n",
      "  batch 1094 loss: 0.6191708136706678\n",
      "LOSS train 0.6191708136706678 valid 0.6022970620807525\n",
      "MAE train 8.780434490160744 valid 8.599308533174224\n",
      "EPOCH 42:\n",
      "  batch 100 loss: 0.6165590884552976\n",
      "  batch 200 loss: 0.6230609175287755\n",
      "  batch 300 loss: 0.6123579795346199\n",
      "  batch 400 loss: 0.6050328786003282\n",
      "  batch 500 loss: 0.6050591170745395\n",
      "  batch 600 loss: 0.6193942987025467\n",
      "  batch 700 loss: 0.6083817203605905\n",
      "  batch 800 loss: 0.6022750078517235\n",
      "  batch 900 loss: 0.6103665195256361\n",
      "  batch 1000 loss: 0.6085304337071454\n",
      "  batch 1094 loss: 0.6140149108772172\n",
      "LOSS train 0.6140149108772172 valid 0.6120731161007289\n",
      "MAE train 8.72144783271835 valid 8.737407028769027\n",
      "EPOCH 43:\n",
      "  batch 100 loss: 0.6276706826372339\n",
      "  batch 200 loss: 0.6152446679984807\n",
      "  batch 300 loss: 0.6179472689972857\n",
      "  batch 400 loss: 0.6046798561279414\n",
      "  batch 500 loss: 0.6242540381186535\n",
      "  batch 600 loss: 0.5956799965364334\n",
      "  batch 700 loss: 0.6019175088371438\n",
      "  batch 800 loss: 0.6071674907592006\n",
      "  batch 900 loss: 0.608423484624075\n",
      "  batch 1000 loss: 0.6167421113845312\n",
      "  batch 1094 loss: 0.6120203546570617\n",
      "LOSS train 0.6120203546570617 valid 0.6027987363596221\n",
      "MAE train 8.663188332197086 valid 8.608346179082618\n",
      "EPOCH 44:\n",
      "  batch 100 loss: 0.6177335122466696\n",
      "  batch 200 loss: 0.6096650472568862\n",
      "  batch 300 loss: 0.5994529875256636\n",
      "  batch 400 loss: 0.6205038214370147\n",
      "  batch 500 loss: 0.6015651183854448\n",
      "  batch 600 loss: 0.6083211152430366\n",
      "  batch 700 loss: 0.6196740545435191\n",
      "  batch 800 loss: 0.6000596609902794\n",
      "  batch 900 loss: 0.610633875825593\n",
      "  batch 1000 loss: 0.6312861370967686\n",
      "  batch 1094 loss: 0.6085500215461878\n",
      "LOSS train 0.6085500215461878 valid 0.604362919063989\n",
      "MAE train 8.650297899264546 valid 8.654421024263234\n",
      "EPOCH 45:\n",
      "  batch 100 loss: 0.6109130099985022\n",
      "  batch 200 loss: 0.5956423285621991\n",
      "  batch 300 loss: 0.5977836945099986\n",
      "  batch 400 loss: 0.6189766562428118\n",
      "  batch 500 loss: 0.6082663091909752\n",
      "  batch 600 loss: 0.5931271316772944\n",
      "  batch 700 loss: 0.6095280292751318\n",
      "  batch 800 loss: 0.6243746809296049\n",
      "  batch 900 loss: 0.6102319155602216\n",
      "  batch 1000 loss: 0.6097115133124106\n",
      "  batch 1094 loss: 0.6358866613228888\n",
      "LOSS train 0.6358866613228888 valid 0.6113139910970005\n",
      "MAE train 8.893779548908237 valid 8.725153079783427\n",
      "EPOCH 46:\n",
      "  batch 100 loss: 0.6193885376715597\n",
      "  batch 200 loss: 0.6163908665946722\n",
      "  batch 300 loss: 0.5893613933236435\n",
      "  batch 400 loss: 0.5991028027235642\n",
      "  batch 500 loss: 0.6265987734449234\n",
      "  batch 600 loss: 0.5835027779630995\n",
      "  batch 700 loss: 0.6159218307890765\n",
      "  batch 800 loss: 0.6196442024385487\n",
      "  batch 900 loss: 0.620512893595045\n",
      "  batch 1000 loss: 0.6107615665647813\n",
      "  batch 1094 loss: 0.6107485739846078\n",
      "LOSS train 0.6107485739846078 valid 0.5970669475070189\n",
      "MAE train 8.653210787304937 valid 8.51541157388604\n",
      "EPOCH 47:\n",
      "  batch 100 loss: 0.609382567553437\n",
      "  batch 200 loss: 0.619377859959132\n",
      "  batch 300 loss: 0.6091508397395148\n",
      "  batch 400 loss: 0.6050626843935965\n",
      "  batch 500 loss: 0.5981236225941521\n",
      "  batch 600 loss: 0.6173403291504047\n",
      "  batch 700 loss: 0.6231906895607304\n",
      "  batch 800 loss: 0.6045714073861269\n",
      "  batch 900 loss: 0.612285557186445\n",
      "  batch 1000 loss: 0.6226005373089551\n",
      "  batch 1094 loss: 0.6021989049937354\n",
      "LOSS train 0.6021989049937354 valid 0.6007665745461694\n",
      "MAE train 8.607955843363264 valid 8.58167986163397\n",
      "EPOCH 48:\n",
      "  batch 100 loss: 0.6200898490019707\n",
      "  batch 200 loss: 0.603751345413038\n",
      "  batch 300 loss: 0.6314637685973227\n",
      "  batch 400 loss: 0.5866856758941651\n",
      "  batch 500 loss: 0.6109884206929476\n",
      "  batch 600 loss: 0.6091102437055945\n",
      "  batch 700 loss: 0.6250434118286328\n",
      "  batch 800 loss: 0.6155588950465505\n",
      "  batch 900 loss: 0.6040983481404467\n",
      "  batch 1000 loss: 0.6002545408687215\n",
      "  batch 1094 loss: 0.604114601080156\n",
      "LOSS train 0.604114601080156 valid 0.6077680197115767\n",
      "MAE train 8.583406738538924 valid 8.676192460018088\n",
      "EPOCH 49:\n",
      "  batch 100 loss: 0.6142576475950474\n",
      "  batch 200 loss: 0.6147025193425079\n",
      "  batch 300 loss: 0.613683067103686\n",
      "  batch 400 loss: 0.6070089329532672\n",
      "  batch 500 loss: 0.6117237252747344\n",
      "  batch 600 loss: 0.6145764712242717\n",
      "  batch 700 loss: 0.6065860903405549\n",
      "  batch 800 loss: 0.6102801564121154\n",
      "  batch 900 loss: 0.6091548672054135\n",
      "  batch 1000 loss: 0.6012947819265125\n",
      "  batch 1094 loss: 0.6087527284693186\n",
      "LOSS train 0.6087527284693186 valid 0.6064989831334332\n",
      "MAE train 8.666849569670832 valid 8.694273926238415\n",
      "EPOCH 50:\n",
      "  batch 100 loss: 0.6209869665707207\n",
      "  batch 200 loss: 0.5990927795435573\n",
      "  batch 300 loss: 0.6047124504456549\n",
      "  batch 400 loss: 0.6131428356022411\n",
      "  batch 500 loss: 0.6205878403478874\n",
      "  batch 600 loss: 0.5950768127535678\n",
      "  batch 700 loss: 0.6072314274778184\n",
      "  batch 800 loss: 0.6178905444701395\n",
      "  batch 900 loss: 0.6096305725422001\n",
      "  batch 1000 loss: 0.6088280924153233\n",
      "  batch 1094 loss: 0.6071015608128789\n",
      "LOSS train 0.6071015608128789 valid 0.598427952815983\n",
      "MAE train 8.671001831003565 valid 8.544269470609422\n",
      "EPOCH 51:\n",
      "  batch 100 loss: 0.59743197487158\n",
      "  batch 200 loss: 0.6164805776544658\n",
      "  batch 300 loss: 0.6121885441980264\n",
      "  batch 400 loss: 0.6182280703180482\n",
      "  batch 500 loss: 0.5956217062782989\n",
      "  batch 600 loss: 0.6068786074341068\n",
      "  batch 700 loss: 0.6132081161284642\n",
      "  batch 800 loss: 0.6101032834440692\n",
      "  batch 900 loss: 0.6107833029245328\n",
      "  batch 1000 loss: 0.6129537918524398\n",
      "  batch 1094 loss: 0.6158584788003579\n",
      "LOSS train 0.6158584788003579 valid 0.5987974066836526\n",
      "MAE train 8.78343029531746 valid 8.54237023027322\n",
      "EPOCH 52:\n",
      "  batch 100 loss: 0.6033560949576493\n",
      "  batch 200 loss: 0.6147948196237539\n",
      "  batch 300 loss: 0.5898657848634281\n",
      "  batch 400 loss: 0.5873420834168376\n",
      "  batch 500 loss: 0.6213489623374923\n",
      "  batch 600 loss: 0.6159941945273277\n",
      "  batch 700 loss: 0.6141684059873688\n",
      "  batch 800 loss: 0.6082965707381423\n",
      "  batch 900 loss: 0.6241274713325253\n",
      "  batch 1000 loss: 0.599918060671089\n",
      "  batch 1094 loss: 0.6259455481688467\n",
      "LOSS train 0.6259455481688467 valid 0.5992846536694943\n",
      "MAE train 8.83590937175784 valid 8.560410979617034\n",
      "EPOCH 53:\n",
      "  batch 100 loss: 0.6226140290345206\n",
      "  batch 200 loss: 0.6189900306024287\n",
      "  batch 300 loss: 0.6256522987961831\n",
      "  batch 400 loss: 0.5801739061598646\n",
      "  batch 500 loss: 0.5975704750839066\n",
      "  batch 600 loss: 0.6239083149218445\n",
      "  batch 700 loss: 0.6120575377486343\n",
      "  batch 800 loss: 0.6137422092060554\n",
      "  batch 900 loss: 0.6044378138778221\n",
      "  batch 1000 loss: 0.6203577549943854\n",
      "  batch 1094 loss: 0.5936210420879278\n",
      "LOSS train 0.5936210420879278 valid 0.6002011453036835\n",
      "MAE train 8.533698687340978 valid 8.607508454912617\n",
      "EPOCH 54:\n",
      "  batch 100 loss: 0.59375699178872\n",
      "  batch 200 loss: 0.6212274094432321\n",
      "  batch 300 loss: 0.5892222095882064\n",
      "  batch 400 loss: 0.6193549269858506\n",
      "  batch 500 loss: 0.6192223073614366\n",
      "  batch 600 loss: 0.6102636594619942\n",
      "  batch 700 loss: 0.5999150237677694\n",
      "  batch 800 loss: 0.6171473449735089\n",
      "  batch 900 loss: 0.6195717798410696\n",
      "  batch 1000 loss: 0.6086362755169483\n",
      "  batch 1094 loss: 0.6089565317648872\n",
      "LOSS train 0.6089565317648872 valid 0.6082171614274718\n",
      "MAE train 8.669294692430185 valid 8.650146959563711\n",
      "EPOCH 55:\n",
      "  batch 100 loss: 0.6145193337822823\n",
      "  batch 200 loss: 0.6174647494258386\n",
      "  batch 300 loss: 0.5995399057459516\n",
      "  batch 400 loss: 0.5976974771313285\n",
      "  batch 500 loss: 0.6202731276117371\n",
      "  batch 600 loss: 0.6145632106629033\n",
      "  batch 700 loss: 0.6001670234838691\n",
      "  batch 800 loss: 0.6104577520510641\n",
      "  batch 900 loss: 0.6090722534879142\n",
      "  batch 1000 loss: 0.6065947003629999\n",
      "  batch 1094 loss: 0.6245420930083087\n",
      "LOSS train 0.6245420930083087 valid 0.5982688877777899\n",
      "MAE train 8.797613270630494 valid 8.525217294014203\n",
      "EPOCH 56:\n",
      "  batch 100 loss: 0.6034183735143323\n",
      "  batch 200 loss: 0.6379103533289756\n",
      "  batch 300 loss: 0.6016256041085853\n",
      "  batch 400 loss: 0.6047094241956438\n",
      "  batch 500 loss: 0.6140205297286225\n",
      "  batch 600 loss: 0.6180824713007849\n",
      "  batch 700 loss: 0.5944340682503593\n",
      "  batch 800 loss: 0.5960932379645538\n",
      "  batch 900 loss: 0.6017974752121712\n",
      "  batch 1000 loss: 0.6211421804530222\n",
      "  batch 1094 loss: 0.6079339515814699\n",
      "LOSS train 0.6079339515814699 valid 0.6093424902336817\n",
      "MAE train 8.639256108560613 valid 8.683612352618258\n",
      "EPOCH 57:\n",
      "  batch 100 loss: 0.6144865968420341\n",
      "  batch 200 loss: 0.58844870650573\n",
      "  batch 300 loss: 0.592626172308967\n",
      "  batch 400 loss: 0.5989168528005003\n",
      "  batch 500 loss: 0.6131351453406725\n",
      "  batch 600 loss: 0.6154505507901841\n",
      "  batch 700 loss: 0.6178359149023911\n",
      "  batch 800 loss: 0.6157704920201413\n",
      "  batch 900 loss: 0.621202908420524\n",
      "  batch 1000 loss: 0.6151721253406882\n",
      "  batch 1094 loss: 0.6063470134343086\n",
      "LOSS train 0.6063470134343086 valid 0.5981875783447533\n",
      "MAE train 8.672851541521242 valid 8.579680411019242\n",
      "EPOCH 58:\n",
      "  batch 100 loss: 0.6090123495694011\n",
      "  batch 200 loss: 0.6181583198291778\n",
      "  batch 300 loss: 0.6012875089494323\n",
      "  batch 400 loss: 0.5982710793357335\n",
      "  batch 500 loss: 0.6304951208809454\n",
      "  batch 600 loss: 0.6095453937800571\n",
      "  batch 700 loss: 0.6000925820190731\n",
      "  batch 800 loss: 0.6168156584916179\n",
      "  batch 900 loss: 0.6211936629430375\n",
      "  batch 1000 loss: 0.6094947557504936\n",
      "  batch 1094 loss: 0.598478304523707\n",
      "LOSS train 0.598478304523707 valid 0.6076077271442281\n",
      "MAE train 8.56364994983925 valid 8.69057236261415\n",
      "EPOCH 59:\n",
      "  batch 100 loss: 0.6263891490416167\n",
      "  batch 200 loss: 0.6215173209746503\n",
      "  batch 300 loss: 0.6160231044307245\n",
      "  batch 400 loss: 0.6018790127974449\n",
      "  batch 500 loss: 0.5948502107298004\n",
      "  batch 600 loss: 0.6178514231649368\n",
      "  batch 700 loss: 0.6062424748728202\n",
      "  batch 800 loss: 0.590150604739259\n",
      "  batch 900 loss: 0.6168544223424024\n",
      "  batch 1000 loss: 0.6144544872174712\n",
      "  batch 1094 loss: 0.6093416490036673\n",
      "LOSS train 0.6093416490036673 valid 0.5986135145327929\n",
      "MAE train 8.66731964276632 valid 8.552248230671346\n",
      "EPOCH 60:\n",
      "  batch 100 loss: 0.6049419290004642\n",
      "  batch 200 loss: 0.5933061085492565\n",
      "  batch 300 loss: 0.614868521148645\n",
      "  batch 400 loss: 0.602459838914846\n",
      "  batch 500 loss: 0.6202710455568751\n",
      "  batch 600 loss: 0.6103926196393903\n",
      "  batch 700 loss: 0.6010472949563932\n",
      "  batch 800 loss: 0.6362330677576611\n",
      "  batch 900 loss: 0.6133051824289635\n",
      "  batch 1000 loss: 0.610963490478784\n",
      "  batch 1094 loss: 0.5928157013234311\n",
      "LOSS train 0.5928157013234311 valid 0.5973239527689203\n",
      "MAE train 8.466440396510452 valid 8.524337795514795\n",
      "EPOCH 61:\n",
      "  batch 100 loss: 0.6155460890584937\n",
      "  batch 200 loss: 0.6145078035199\n",
      "  batch 300 loss: 0.5973852479236047\n",
      "  batch 400 loss: 0.6171751580253717\n",
      "  batch 500 loss: 0.6028916091186051\n",
      "  batch 600 loss: 0.6069234325705141\n",
      "  batch 700 loss: 0.607265150872389\n",
      "  batch 800 loss: 0.6052755698817635\n",
      "  batch 900 loss: 0.6050429745511434\n",
      "  batch 1000 loss: 0.6260913230496352\n",
      "  batch 1094 loss: 0.5953389373972883\n",
      "LOSS train 0.5953389373972883 valid 0.605393075653137\n",
      "MAE train 8.549121563483252 valid 8.664935146653349\n",
      "EPOCH 62:\n",
      "  batch 100 loss: 0.628654592818937\n",
      "  batch 200 loss: 0.5950245171163148\n",
      "  batch 300 loss: 0.6167452823940528\n",
      "  batch 400 loss: 0.6326982781390924\n",
      "  batch 500 loss: 0.596180762663413\n",
      "  batch 600 loss: 0.6004026428610737\n",
      "  batch 700 loss: 0.601612068497636\n",
      "  batch 800 loss: 0.5947501423096042\n",
      "  batch 900 loss: 0.5918361254703076\n",
      "  batch 1000 loss: 0.633717502727498\n",
      "  batch 1094 loss: 0.6145739455930597\n",
      "LOSS train 0.6145739455930597 valid 0.5983245114018877\n",
      "MAE train 8.693862625625398 valid 8.556996423428592\n",
      "EPOCH 63:\n",
      "  batch 100 loss: 0.6056162238005653\n",
      "  batch 200 loss: 0.607638370413036\n",
      "  batch 300 loss: 0.5975866354952935\n",
      "  batch 400 loss: 0.6222301708027717\n",
      "  batch 500 loss: 0.6173330261942095\n",
      "  batch 600 loss: 0.6214393735709931\n",
      "  batch 700 loss: 0.5965293841347066\n",
      "  batch 800 loss: 0.6122717341601028\n",
      "  batch 900 loss: 0.6104520434909815\n",
      "  batch 1000 loss: 0.6142271301357547\n",
      "  batch 1094 loss: 0.5918745198252725\n",
      "LOSS train 0.5918745198252725 valid 0.6015153711289328\n",
      "MAE train 8.454439828132523 valid 8.604663034429818\n",
      "EPOCH 64:\n",
      "  batch 100 loss: 0.6066480636658493\n",
      "  batch 200 loss: 0.5926252184078298\n",
      "  batch 300 loss: 0.6082774664945488\n",
      "  batch 400 loss: 0.6165658046232418\n",
      "  batch 500 loss: 0.605244513452198\n",
      "  batch 600 loss: 0.6082655279063175\n",
      "  batch 700 loss: 0.6110700258534948\n",
      "  batch 800 loss: 0.6105875749462077\n",
      "  batch 900 loss: 0.6001237431097514\n",
      "  batch 1000 loss: 0.6065601507477821\n",
      "  batch 1094 loss: 0.6194749927084763\n",
      "LOSS train 0.6194749927084763 valid 0.5968868458662813\n",
      "MAE train 8.729462335987998 valid 8.527453905621941\n",
      "EPOCH 65:\n",
      "  batch 100 loss: 0.621701511831386\n",
      "  batch 200 loss: 0.6190610563816362\n",
      "  batch 300 loss: 0.6131997719067235\n",
      "  batch 400 loss: 0.6008382820339991\n",
      "  batch 500 loss: 0.6012930779571405\n",
      "  batch 600 loss: 0.6041767692096234\n",
      "  batch 700 loss: 0.5942120466215459\n",
      "  batch 800 loss: 0.610046003616332\n",
      "  batch 900 loss: 0.6182242249137819\n",
      "  batch 1000 loss: 0.6075708635167698\n",
      "  batch 1094 loss: 0.6102087075060811\n",
      "LOSS train 0.6102087075060811 valid 0.5985107115674201\n",
      "MAE train 8.69159348691053 valid 8.563901960931673\n",
      "EPOCH 66:\n",
      "  batch 100 loss: 0.6122486033747654\n",
      "  batch 200 loss: 0.5864908163040735\n",
      "  batch 300 loss: 0.6143626286603401\n",
      "  batch 400 loss: 0.6112043149340733\n",
      "  batch 500 loss: 0.6100017421161161\n",
      "  batch 600 loss: 0.618197564859993\n",
      "  batch 700 loss: 0.6125725341401814\n",
      "  batch 800 loss: 0.6204539016733737\n",
      "  batch 900 loss: 0.5945792007197074\n",
      "  batch 1000 loss: 0.6003978086320535\n",
      "  batch 1094 loss: 0.6077921227762448\n",
      "LOSS train 0.6077921227762448 valid 0.6007188414287224\n",
      "MAE train 8.69352367048578 valid 8.590170257837531\n",
      "EPOCH 67:\n",
      "  batch 100 loss: 0.6069409867136603\n",
      "  batch 200 loss: 0.606932887516621\n",
      "  batch 300 loss: 0.6028997383118238\n",
      "  batch 400 loss: 0.6157562473112732\n",
      "  batch 500 loss: 0.6080657319217179\n",
      "  batch 600 loss: 0.613212021996977\n",
      "  batch 700 loss: 0.6018223637030963\n",
      "  batch 800 loss: 0.6103446146526157\n",
      "  batch 900 loss: 0.6117714441451659\n",
      "  batch 1000 loss: 0.6031983182952024\n",
      "  batch 1094 loss: 0.6173104918590031\n",
      "LOSS train 0.6173104918590031 valid 0.6009662435639403\n",
      "MAE train 8.67315954349665 valid 8.594441025699961\n",
      "EPOCH 68:\n",
      "  batch 100 loss: 0.6120038877536538\n",
      "  batch 200 loss: 0.6097298459817501\n",
      "  batch 300 loss: 0.6248359017302356\n",
      "  batch 400 loss: 0.634383323639729\n",
      "  batch 500 loss: 0.5955860674695193\n",
      "  batch 600 loss: 0.5948412943486526\n",
      "  batch 700 loss: 0.6075346547472741\n",
      "  batch 800 loss: 0.5998458502830769\n",
      "  batch 900 loss: 0.6051268786968006\n",
      "  batch 1000 loss: 0.6011027574440763\n",
      "  batch 1094 loss: 0.6124586083485705\n",
      "LOSS train 0.6124586083485705 valid 0.5994151301170058\n",
      "MAE train 8.695018888686931 valid 8.55152840612708\n",
      "EPOCH 69:\n",
      "  batch 100 loss: 0.6076842008423057\n",
      "  batch 200 loss: 0.6073941298502225\n",
      "  batch 300 loss: 0.6111505442635522\n",
      "  batch 400 loss: 0.6214100033566189\n",
      "  batch 500 loss: 0.6212464215710597\n",
      "  batch 600 loss: 0.6159387462232447\n",
      "  batch 700 loss: 0.5958500388135143\n",
      "  batch 800 loss: 0.6089191294965798\n",
      "  batch 900 loss: 0.6121315138261471\n",
      "  batch 1000 loss: 0.6017437514614147\n",
      "  batch 1094 loss: 0.6027392977954252\n",
      "LOSS train 0.6027392977954252 valid 0.6046375762622911\n",
      "MAE train 8.591296462741616 valid 8.627885219892784\n",
      "EPOCH 70:\n",
      "  batch 100 loss: 0.6157856517715009\n",
      "  batch 200 loss: 0.6081064717689305\n",
      "  batch 300 loss: 0.6130256630136762\n",
      "  batch 400 loss: 0.6058109298293598\n",
      "  batch 500 loss: 0.590631043839389\n",
      "  batch 600 loss: 0.5962482160865422\n",
      "  batch 700 loss: 0.6215712928156502\n",
      "  batch 800 loss: 0.6131734224372744\n",
      "  batch 900 loss: 0.6214875915767398\n",
      "  batch 1000 loss: 0.6063604417620886\n",
      "  batch 1094 loss: 0.601943985905157\n",
      "LOSS train 0.601943985905157 valid 0.6213483542389538\n",
      "MAE train 8.606310755680878 valid 8.891771248991363\n",
      "EPOCH 71:\n",
      "  batch 100 loss: 0.6155573860982675\n",
      "  batch 200 loss: 0.6010960934310795\n",
      "  batch 300 loss: 0.5910046006533269\n",
      "  batch 400 loss: 0.6015310397862347\n",
      "  batch 500 loss: 0.608353834788799\n",
      "  batch 600 loss: 0.5961671095346417\n",
      "  batch 700 loss: 0.618181798574287\n",
      "  batch 800 loss: 0.6171470940554049\n",
      "  batch 900 loss: 0.6187810947365915\n",
      "  batch 1000 loss: 0.6052036604307769\n",
      "  batch 1094 loss: 0.6169020649989192\n",
      "LOSS train 0.6169020649989192 valid 0.5955031689794517\n",
      "MAE train 8.69492992717326 valid 8.496043515244537\n",
      "EPOCH 72:\n",
      "  batch 100 loss: 0.6118150078733737\n",
      "  batch 200 loss: 0.6018513985394988\n",
      "  batch 300 loss: 0.6142507903121327\n",
      "  batch 400 loss: 0.6064160155017759\n",
      "  batch 500 loss: 0.599987891045384\n",
      "  batch 600 loss: 0.6187603111048025\n",
      "  batch 700 loss: 0.6106784784158747\n",
      "  batch 800 loss: 0.6039449692822503\n",
      "  batch 900 loss: 0.6116941154219693\n",
      "  batch 1000 loss: 0.603795740591553\n",
      "  batch 1094 loss: 0.6006288327537025\n",
      "LOSS train 0.6006288327537025 valid 0.5985648029133095\n",
      "MAE train 8.596169722935148 valid 8.52581523688628\n",
      "EPOCH 73:\n",
      "  batch 100 loss: 0.6065971405834188\n",
      "  batch 200 loss: 0.6174763346559246\n",
      "  batch 300 loss: 0.59901528910055\n",
      "  batch 400 loss: 0.6201549667098347\n",
      "  batch 500 loss: 0.618183357830751\n",
      "  batch 600 loss: 0.5985256772923716\n",
      "  batch 700 loss: 0.5997698349142427\n",
      "  batch 800 loss: 0.6163170600634976\n",
      "  batch 900 loss: 0.6009001671155418\n",
      "  batch 1000 loss: 0.6017770971700539\n",
      "  batch 1094 loss: 0.612103076971259\n",
      "LOSS train 0.612103076971259 valid 0.6172538483649634\n",
      "MAE train 8.689227239728696 valid 8.804907679275383\n",
      "EPOCH 74:\n",
      "  batch 100 loss: 0.6117771216782915\n",
      "  batch 200 loss: 0.597643465815808\n",
      "  batch 300 loss: 0.6221856531998738\n",
      "  batch 400 loss: 0.6124627086331725\n",
      "  batch 500 loss: 0.6067073430844372\n",
      "  batch 600 loss: 0.6041264723441349\n",
      "  batch 700 loss: 0.6033280434032539\n",
      "  batch 800 loss: 0.6003460525502633\n",
      "  batch 900 loss: 0.6039565324861144\n",
      "  batch 1000 loss: 0.6180473399200401\n",
      "  batch 1094 loss: 0.6135409214967442\n",
      "LOSS train 0.6135409214967442 valid 0.6014569331240104\n",
      "MAE train 8.708145935926199 valid 8.594529174162775\n",
      "EPOCH 75:\n",
      "  batch 100 loss: 0.6155152469904983\n",
      "  batch 200 loss: 0.6149913459092683\n",
      "  batch 300 loss: 0.6075197549101267\n",
      "  batch 400 loss: 0.5983347498668575\n",
      "  batch 500 loss: 0.6026629815505343\n",
      "  batch 600 loss: 0.6058598900094823\n",
      "  batch 700 loss: 0.6073428639256909\n",
      "  batch 800 loss: 0.6061161410012962\n",
      "  batch 900 loss: 0.6262509102659213\n",
      "  batch 1000 loss: 0.5971572670923494\n",
      "  batch 1094 loss: 0.5998648775690042\n",
      "LOSS train 0.5998648775690042 valid 0.5993504452597871\n",
      "MAE train 8.585985312913058 valid 8.560387349696878\n",
      "EPOCH 76:\n",
      "  batch 100 loss: 0.6086993459179983\n",
      "  batch 200 loss: 0.6141702039785755\n",
      "  batch 300 loss: 0.607963718490108\n",
      "  batch 400 loss: 0.6293692575920167\n",
      "  batch 500 loss: 0.6024838761982989\n",
      "  batch 600 loss: 0.6098002257821701\n",
      "  batch 700 loss: 0.6045299932943647\n",
      "  batch 800 loss: 0.590900000874583\n",
      "  batch 900 loss: 0.6017975893486822\n",
      "  batch 1000 loss: 0.6252591295912815\n",
      "  batch 1094 loss: 0.5884588549928675\n",
      "LOSS train 0.5884588549928675 valid 0.596549845469791\n",
      "MAE train 8.461413936791862 valid 8.512275555413723\n",
      "EPOCH 77:\n",
      "  batch 100 loss: 0.593380783863084\n",
      "  batch 200 loss: 0.6032450095171545\n",
      "  batch 300 loss: 0.6109919297345687\n",
      "  batch 400 loss: 0.6196237455629404\n",
      "  batch 500 loss: 0.6356253479008922\n",
      "  batch 600 loss: 0.6101874702762083\n",
      "  batch 700 loss: 0.590434284820718\n",
      "  batch 800 loss: 0.6097857745281986\n",
      "  batch 900 loss: 0.5993030459014731\n",
      "  batch 1000 loss: 0.6168868152156204\n",
      "  batch 1094 loss: 0.588771829025925\n",
      "LOSS train 0.588771829025925 valid 0.6012066240794967\n",
      "MAE train 8.466076806681516 valid 8.563582975714246\n",
      "EPOCH 78:\n",
      "  batch 100 loss: 0.6367246965386383\n",
      "  batch 200 loss: 0.6002499239818352\n",
      "  batch 300 loss: 0.613899051786414\n",
      "  batch 400 loss: 0.6115635395491764\n",
      "  batch 500 loss: 0.5837645549055587\n",
      "  batch 600 loss: 0.5948006319664144\n",
      "  batch 700 loss: 0.6131160026266689\n",
      "  batch 800 loss: 0.6072384826067863\n",
      "  batch 900 loss: 0.6037173280585716\n",
      "  batch 1000 loss: 0.6094700526841683\n",
      "  batch 1094 loss: 0.6134856465616506\n",
      "LOSS train 0.6134856465616506 valid 0.5988999557968463\n",
      "MAE train 8.728540418831216 valid 8.581275740000818\n",
      "EPOCH 79:\n",
      "  batch 100 loss: 0.5992408497054683\n",
      "  batch 200 loss: 0.5991707442526917\n",
      "  batch 300 loss: 0.6108369751681549\n",
      "  batch 400 loss: 0.6061479744097752\n",
      "  batch 500 loss: 0.6132212373356506\n",
      "  batch 600 loss: 0.5992241404041281\n",
      "  batch 700 loss: 0.6083108706990498\n",
      "  batch 800 loss: 0.606196517479973\n",
      "  batch 900 loss: 0.6170306957722957\n",
      "  batch 1000 loss: 0.6140560968933728\n",
      "  batch 1094 loss: 0.6170513675647618\n",
      "LOSS train 0.6170513675647618 valid 0.6035349170414552\n",
      "MAE train 8.692764014506727 valid 8.646000871706903\n",
      "EPOCH 80:\n",
      "  batch 100 loss: 0.6223474688493258\n",
      "  batch 200 loss: 0.6199420293952923\n",
      "  batch 300 loss: 0.5998037374663583\n",
      "  batch 400 loss: 0.6127034768884824\n",
      "  batch 500 loss: 0.6079287420474125\n",
      "  batch 600 loss: 0.5931789304115441\n",
      "  batch 700 loss: 0.595087067227916\n",
      "  batch 800 loss: 0.5945749168141903\n",
      "  batch 900 loss: 0.6087022134997746\n",
      "  batch 1000 loss: 0.6105066264251513\n",
      "  batch 1094 loss: 0.614050244791401\n",
      "LOSS train 0.614050244791401 valid 0.5991153557078778\n",
      "MAE train 8.673868882555583 valid 8.581607012141468\n",
      "EPOCH 81:\n",
      "  batch 100 loss: 0.6070204195004165\n",
      "  batch 200 loss: 0.5939212274137118\n",
      "  batch 300 loss: 0.5996178386548437\n",
      "  batch 400 loss: 0.6169254673055516\n",
      "  batch 500 loss: 0.6305572569791918\n",
      "  batch 600 loss: 0.6092971775456764\n",
      "  batch 700 loss: 0.6013444442010685\n",
      "  batch 800 loss: 0.6092574475627486\n",
      "  batch 900 loss: 0.6233350554727429\n",
      "  batch 1000 loss: 0.5912279376483766\n",
      "  batch 1094 loss: 0.6038732408601348\n",
      "LOSS train 0.6038732408601348 valid 0.6023145825059705\n",
      "MAE train 8.618483510162536 valid 8.639728587476124\n",
      "EPOCH 82:\n",
      "  batch 100 loss: 0.6127140895761475\n",
      "  batch 200 loss: 0.6146722805860617\n",
      "  batch 300 loss: 0.6101324485436148\n",
      "  batch 400 loss: 0.60916922726673\n",
      "  batch 500 loss: 0.6126245807430324\n",
      "  batch 600 loss: 0.599840962940758\n",
      "  batch 700 loss: 0.6015290171306242\n",
      "  batch 800 loss: 0.6068076282290462\n",
      "  batch 900 loss: 0.6117136563963467\n",
      "  batch 1000 loss: 0.6050126257425602\n",
      "  batch 1094 loss: 0.5989248988738449\n",
      "LOSS train 0.5989248988738449 valid 0.6001709139131053\n",
      "MAE train 8.527943863730668 valid 8.577992467192157\n",
      "EPOCH 83:\n",
      "  batch 100 loss: 0.6338102321801738\n",
      "  batch 200 loss: 0.6030012990008725\n",
      "  batch 300 loss: 0.6152006293215116\n",
      "  batch 400 loss: 0.6035319825455965\n",
      "  batch 500 loss: 0.60630473395342\n",
      "  batch 600 loss: 0.6019307579705624\n",
      "  batch 700 loss: 0.615743436080558\n",
      "  batch 800 loss: 0.5790127141559814\n",
      "  batch 900 loss: 0.6087294882961946\n",
      "  batch 1000 loss: 0.5979593951815544\n",
      "  batch 1094 loss: 0.611574456816243\n",
      "LOSS train 0.611574456816243 valid 0.6021916482622834\n",
      "MAE train 8.65705292832252 valid 8.586739430695324\n",
      "EPOCH 84:\n",
      "  batch 100 loss: 0.5985330282723567\n",
      "  batch 200 loss: 0.6034155466914634\n",
      "  batch 300 loss: 0.6001493374949325\n",
      "  batch 400 loss: 0.6163180619092529\n",
      "  batch 500 loss: 0.6138011238409418\n",
      "  batch 600 loss: 0.601621270372955\n",
      "  batch 700 loss: 0.6106434306569573\n",
      "  batch 800 loss: 0.6200425200432071\n",
      "  batch 900 loss: 0.5945377494633978\n",
      "  batch 1000 loss: 0.6076610717968337\n",
      "  batch 1094 loss: 0.6050855235509631\n",
      "LOSS train 0.6050855235509631 valid 0.5979631558471693\n",
      "MAE train 8.665963153311957 valid 8.521154540741595\n",
      "EPOCH 85:\n",
      "  batch 100 loss: 0.6037952397886903\n",
      "  batch 200 loss: 0.6087084793786742\n",
      "  batch 300 loss: 0.5954843354271574\n",
      "  batch 400 loss: 0.6160723810956273\n",
      "  batch 500 loss: 0.5981009594295111\n",
      "  batch 600 loss: 0.6083807158298613\n",
      "  batch 700 loss: 0.597651748624034\n",
      "  batch 800 loss: 0.6162308319207223\n",
      "  batch 900 loss: 0.5946138265741433\n",
      "  batch 1000 loss: 0.615839042315266\n",
      "  batch 1094 loss: 0.6126619215539421\n",
      "LOSS train 0.6126619215539421 valid 0.5974260457822136\n",
      "MAE train 8.704520691182788 valid 8.540921983758723\n",
      "EPOCH 86:\n",
      "  batch 100 loss: 0.6206051482064429\n",
      "  batch 200 loss: 0.6166384282850181\n",
      "  batch 300 loss: 0.6166312121768869\n",
      "  batch 400 loss: 0.6289566290341163\n",
      "  batch 500 loss: 0.6020296552512575\n",
      "  batch 600 loss: 0.6052109778858816\n",
      "  batch 700 loss: 0.5977785542808904\n",
      "  batch 800 loss: 0.6043527300109972\n",
      "  batch 900 loss: 0.5936352395957393\n",
      "  batch 1000 loss: 0.5958210804375836\n",
      "  batch 1094 loss: 0.6050476316723203\n",
      "LOSS train 0.6050476316723203 valid 0.5956090733606958\n",
      "MAE train 8.602625606850053 valid 8.502702179663283\n",
      "EPOCH 87:\n",
      "  batch 100 loss: 0.6126052971578428\n",
      "  batch 200 loss: 0.588637331551855\n",
      "  batch 300 loss: 0.6102840426847582\n",
      "  batch 400 loss: 0.6155070801566548\n",
      "  batch 500 loss: 0.617661712258396\n",
      "  batch 600 loss: 0.596207023138146\n",
      "  batch 700 loss: 0.6136533598513373\n",
      "  batch 800 loss: 0.6010440642817724\n",
      "  batch 900 loss: 0.6118413222950223\n",
      "  batch 1000 loss: 0.6099070222248716\n",
      "  batch 1094 loss: 0.5969419017307015\n",
      "LOSS train 0.5969419017307015 valid 0.5951698757258799\n",
      "MAE train 8.537602675944923 valid 8.50273428070302\n",
      "EPOCH 88:\n",
      "  batch 100 loss: 0.6255757090113218\n",
      "  batch 200 loss: 0.6073779934692198\n",
      "  batch 300 loss: 0.6059185612389595\n",
      "  batch 400 loss: 0.6222842088714975\n",
      "  batch 500 loss: 0.5988524140355276\n",
      "  batch 600 loss: 0.6010950654460593\n",
      "  batch 700 loss: 0.6025309531756904\n",
      "  batch 800 loss: 0.615682152438278\n",
      "  batch 900 loss: 0.6026925658937311\n",
      "  batch 1000 loss: 0.5872378621122373\n",
      "  batch 1094 loss: 0.6059600273676817\n",
      "LOSS train 0.6059600273676817 valid 0.5973303043448378\n",
      "MAE train 8.580364798921993 valid 8.522925416621156\n",
      "EPOCH 89:\n",
      "  batch 100 loss: 0.6037559913093046\n",
      "  batch 200 loss: 0.632411273423233\n",
      "  batch 300 loss: 0.6140346325830567\n",
      "  batch 400 loss: 0.6004656085057039\n",
      "  batch 500 loss: 0.5946434096913582\n",
      "  batch 600 loss: 0.6039881945777313\n",
      "  batch 700 loss: 0.6218376313422557\n",
      "  batch 800 loss: 0.6038251565840055\n",
      "  batch 900 loss: 0.5941391594168394\n",
      "  batch 1000 loss: 0.6002576543225198\n",
      "  batch 1094 loss: 0.6148453755491884\n",
      "LOSS train 0.6148453755491884 valid 0.5986204035989584\n",
      "MAE train 8.671058987284816 valid 8.595448809960834\n",
      "EPOCH 90:\n",
      "  batch 100 loss: 0.6161297998078245\n",
      "  batch 200 loss: 0.5997993171370969\n",
      "  batch 300 loss: 0.5955421335059237\n",
      "  batch 400 loss: 0.5904214006756803\n",
      "  batch 500 loss: 0.5993984737437666\n",
      "  batch 600 loss: 0.6152991594836634\n",
      "  batch 700 loss: 0.6120370793212001\n",
      "  batch 800 loss: 0.6045249966051987\n",
      "  batch 900 loss: 0.6249984766325414\n",
      "  batch 1000 loss: 0.6015968505442177\n",
      "  batch 1094 loss: 0.6180743853976749\n",
      "LOSS train 0.6180743853976749 valid 0.5956186831181086\n",
      "MAE train 8.73747936858768 valid 8.496406199443346\n",
      "EPOCH 91:\n",
      "  batch 100 loss: 0.593328392946517\n",
      "  batch 200 loss: 0.6053369113005697\n",
      "  batch 300 loss: 0.5993876829332477\n",
      "  batch 400 loss: 0.6130861066864755\n",
      "  batch 500 loss: 0.6084871799901101\n",
      "  batch 600 loss: 0.6015079242897414\n",
      "  batch 700 loss: 0.6064661691024125\n",
      "  batch 800 loss: 0.6113790067757112\n",
      "  batch 900 loss: 0.6177591862496867\n",
      "  batch 1000 loss: 0.6137542087256882\n",
      "  batch 1094 loss: 0.6094837359233052\n",
      "LOSS train 0.6094837359233052 valid 0.6301885732992845\n",
      "MAE train 8.620314460560753 valid 9.01627373696632\n",
      "EPOCH 92:\n",
      "  batch 100 loss: 0.6152864703793888\n",
      "  batch 200 loss: 0.5993201184763559\n",
      "  batch 300 loss: 0.6160588227656905\n",
      "  batch 400 loss: 0.6185430080606595\n",
      "  batch 500 loss: 0.610334780287135\n",
      "  batch 600 loss: 0.5892594040563526\n",
      "  batch 700 loss: 0.6082815261976595\n",
      "  batch 800 loss: 0.6127971116884369\n",
      "  batch 900 loss: 0.6240382333181603\n",
      "  batch 1000 loss: 0.593722304165182\n",
      "  batch 1094 loss: 0.5991245595547278\n",
      "LOSS train 0.5991245595547278 valid 0.5981302256542173\n",
      "MAE train 8.54995177917397 valid 8.535317334184828\n",
      "EPOCH 93:\n",
      "  batch 100 loss: 0.5970542630888329\n",
      "  batch 200 loss: 0.6015839480094148\n",
      "  batch 300 loss: 0.6107361598578582\n",
      "  batch 400 loss: 0.5925122205215433\n",
      "  batch 500 loss: 0.608566659523996\n",
      "  batch 600 loss: 0.6051305628365129\n",
      "  batch 700 loss: 0.6056889678674429\n",
      "  batch 800 loss: 0.6191606912124046\n",
      "  batch 900 loss: 0.5984603839753538\n",
      "  batch 1000 loss: 0.6198650790832598\n",
      "  batch 1094 loss: 0.607865263213038\n",
      "LOSS train 0.607865263213038 valid 0.5954184877236064\n",
      "MAE train 8.666988650977954 valid 8.493754843242709\n",
      "EPOCH 94:\n",
      "  batch 100 loss: 0.6013298640724508\n",
      "  batch 200 loss: 0.5985592971069563\n",
      "  batch 300 loss: 0.5926275908383432\n",
      "  batch 400 loss: 0.5901971294611379\n",
      "  batch 500 loss: 0.6060395277016791\n",
      "  batch 600 loss: 0.6092092121432717\n",
      "  batch 700 loss: 0.6020412754246047\n",
      "  batch 800 loss: 0.6222030873696511\n",
      "  batch 900 loss: 0.6186099200717445\n",
      "  batch 1000 loss: 0.6063219874884026\n",
      "  batch 1094 loss: 0.622922555161691\n",
      "LOSS train 0.622922555161691 valid 0.5995016593574732\n",
      "MAE train 8.826513532359765 valid 8.563455383135274\n",
      "EPOCH 95:\n",
      "  batch 100 loss: 0.6032724112434519\n",
      "  batch 200 loss: 0.6174694738354533\n",
      "  batch 300 loss: 0.6045307167549384\n",
      "  batch 400 loss: 0.612300003353532\n",
      "  batch 500 loss: 0.6025106766396485\n",
      "  batch 600 loss: 0.6055199691429944\n",
      "  batch 700 loss: 0.6050396664278379\n",
      "  batch 800 loss: 0.593709663481644\n",
      "  batch 900 loss: 0.5946227041849272\n",
      "  batch 1000 loss: 0.6183746590982387\n",
      "  batch 1094 loss: 0.624511604951994\n",
      "LOSS train 0.624511604951994 valid 0.5992612797613576\n",
      "MAE train 8.813706990921391 valid 8.568039960820249\n",
      "EPOCH 96:\n",
      "  batch 100 loss: 0.6246166868771953\n",
      "  batch 200 loss: 0.6095682096107506\n",
      "  batch 300 loss: 0.603065796229174\n",
      "  batch 400 loss: 0.5838997296891923\n",
      "  batch 500 loss: 0.6246869944919411\n",
      "  batch 600 loss: 0.5936132341023977\n",
      "  batch 700 loss: 0.6257198960232095\n",
      "  batch 800 loss: 0.6254523041552235\n",
      "  batch 900 loss: 0.6016662953343602\n",
      "  batch 1000 loss: 0.5856095756972958\n",
      "  batch 1094 loss: 0.5951151949970347\n",
      "LOSS train 0.5951151949970347 valid 0.5978963793250541\n",
      "MAE train 8.56129993101095 valid 8.534538206239148\n",
      "EPOCH 97:\n",
      "  batch 100 loss: 0.6012882210566722\n",
      "  batch 200 loss: 0.5945849967385554\n",
      "  batch 300 loss: 0.614569109847056\n",
      "  batch 400 loss: 0.5975507530471088\n",
      "  batch 500 loss: 0.6106691326907038\n",
      "  batch 600 loss: 0.6109593901480462\n",
      "  batch 700 loss: 0.6326019047227429\n",
      "  batch 800 loss: 0.6008214498062161\n",
      "  batch 900 loss: 0.6124646297879739\n",
      "  batch 1000 loss: 0.5939671014845189\n",
      "  batch 1094 loss: 0.5958449255967803\n",
      "LOSS train 0.5958449255967803 valid 0.6062423745410815\n",
      "MAE train 8.525477691269579 valid 8.722494765046553\n",
      "EPOCH 98:\n",
      "  batch 100 loss: 0.604566604164887\n",
      "  batch 200 loss: 0.6121598469432555\n",
      "  batch 300 loss: 0.6162932162539443\n",
      "  batch 400 loss: 0.5861772282248886\n",
      "  batch 500 loss: 0.6071727439018323\n",
      "  batch 600 loss: 0.6048671208663008\n",
      "  batch 700 loss: 0.617178579145122\n",
      "  batch 800 loss: 0.5870246647141351\n",
      "  batch 900 loss: 0.6264898646315651\n",
      "  batch 1000 loss: 0.6082305552200861\n",
      "  batch 1094 loss: 0.6056949534313074\n",
      "LOSS train 0.6056949534313074 valid 0.6139314541081633\n",
      "MAE train 8.653425343510953 valid 8.776760311839396\n",
      "EPOCH 99:\n",
      "  batch 100 loss: 0.6060335707022408\n",
      "  batch 200 loss: 0.5976911748705208\n",
      "  batch 300 loss: 0.6245388891064966\n",
      "  batch 400 loss: 0.6157126311333678\n",
      "  batch 500 loss: 0.5858834511332244\n",
      "  batch 600 loss: 0.6070782463514725\n",
      "  batch 700 loss: 0.6174031015854159\n",
      "  batch 800 loss: 0.6242032673732771\n",
      "  batch 900 loss: 0.6038548709998516\n",
      "  batch 1000 loss: 0.5961922322380804\n",
      "  batch 1094 loss: 0.5954539857830151\n",
      "LOSS train 0.5954539857830151 valid 0.5962353573108242\n",
      "MAE train 8.497928343131239 valid 8.509111279827732\n",
      "EPOCH 100:\n",
      "  batch 100 loss: 0.592480832899201\n",
      "  batch 200 loss: 0.6192815116791688\n",
      "  batch 300 loss: 0.6090875185978146\n",
      "  batch 400 loss: 0.6034674860472861\n",
      "  batch 500 loss: 0.6059679531381473\n",
      "  batch 600 loss: 0.5814155521793346\n",
      "  batch 700 loss: 0.606759283933409\n",
      "  batch 800 loss: 0.6091524324202325\n",
      "  batch 900 loss: 0.5993730029765056\n",
      "  batch 1000 loss: 0.6186646733018953\n",
      "  batch 1094 loss: 0.6296818359753945\n",
      "LOSS train 0.6296818359753945 valid 0.6026544441941886\n",
      "MAE train 8.774926447965438 valid 8.614207186254417\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f\"runs/LinearTrussFNN_{timestamp}\")\n",
    "best_loss = np.infty\n",
    "best_test_loss = np.infty\n",
    "mae_fn = nn.L1Loss()\n",
    "for epoch in range(N_EPOCH):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    running_train_loss = 0.\n",
    "    running_train_mae = 0\n",
    "    running_len = 0\n",
    "    last_loss = np.infty\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        running_len += 1\n",
    "        x, y, _ = batch\n",
    "        x = tensor(x_scaler.transform(x), device=device)\n",
    "        y = tensor(y_scaler.transform(y), device=device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mae = mae_fn(tensor(y_scaler.inverse_transform(y_pred.cpu().detach().numpy())),\n",
    "                     tensor(y_scaler.inverse_transform(y.cpu().detach().numpy())))\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "        running_train_mae += mae.item()\n",
    "        \n",
    "        if running_len == 100 or (i+1) == len(train_dl):\n",
    "            last_loss = running_train_loss / running_len # loss per batch\n",
    "            last_mae = running_train_mae / running_len\n",
    "            \n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch * len(train_dl) + i + 1\n",
    "            writer.add_scalar('MSE/train', last_loss, tb_x)\n",
    "            writer.add_scalar('MAE/train', last_mae, tb_x)\n",
    "            running_train_loss = 0\n",
    "            running_train_mae = 0                  \n",
    "            running_len = 0\n",
    "\n",
    "    avg_train_loss = last_loss\n",
    "    avg_train_mae = last_mae\n",
    "    \n",
    "    running_test_loss = 0\n",
    "    running_test_mae = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_dl):  # Use DataLoader to load test data in batches\n",
    "            x, y, _ = batch\n",
    "            x = tensor(x_scaler.transform(x), device=device)\n",
    "            y = tensor(y_scaler.transform(y), device=device)\n",
    "            \n",
    "            y_pred = model(x)\n",
    "            test_loss = loss_fn(y_pred, y)\n",
    "            running_test_loss += test_loss.item()\n",
    "\n",
    "            mae = mae_fn(tensor(y_scaler.inverse_transform(y_pred.cpu().detach().numpy())),\n",
    "                         tensor(y_scaler.inverse_transform(y.cpu().detach().numpy())))\n",
    "\n",
    "            running_test_mae += mae.item()\n",
    "\n",
    "    avg_test_loss = running_test_loss / len(test_dl)  # Divide by number of batches\n",
    "    avg_test_mae = running_test_mae / len(test_dl)  # Divide by number of batches\n",
    "    print('LOSS train {} valid {}'.format(avg_train_loss, avg_test_loss))\n",
    "    print('MAE train {} valid {}'.format(avg_train_mae, avg_test_mae))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalar('MSE/validation', avg_test_loss, epoch)\n",
    "    writer.add_scalar('MAE/validation', avg_test_mae, epoch)\n",
    "    \n",
    "    #writer.add_scalars('Training vs. Validation Loss',\n",
    "    #                { 'Training' : avg_train_loss, 'Validation' : avg_test_loss },\n",
    "    #                epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15096da-b3c7-4165-8f6a-0f546944b72b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
