{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:08:42.630143Z",
     "start_time": "2025-02-10T14:08:42.625905Z"
    }
   },
   "cell_type": "code",
   "source": "%pwd",
   "id": "6e7e5c185a6eda1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aflamant/Documents/courses/2024-2025/mémoire/03-code/memoire/MLP/experiments/01-single_ea'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:08:51.343945Z",
     "start_time": "2025-02-10T14:08:51.341297Z"
    }
   },
   "cell_type": "code",
   "source": "%cd ../..",
   "id": "c88e4b1b23c52dc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aflamant/Documents/courses/2024-2025/mémoire/03-code/memoire/MLP\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import libraries",
   "id": "ba9405c5621584ae"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-10T14:09:07.921715Z",
     "start_time": "2025-02-10T14:08:57.556679Z"
    }
   },
   "source": [
    "RANDOM_STATE = 42\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from MLP.dataset import (TenBarsCantileverTrussSingleEADataset,\n",
    "                     TwoBarsTrussSingleEADataset,\n",
    "                     BiSupportedTrussBeamSingleEADataset)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from MLP.models.architecture import MultiLayerPerceptron\n",
    "from MLP.models.processing import StandardScaler\n",
    "from MLP.loss import StiffnessToDisplacementLoss, StiffnessToLoadLoss, construct_k_from_ea\n",
    "\n",
    "np.random.seed(RANDOM_STATE)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 12\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KFold\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (TenBarsCantileverTrussSingleEADataset,\n\u001B[1;32m     10\u001B[0m                      TwoBarsTrussSingleEADataset,\n\u001B[1;32m     11\u001B[0m                      BiSupportedTrussBeamSingleEADataset)\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorboard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SummaryWriter\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marchitecture\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MultiLayerPerceptron\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n",
      "File \u001B[0;32m~/miniconda3/envs/memoire/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorboard\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdistutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LooseVersion\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(tensorboard, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m LooseVersion(\n\u001B[1;32m      5\u001B[0m     tensorboard\u001B[38;5;241m.\u001B[39m__version__\n\u001B[1;32m      6\u001B[0m ) \u001B[38;5;241m<\u001B[39m LooseVersion(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.15\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T10:30:41.560214Z",
     "start_time": "2025-01-20T10:30:41.546139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if torch.mps.is_available()\n",
    "    else 'cpu'\n",
    ")"
   ],
   "id": "49d00f755d341599",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T10:30:41.565161Z",
     "start_time": "2025-01-20T10:30:41.563862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "c1 = StiffnessToLoadLoss()\n",
    "c2 = StiffnessToDisplacementLoss()"
   ],
   "id": "5f4300b8293adb36",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data",
   "id": "fc15e5855a902397"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T10:30:41.605850Z",
     "start_time": "2025-01-20T10:30:41.568501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "connectivity = {}\n",
    "connectivity['cantilever'] = torch.tensor([[0, 1],\n",
    "                                           [1, 2],\n",
    "                                           [3, 4],\n",
    "                                           [4, 5],\n",
    "                                           [1, 4],\n",
    "                                           [2, 5],\n",
    "                                           [0, 4],\n",
    "                                           [3, 1],\n",
    "                                           [1, 5],\n",
    "                                           [4, 2]]).to(device)\n",
    "connectivity['triangle'] = torch.tensor([[0, 1],\n",
    "                                         [1, 2]]).to(device)\n",
    "connectivity['beam'] = torch.tensor([[0, 1],\n",
    "                                     [1, 2],\n",
    "                                     [2, 3],\n",
    "                                     [3, 4],\n",
    "                                     [5, 6],\n",
    "                                     [6, 7],\n",
    "                                     [7, 8],\n",
    "                                     [8, 9],\n",
    "                                     [0, 5],\n",
    "                                     [1, 6],\n",
    "                                     [2, 7],\n",
    "                                     [3, 8],\n",
    "                                     [4, 9],\n",
    "                                     [0, 6],\n",
    "                                     [5, 1],\n",
    "                                     [1, 7],\n",
    "                                     [6, 2],\n",
    "                                     [2, 8],\n",
    "                                     [7, 3],\n",
    "                                     [3, 9],\n",
    "                                     [8, 4]]).to(device)"
   ],
   "id": "c568f948e1f4d9fc",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T10:30:41.630818Z",
     "start_time": "2025-01-20T10:30:41.628408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "support = {}\n",
    "support['cantilever'] = torch.tensor([0, 1, 6, 7]).to(device)\n",
    "support['triangle'] = torch.tensor([0, 1, 4, 5]).to(device)\n",
    "support['beam'] = torch.tensor([10, 11, 18, 19]).to(device)"
   ],
   "id": "87a5072404d8b2ea",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T10:30:48.754582Z",
     "start_time": "2025-01-20T10:30:41.648743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "names = ['beam', 'cantilever', 'triangle']\n",
    "\n",
    "path = {\n",
    "    'beam': 'data/dataset/beam/data.hdf5',\n",
    "    'cantilever': 'data/dataset/cantilever/data.hdf5',\n",
    "    'triangle': 'data/dataset/triangle/data.hdf5'\n",
    "}\n",
    "\n",
    "_dataset = {\n",
    "    'beam': BiSupportedTrussBeamSingleEADataset(path['beam']),\n",
    "    'cantilever': TenBarsCantileverTrussSingleEADataset(path['cantilever']),\n",
    "    'triangle': TwoBarsTrussSingleEADataset(path['triangle'])\n",
    "}"
   ],
   "id": "b234aab5734f883b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T10:30:48.784715Z",
     "start_time": "2025-01-20T10:30:48.777401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_train_ds = {}\n",
    "_test_ds = {}\n",
    "\n",
    "for typology in names:\n",
    "    train_ds, test_ds = random_split(_dataset[typology], (0.8, 0.2))\n",
    "    _train_ds[typology] = train_ds\n",
    "    _test_ds[typology] = test_ds"
   ],
   "id": "6c77f9f1a59f331c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "7c139f0c4e41e181"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T11:09:25.790683Z",
     "start_time": "2025-01-20T11:09:25.783524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(\n",
    "        tb_writer,\n",
    "        dataset,\n",
    "        elems,\n",
    "        supports,\n",
    "        out_size=1,\n",
    "        hidden_size=70,\n",
    "        n_hidden=3,\n",
    "        activation=nn.ReLU,\n",
    "        activation_params={},\n",
    "        learning_rate=1e-3,\n",
    "        n_folds=3,\n",
    "        n_epochs=30,\n",
    "):\n",
    "    in_size = len(dataset[0][0])\n",
    "    N_EPOCHS = n_epochs\n",
    "    BATCH_SIZE = 2048  # Not an hyperparameter\n",
    "    LEARNING_RATE = learning_rate\n",
    "\n",
    "    train_losses_fold = [[] for _ in range(n_folds)]\n",
    "    train_data_losses_fold = [[] for _ in range(n_folds)]\n",
    "    train_physics_losses_fold = [[] for _ in range(n_folds)]\n",
    "\n",
    "    val_losses_fold = [[] for _ in range(n_folds)]\n",
    "    val_data_losses_fold = [[] for _ in range(n_folds)]\n",
    "    val_physics_losses_fold = [[] for _ in range(n_folds)]\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    for fold, (train_indices, test_indices) in enumerate(kf.split(dataset)):\n",
    "        # Dataset and Dataloader\n",
    "        train_ds = Subset(dataset, train_indices)\n",
    "        test_ds = Subset(dataset, test_indices)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, )\n",
    "        val_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, )\n",
    "\n",
    "        # Training scaler\n",
    "        x_scaler = StandardScaler(in_size).to(device)\n",
    "        y_scaler = StandardScaler(out_size).to(device)\n",
    "        for x, y, _, _, _ in train_loader:\n",
    "            x_scaler.partial_fit(x.to(device))\n",
    "            y_scaler.partial_fit(y.to(device))\n",
    "\n",
    "        # Model setup\n",
    "        model = MultiLayerPerceptron(in_size, out_size, hidden_size, n_hidden, activation,\n",
    "                                     activation_params).to(device)\n",
    "\n",
    "        data_criterion = nn.MSELoss()\n",
    "        physics_criterion = StiffnessToLoadLoss()\n",
    "\n",
    "        physics_loss_scale = None\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        # Model training\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            # Train\n",
    "            model.train()\n",
    "\n",
    "            total_loss = 0.0\n",
    "            total_data_loss = 0.0\n",
    "            total_physics_loss = 0.0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                data, ea, nodes, u, q = batch\n",
    "\n",
    "                data, ea = data.to(device), ea.to(device)\n",
    "                nodes, u, q = nodes.to(device), u.to(device), q.to(device)\n",
    "\n",
    "                z_data = x_scaler.transform(data)\n",
    "                z_ea = y_scaler.transform(ea)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                z_pred_ea = model(z_data)\n",
    "\n",
    "                ea_pred = y_scaler.inverse_transform(z_pred_ea)\n",
    "                stiffness_pred = construct_k_from_ea(ea_pred, nodes, elems, supports, device=device)\n",
    "\n",
    "                data_loss = data_criterion(z_pred_ea, z_ea)\n",
    "                physics_loss = physics_criterion(stiffness_pred, u, q)\n",
    "\n",
    "                if physics_loss_scale is None: physics_loss_scale = physics_loss.item()\n",
    "\n",
    "                physics_loss /= physics_loss_scale\n",
    "\n",
    "                loss = data_loss + physics_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_data_loss += data_loss.item()\n",
    "                total_physics_loss += physics_loss.item()\n",
    "\n",
    "            # Log\n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_physics_loss = total_physics_loss / len(train_loader)\n",
    "            train_data_loss = total_data_loss / len(train_loader)\n",
    "\n",
    "            train_losses_fold[fold].append(train_loss)\n",
    "            train_physics_losses_fold[fold].append(train_physics_loss)\n",
    "            train_data_losses_fold[fold].append(train_data_loss)\n",
    "\n",
    "            # Validate\n",
    "            model.eval()\n",
    "            total_loss = 0.0\n",
    "            total_data_loss = 0.0\n",
    "            total_physics_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    data, ea, nodes, u, q = batch\n",
    "\n",
    "                    data, ea = data.to(device), ea.to(device)\n",
    "                    nodes, u, q = nodes.to(device), u.to(device), q.to(device)\n",
    "\n",
    "                    z_data = x_scaler.transform(data)\n",
    "                    z_ea = y_scaler.transform(ea)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    z_pred_ea = model(z_data)\n",
    "\n",
    "                    ea_pred = y_scaler.inverse_transform(z_pred_ea)\n",
    "                    stiffness_pred = construct_k_from_ea(ea_pred, nodes, elems, supports, device=device)\n",
    "\n",
    "                    data_loss = data_criterion(z_pred_ea, z_ea)\n",
    "                    physics_loss = physics_criterion(stiffness_pred, u, q) / physics_loss_scale\n",
    "\n",
    "                    loss = data_loss + physics_loss\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    total_data_loss += data_loss.item()\n",
    "                    total_physics_loss += physics_loss.item()\n",
    "\n",
    "            # Log\n",
    "            val_loss = total_loss / len(val_loader)\n",
    "            val_physics_loss = total_physics_loss / len(val_loader)\n",
    "            val_data_loss = total_data_loss / len(val_loader)\n",
    "\n",
    "            val_losses_fold[fold].append(val_loss)\n",
    "            val_physics_losses_fold[fold].append(val_physics_loss)\n",
    "            val_data_losses_fold[fold].append(val_data_loss)\n",
    "\n",
    "            tb_writer.add_scalar(f'Loss/total/train_FOLD_{fold}', train_loss, epoch)\n",
    "            tb_writer.add_scalar(f'Loss/total/val_FOLD_{fold}', val_loss, epoch)\n",
    "            tb_writer.add_scalar(f'Loss/data/train_FOLD_{fold}', train_data_loss, epoch)\n",
    "            tb_writer.add_scalar(f'Loss/data/val_FOLD_{fold}', val_data_loss, epoch)\n",
    "            tb_writer.add_scalar(f'Loss/physics/train_FOLD_{fold}', train_physics_loss, epoch)\n",
    "            tb_writer.add_scalar(f'Loss/physics/val_FOLD_{fold}', val_physics_loss, epoch)\n",
    "\n",
    "            print(\n",
    "                f\"Fold {fold + 1}, Epoch {epoch + 1}/{N_EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        print(f\"Finished fold {fold + 1}\")\n",
    "\n",
    "    train_losses = np.mean(train_losses_fold, axis=0)\n",
    "    train_data_losses = np.mean(train_data_losses_fold, axis=0)\n",
    "    train_physics_losses = np.mean(train_physics_losses_fold, axis=0)\n",
    "\n",
    "    val_losses = np.mean(val_losses_fold, axis=0)\n",
    "    val_data_losses = np.mean(val_data_losses_fold, axis=0)\n",
    "    val_physics_losses = np.mean(val_physics_losses_fold, axis=0)\n",
    "\n",
    "    for i in range(len(train_losses)):\n",
    "        tb_writer.add_scalar(f'Loss/total/train', train_losses[i], i)\n",
    "        tb_writer.add_scalar(f'Loss/total/val', val_losses[i], i)\n",
    "        tb_writer.add_scalar(f'Loss/data/train', train_data_losses[i], i)\n",
    "        tb_writer.add_scalar(f'Loss/data/val', val_data_losses[i], i)\n",
    "        tb_writer.add_scalar(f'Loss/physics/train', train_physics_losses[i], i)\n",
    "        tb_writer.add_scalar(f'Loss/physics/val', val_physics_losses[i], i)"
   ],
   "id": "9290965238537b5",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Beam 4x60\n",
    "# Triangle 3x50\n",
    "# Cantilever 4x50\n",
    "activation = nn.ReLU\n",
    "activation_params = {}\n",
    "lr = 1e-3\n",
    "dataset_name = 'triangle'\n",
    "for n_layers in [3, 4]:\n",
    "    for layer_size in [40, 50, 60]:\n",
    "        if len(activation_params) == 0:\n",
    "            writer = SummaryWriter(\n",
    "                f\"./runs/{dataset_name}/FEI_MLP/{n_layers}/{layer_size}/{activation.__name__}/lr_{lr}\")\n",
    "        else:\n",
    "            writer = SummaryWriter(\n",
    "                f\"./runs/{dataset_name}/FEI_MLP/{n_layers}/{layer_size}/{activation.__name__}/{\"\".join([f\"{k}_{v}\" for k, v in activation_params.items()])}/lr_{lr}\")\n",
    "\n",
    "        main(writer, _train_ds[dataset_name], connectivity[dataset_name], support[dataset_name], hidden_size=layer_size,\n",
    "             n_hidden=n_layers, learning_rate=lr, activation=activation, activation_params=activation_params)"
   ],
   "id": "c5bda526fa9c8833",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T14:39:45.396725Z",
     "start_time": "2025-01-20T14:20:50.771576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "activation = nn.ReLU\n",
    "activation_params = {}\n",
    "lr = 1e-3\n",
    "dataset_name = 'beam'\n",
    "n_layers = 4\n",
    "layer_size = 60\n",
    "for lr in np.logspace(-4, -1, num=5, ):\n",
    "    if len(activation_params) == 0:\n",
    "        writer = SummaryWriter(f\"./runs/{dataset_name}/FEI_MLP/{n_layers}/{layer_size}/{activation.__name__}/lr_{lr}\")\n",
    "    else:\n",
    "        writer = SummaryWriter(\n",
    "            f\"./runs/{dataset_name}/FEI_MLP/{n_layers}/{layer_size}/{activation.__name__}/{\"\".join([f\"{k}_{v}\" for k, v in activation_params.items()])}/lr_{lr}\")\n",
    "\n",
    "    main(writer, _train_ds[dataset_name], connectivity[dataset_name], support[dataset_name], hidden_size=layer_size,\n",
    "         n_hidden=n_layers, learning_rate=lr, activation=activation, activation_params=activation_params, n_epochs=150)"
   ],
   "id": "722d31cfd6350a81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/150, Train Loss: 1.9716, Val Loss: 1.9082\n",
      "Fold 1, Epoch 2/150, Train Loss: 1.8207, Val Loss: 1.7254\n",
      "Fold 1, Epoch 3/150, Train Loss: 1.6111, Val Loss: 1.5007\n",
      "Fold 1, Epoch 4/150, Train Loss: 1.4005, Val Loss: 1.3190\n",
      "Fold 1, Epoch 5/150, Train Loss: 1.2375, Val Loss: 1.1658\n",
      "Fold 1, Epoch 6/150, Train Loss: 1.0679, Val Loss: 0.9596\n",
      "Fold 1, Epoch 7/150, Train Loss: 0.8322, Val Loss: 0.7023\n",
      "Fold 1, Epoch 8/150, Train Loss: 0.5964, Val Loss: 0.4906\n",
      "Fold 1, Epoch 9/150, Train Loss: 0.4146, Val Loss: 0.3379\n",
      "Fold 1, Epoch 10/150, Train Loss: 0.2921, Val Loss: 0.2458\n",
      "Fold 1, Epoch 11/150, Train Loss: 0.2213, Val Loss: 0.1954\n",
      "Fold 1, Epoch 12/150, Train Loss: 0.1829, Val Loss: 0.1674\n",
      "Fold 1, Epoch 13/150, Train Loss: 0.1599, Val Loss: 0.1492\n",
      "Fold 1, Epoch 14/150, Train Loss: 0.1444, Val Loss: 0.1364\n",
      "Fold 1, Epoch 15/150, Train Loss: 0.1330, Val Loss: 0.1268\n",
      "Fold 1, Epoch 16/150, Train Loss: 0.1240, Val Loss: 0.1191\n",
      "Fold 1, Epoch 17/150, Train Loss: 0.1168, Val Loss: 0.1129\n",
      "Fold 1, Epoch 18/150, Train Loss: 0.1105, Val Loss: 0.1073\n",
      "Fold 1, Epoch 19/150, Train Loss: 0.1052, Val Loss: 0.1033\n",
      "Fold 1, Epoch 20/150, Train Loss: 0.1012, Val Loss: 0.0979\n",
      "Fold 1, Epoch 21/150, Train Loss: 0.0987, Val Loss: 0.0944\n",
      "Fold 1, Epoch 22/150, Train Loss: 0.0925, Val Loss: 0.0897\n",
      "Fold 1, Epoch 23/150, Train Loss: 0.0882, Val Loss: 0.0861\n",
      "Fold 1, Epoch 24/150, Train Loss: 0.0844, Val Loss: 0.0826\n",
      "Fold 1, Epoch 25/150, Train Loss: 0.0810, Val Loss: 0.0794\n",
      "Fold 1, Epoch 26/150, Train Loss: 0.0777, Val Loss: 0.0763\n",
      "Fold 1, Epoch 27/150, Train Loss: 0.0746, Val Loss: 0.0733\n",
      "Fold 1, Epoch 28/150, Train Loss: 0.0717, Val Loss: 0.0704\n",
      "Fold 1, Epoch 29/150, Train Loss: 0.0689, Val Loss: 0.0677\n",
      "Fold 1, Epoch 30/150, Train Loss: 0.0662, Val Loss: 0.0653\n",
      "Fold 1, Epoch 31/150, Train Loss: 0.0638, Val Loss: 0.0629\n",
      "Fold 1, Epoch 32/150, Train Loss: 0.0615, Val Loss: 0.0609\n",
      "Fold 1, Epoch 33/150, Train Loss: 0.0596, Val Loss: 0.0589\n",
      "Fold 1, Epoch 34/150, Train Loss: 0.0585, Val Loss: 0.0571\n",
      "Fold 1, Epoch 35/150, Train Loss: 0.0573, Val Loss: 0.0556\n",
      "Fold 1, Epoch 36/150, Train Loss: 0.0550, Val Loss: 0.0532\n",
      "Fold 1, Epoch 37/150, Train Loss: 0.0522, Val Loss: 0.0515\n",
      "Fold 1, Epoch 38/150, Train Loss: 0.0502, Val Loss: 0.0497\n",
      "Fold 1, Epoch 39/150, Train Loss: 0.0484, Val Loss: 0.0483\n",
      "Fold 1, Epoch 40/150, Train Loss: 0.0469, Val Loss: 0.0469\n",
      "Fold 1, Epoch 41/150, Train Loss: 0.0455, Val Loss: 0.0456\n",
      "Fold 1, Epoch 42/150, Train Loss: 0.0442, Val Loss: 0.0443\n",
      "Fold 1, Epoch 43/150, Train Loss: 0.0429, Val Loss: 0.0432\n",
      "Fold 1, Epoch 44/150, Train Loss: 0.0417, Val Loss: 0.0420\n",
      "Fold 1, Epoch 45/150, Train Loss: 0.0405, Val Loss: 0.0409\n",
      "Fold 1, Epoch 46/150, Train Loss: 0.0394, Val Loss: 0.0398\n",
      "Fold 1, Epoch 47/150, Train Loss: 0.0384, Val Loss: 0.0388\n",
      "Fold 1, Epoch 48/150, Train Loss: 0.0374, Val Loss: 0.0378\n",
      "Fold 1, Epoch 49/150, Train Loss: 0.0364, Val Loss: 0.0369\n",
      "Fold 1, Epoch 50/150, Train Loss: 0.0355, Val Loss: 0.0360\n",
      "Fold 1, Epoch 51/150, Train Loss: 0.0346, Val Loss: 0.0352\n",
      "Fold 1, Epoch 52/150, Train Loss: 0.0337, Val Loss: 0.0343\n",
      "Fold 1, Epoch 53/150, Train Loss: 0.0329, Val Loss: 0.0336\n",
      "Fold 1, Epoch 54/150, Train Loss: 0.0321, Val Loss: 0.0327\n",
      "Fold 1, Epoch 55/150, Train Loss: 0.0314, Val Loss: 0.0321\n",
      "Fold 1, Epoch 56/150, Train Loss: 0.0308, Val Loss: 0.0312\n",
      "Fold 1, Epoch 57/150, Train Loss: 0.0303, Val Loss: 0.0310\n",
      "Fold 1, Epoch 58/150, Train Loss: 0.0298, Val Loss: 0.0302\n",
      "Fold 1, Epoch 59/150, Train Loss: 0.0298, Val Loss: 0.0298\n",
      "Fold 1, Epoch 60/150, Train Loss: 0.0286, Val Loss: 0.0290\n",
      "Fold 1, Epoch 61/150, Train Loss: 0.0278, Val Loss: 0.0285\n",
      "Fold 1, Epoch 62/150, Train Loss: 0.0272, Val Loss: 0.0278\n",
      "Fold 1, Epoch 63/150, Train Loss: 0.0266, Val Loss: 0.0275\n",
      "Fold 1, Epoch 64/150, Train Loss: 0.0261, Val Loss: 0.0268\n",
      "Fold 1, Epoch 65/150, Train Loss: 0.0257, Val Loss: 0.0265\n",
      "Fold 1, Epoch 66/150, Train Loss: 0.0252, Val Loss: 0.0259\n",
      "Fold 1, Epoch 67/150, Train Loss: 0.0248, Val Loss: 0.0257\n",
      "Fold 1, Epoch 68/150, Train Loss: 0.0244, Val Loss: 0.0251\n",
      "Fold 1, Epoch 69/150, Train Loss: 0.0239, Val Loss: 0.0249\n",
      "Fold 1, Epoch 70/150, Train Loss: 0.0236, Val Loss: 0.0243\n",
      "Fold 1, Epoch 71/150, Train Loss: 0.0232, Val Loss: 0.0242\n",
      "Fold 1, Epoch 72/150, Train Loss: 0.0228, Val Loss: 0.0236\n",
      "Fold 1, Epoch 73/150, Train Loss: 0.0225, Val Loss: 0.0235\n",
      "Fold 1, Epoch 74/150, Train Loss: 0.0221, Val Loss: 0.0229\n",
      "Fold 1, Epoch 75/150, Train Loss: 0.0218, Val Loss: 0.0228\n",
      "Fold 1, Epoch 76/150, Train Loss: 0.0215, Val Loss: 0.0223\n",
      "Fold 1, Epoch 77/150, Train Loss: 0.0212, Val Loss: 0.0222\n",
      "Fold 1, Epoch 78/150, Train Loss: 0.0209, Val Loss: 0.0217\n",
      "Fold 1, Epoch 79/150, Train Loss: 0.0206, Val Loss: 0.0217\n",
      "Fold 1, Epoch 80/150, Train Loss: 0.0203, Val Loss: 0.0212\n",
      "Fold 1, Epoch 81/150, Train Loss: 0.0201, Val Loss: 0.0212\n",
      "Fold 1, Epoch 82/150, Train Loss: 0.0198, Val Loss: 0.0207\n",
      "Fold 1, Epoch 83/150, Train Loss: 0.0196, Val Loss: 0.0207\n",
      "Fold 1, Epoch 84/150, Train Loss: 0.0193, Val Loss: 0.0202\n",
      "Fold 1, Epoch 85/150, Train Loss: 0.0191, Val Loss: 0.0202\n",
      "Fold 1, Epoch 86/150, Train Loss: 0.0189, Val Loss: 0.0197\n",
      "Fold 1, Epoch 87/150, Train Loss: 0.0186, Val Loss: 0.0198\n",
      "Fold 1, Epoch 88/150, Train Loss: 0.0184, Val Loss: 0.0193\n",
      "Fold 1, Epoch 89/150, Train Loss: 0.0181, Val Loss: 0.0193\n",
      "Fold 1, Epoch 90/150, Train Loss: 0.0179, Val Loss: 0.0189\n",
      "Fold 1, Epoch 91/150, Train Loss: 0.0177, Val Loss: 0.0189\n",
      "Fold 1, Epoch 92/150, Train Loss: 0.0175, Val Loss: 0.0185\n",
      "Fold 1, Epoch 93/150, Train Loss: 0.0173, Val Loss: 0.0185\n",
      "Fold 1, Epoch 94/150, Train Loss: 0.0171, Val Loss: 0.0182\n",
      "Fold 1, Epoch 95/150, Train Loss: 0.0169, Val Loss: 0.0181\n",
      "Fold 1, Epoch 96/150, Train Loss: 0.0167, Val Loss: 0.0178\n",
      "Fold 1, Epoch 97/150, Train Loss: 0.0165, Val Loss: 0.0178\n",
      "Fold 1, Epoch 98/150, Train Loss: 0.0163, Val Loss: 0.0175\n",
      "Fold 1, Epoch 99/150, Train Loss: 0.0162, Val Loss: 0.0175\n",
      "Fold 1, Epoch 100/150, Train Loss: 0.0160, Val Loss: 0.0171\n",
      "Fold 1, Epoch 101/150, Train Loss: 0.0159, Val Loss: 0.0172\n",
      "Fold 1, Epoch 102/150, Train Loss: 0.0158, Val Loss: 0.0167\n",
      "Fold 1, Epoch 103/150, Train Loss: 0.0157, Val Loss: 0.0170\n",
      "Fold 1, Epoch 104/150, Train Loss: 0.0158, Val Loss: 0.0165\n",
      "Fold 1, Epoch 105/150, Train Loss: 0.0153, Val Loss: 0.0165\n",
      "Fold 1, Epoch 106/150, Train Loss: 0.0151, Val Loss: 0.0162\n",
      "Fold 1, Epoch 107/150, Train Loss: 0.0149, Val Loss: 0.0161\n",
      "Fold 1, Epoch 108/150, Train Loss: 0.0147, Val Loss: 0.0159\n",
      "Fold 1, Epoch 109/150, Train Loss: 0.0146, Val Loss: 0.0158\n",
      "Fold 1, Epoch 110/150, Train Loss: 0.0144, Val Loss: 0.0157\n",
      "Fold 1, Epoch 111/150, Train Loss: 0.0143, Val Loss: 0.0155\n",
      "Fold 1, Epoch 112/150, Train Loss: 0.0141, Val Loss: 0.0154\n",
      "Fold 1, Epoch 113/150, Train Loss: 0.0140, Val Loss: 0.0153\n",
      "Fold 1, Epoch 114/150, Train Loss: 0.0139, Val Loss: 0.0152\n",
      "Fold 1, Epoch 115/150, Train Loss: 0.0138, Val Loss: 0.0151\n",
      "Fold 1, Epoch 116/150, Train Loss: 0.0136, Val Loss: 0.0150\n",
      "Fold 1, Epoch 117/150, Train Loss: 0.0135, Val Loss: 0.0149\n",
      "Fold 1, Epoch 118/150, Train Loss: 0.0134, Val Loss: 0.0148\n",
      "Fold 1, Epoch 119/150, Train Loss: 0.0133, Val Loss: 0.0147\n",
      "Fold 1, Epoch 120/150, Train Loss: 0.0132, Val Loss: 0.0146\n",
      "Fold 1, Epoch 121/150, Train Loss: 0.0130, Val Loss: 0.0145\n",
      "Fold 1, Epoch 122/150, Train Loss: 0.0129, Val Loss: 0.0144\n",
      "Fold 1, Epoch 123/150, Train Loss: 0.0128, Val Loss: 0.0143\n",
      "Fold 1, Epoch 124/150, Train Loss: 0.0127, Val Loss: 0.0142\n",
      "Fold 1, Epoch 125/150, Train Loss: 0.0126, Val Loss: 0.0141\n",
      "Fold 1, Epoch 126/150, Train Loss: 0.0125, Val Loss: 0.0140\n",
      "Fold 1, Epoch 127/150, Train Loss: 0.0124, Val Loss: 0.0139\n",
      "Fold 1, Epoch 128/150, Train Loss: 0.0123, Val Loss: 0.0138\n",
      "Fold 1, Epoch 129/150, Train Loss: 0.0122, Val Loss: 0.0137\n",
      "Fold 1, Epoch 130/150, Train Loss: 0.0121, Val Loss: 0.0136\n",
      "Fold 1, Epoch 131/150, Train Loss: 0.0120, Val Loss: 0.0135\n",
      "Fold 1, Epoch 132/150, Train Loss: 0.0119, Val Loss: 0.0134\n",
      "Fold 1, Epoch 133/150, Train Loss: 0.0118, Val Loss: 0.0134\n",
      "Fold 1, Epoch 134/150, Train Loss: 0.0117, Val Loss: 0.0132\n",
      "Fold 1, Epoch 135/150, Train Loss: 0.0116, Val Loss: 0.0132\n",
      "Fold 1, Epoch 136/150, Train Loss: 0.0115, Val Loss: 0.0130\n",
      "Fold 1, Epoch 137/150, Train Loss: 0.0114, Val Loss: 0.0130\n",
      "Fold 1, Epoch 138/150, Train Loss: 0.0114, Val Loss: 0.0129\n",
      "Fold 1, Epoch 139/150, Train Loss: 0.0113, Val Loss: 0.0129\n",
      "Fold 1, Epoch 140/150, Train Loss: 0.0112, Val Loss: 0.0127\n",
      "Fold 1, Epoch 141/150, Train Loss: 0.0111, Val Loss: 0.0127\n",
      "Fold 1, Epoch 142/150, Train Loss: 0.0110, Val Loss: 0.0126\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 17\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     writer \u001B[38;5;241m=\u001B[39m SummaryWriter(\n\u001B[1;32m     15\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./runs/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/FEI_MLP/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_layers\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mactivation\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mv\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mk,\u001B[38;5;250m \u001B[39mv\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mactivation_params\u001B[38;5;241m.\u001B[39mitems()])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/lr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m main(writer, _train_ds[dataset_name], connectivity[dataset_name], support[dataset_name], hidden_size\u001B[38;5;241m=\u001B[39mlayer_size,\n\u001B[1;32m     18\u001B[0m      n_hidden\u001B[38;5;241m=\u001B[39mn_layers, learning_rate\u001B[38;5;241m=\u001B[39mlr, activation\u001B[38;5;241m=\u001B[39mactivation, activation_params\u001B[38;5;241m=\u001B[39mactivation_params, n_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m150\u001B[39m)\n",
      "Cell \u001B[0;32mIn[15], line 89\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(tb_writer, dataset, elems, supports, out_size, hidden_size, n_hidden, activation, activation_params, learning_rate, n_folds, n_epochs)\u001B[0m\n\u001B[1;32m     87\u001B[0m loss \u001B[38;5;241m=\u001B[39m data_loss \u001B[38;5;241m+\u001B[39m physics_loss\n\u001B[1;32m     88\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 89\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     91\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     92\u001B[0m total_data_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m data_loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/miniconda3/envs/memoire/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m             )\n\u001B[0;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    490\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/memoire/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/miniconda3/envs/memoire/lib/python3.12/site-packages/torch/optim/adam.py:223\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    211\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    213\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    214\u001B[0m         group,\n\u001B[1;32m    215\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    220\u001B[0m         state_steps,\n\u001B[1;32m    221\u001B[0m     )\n\u001B[0;32m--> 223\u001B[0m     adam(\n\u001B[1;32m    224\u001B[0m         params_with_grad,\n\u001B[1;32m    225\u001B[0m         grads,\n\u001B[1;32m    226\u001B[0m         exp_avgs,\n\u001B[1;32m    227\u001B[0m         exp_avg_sqs,\n\u001B[1;32m    228\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    229\u001B[0m         state_steps,\n\u001B[1;32m    230\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    231\u001B[0m         has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    232\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    233\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    234\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    235\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    236\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    237\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    238\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    239\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    240\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    241\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    242\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    243\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    244\u001B[0m     )\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/miniconda3/envs/memoire/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/memoire/lib/python3.12/site-packages/torch/optim/adam.py:784\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    782\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 784\u001B[0m func(\n\u001B[1;32m    785\u001B[0m     params,\n\u001B[1;32m    786\u001B[0m     grads,\n\u001B[1;32m    787\u001B[0m     exp_avgs,\n\u001B[1;32m    788\u001B[0m     exp_avg_sqs,\n\u001B[1;32m    789\u001B[0m     max_exp_avg_sqs,\n\u001B[1;32m    790\u001B[0m     state_steps,\n\u001B[1;32m    791\u001B[0m     amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    792\u001B[0m     has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    793\u001B[0m     beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    794\u001B[0m     beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    795\u001B[0m     lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[1;32m    796\u001B[0m     weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[1;32m    797\u001B[0m     eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m    798\u001B[0m     maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[1;32m    799\u001B[0m     capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m    800\u001B[0m     differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[1;32m    801\u001B[0m     grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[1;32m    802\u001B[0m     found_inf\u001B[38;5;241m=\u001B[39mfound_inf,\n\u001B[1;32m    803\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/memoire/lib/python3.12/site-packages/torch/optim/adam.py:430\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    428\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 430\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    432\u001B[0m     param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n\u001B[1;32m    434\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T17:05:41.171264Z",
     "start_time": "2025-01-18T16:50:59.650957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_layers = 4\n",
    "layer_size = 40\n",
    "lr = 4e-4\n",
    "\n",
    "for activation, activation_params in zip(\n",
    "        [nn.ReLU, nn.Tanh, nn.Sigmoid, nn.LeakyReLU, nn.LeakyReLU, nn.LeakyReLU],\n",
    "        [{}, {}, {}, {'negative_slope': 0.01}, {'negative_slope': 0.02}, {'negative_slope': 0.05}]\n",
    "):\n",
    "    if len(activation_params) == 0:\n",
    "        writer = SummaryWriter(f\"./runs/{dataset_name}/MLP/{n_layers}/{layer_size}/{activation.__name__}/lr_{lr}\")\n",
    "    else:\n",
    "        writer = SummaryWriter(\n",
    "            f\"./runs/{dataset_name}/MLP/{n_layers}/{layer_size}/{activation.__name__}/{\"\".join([f\"{k}_{v}\" for k, v in activation_params.items()])}/lr_{lr}\")\n",
    "\n",
    "    main(writer, _train_ds[dataset_name], hidden_size=layer_size, n_hidden=n_layers, learning_rate=lr,\n",
    "         activation=activation, activation_params=activation_params, n_epochs=50)"
   ],
   "id": "fe55fd7cd8715419",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/50, Train Loss: 0.9698, Val Loss: 0.9121\n",
      "Fold 1, Epoch 2/50, Train Loss: 0.7999, Val Loss: 0.6349\n",
      "Fold 1, Epoch 3/50, Train Loss: 0.4544, Val Loss: 0.3158\n",
      "Fold 1, Epoch 4/50, Train Loss: 0.2541, Val Loss: 0.2030\n",
      "Fold 1, Epoch 5/50, Train Loss: 0.1708, Val Loss: 0.1469\n",
      "Fold 1, Epoch 6/50, Train Loss: 0.1302, Val Loss: 0.1210\n",
      "Fold 1, Epoch 7/50, Train Loss: 0.1105, Val Loss: 0.1054\n",
      "Fold 1, Epoch 8/50, Train Loss: 0.0969, Val Loss: 0.0934\n",
      "Fold 1, Epoch 9/50, Train Loss: 0.0861, Val Loss: 0.0836\n",
      "Fold 1, Epoch 10/50, Train Loss: 0.0773, Val Loss: 0.0770\n",
      "Fold 1, Epoch 11/50, Train Loss: 0.0705, Val Loss: 0.0722\n",
      "Fold 1, Epoch 12/50, Train Loss: 0.0649, Val Loss: 0.0652\n",
      "Fold 1, Epoch 13/50, Train Loss: 0.0604, Val Loss: 0.0630\n",
      "Fold 1, Epoch 14/50, Train Loss: 0.0567, Val Loss: 0.0593\n",
      "Fold 1, Epoch 15/50, Train Loss: 0.0532, Val Loss: 0.0529\n",
      "Fold 1, Epoch 16/50, Train Loss: 0.0500, Val Loss: 0.0534\n",
      "Fold 1, Epoch 17/50, Train Loss: 0.0473, Val Loss: 0.0499\n",
      "Fold 1, Epoch 18/50, Train Loss: 0.0449, Val Loss: 0.0554\n",
      "Fold 1, Epoch 19/50, Train Loss: 0.0427, Val Loss: 0.0431\n",
      "Fold 1, Epoch 20/50, Train Loss: 0.0407, Val Loss: 0.0449\n",
      "Fold 1, Epoch 21/50, Train Loss: 0.0389, Val Loss: 0.0497\n",
      "Fold 1, Epoch 22/50, Train Loss: 0.0371, Val Loss: 0.0491\n",
      "Fold 1, Epoch 23/50, Train Loss: 0.0359, Val Loss: 0.0507\n",
      "Fold 1, Epoch 24/50, Train Loss: 0.0342, Val Loss: 0.0398\n",
      "Fold 1, Epoch 25/50, Train Loss: 0.0327, Val Loss: 0.0365\n",
      "Fold 1, Epoch 26/50, Train Loss: 0.0313, Val Loss: 0.0398\n",
      "Fold 1, Epoch 27/50, Train Loss: 0.0303, Val Loss: 0.0448\n",
      "Fold 1, Epoch 28/50, Train Loss: 0.0297, Val Loss: 0.0567\n",
      "Fold 1, Epoch 29/50, Train Loss: 0.0289, Val Loss: 0.0312\n",
      "Fold 1, Epoch 30/50, Train Loss: 0.0280, Val Loss: 0.0326\n",
      "Fold 1, Epoch 31/50, Train Loss: 0.0273, Val Loss: 0.0336\n",
      "Fold 1, Epoch 32/50, Train Loss: 0.0267, Val Loss: 0.0378\n",
      "Fold 1, Epoch 33/50, Train Loss: 0.0260, Val Loss: 0.0386\n",
      "Fold 1, Epoch 34/50, Train Loss: 0.0254, Val Loss: 0.0409\n",
      "Fold 1, Epoch 35/50, Train Loss: 0.0248, Val Loss: 0.0414\n",
      "Fold 1, Epoch 36/50, Train Loss: 0.0242, Val Loss: 0.0360\n",
      "Fold 1, Epoch 37/50, Train Loss: 0.0236, Val Loss: 0.0413\n",
      "Fold 1, Epoch 38/50, Train Loss: 0.0235, Val Loss: 0.0569\n",
      "Fold 1, Epoch 39/50, Train Loss: 0.0243, Val Loss: 0.0240\n",
      "Fold 1, Epoch 40/50, Train Loss: 0.0224, Val Loss: 0.0234\n",
      "Fold 1, Epoch 41/50, Train Loss: 0.0218, Val Loss: 0.0228\n",
      "Fold 1, Epoch 42/50, Train Loss: 0.0214, Val Loss: 0.0225\n",
      "Fold 1, Epoch 43/50, Train Loss: 0.0210, Val Loss: 0.0222\n",
      "Fold 1, Epoch 44/50, Train Loss: 0.0206, Val Loss: 0.0220\n",
      "Fold 1, Epoch 45/50, Train Loss: 0.0204, Val Loss: 0.0218\n",
      "Fold 1, Epoch 46/50, Train Loss: 0.0200, Val Loss: 0.0216\n",
      "Fold 1, Epoch 47/50, Train Loss: 0.0197, Val Loss: 0.0215\n",
      "Fold 1, Epoch 48/50, Train Loss: 0.0194, Val Loss: 0.0214\n",
      "Fold 1, Epoch 49/50, Train Loss: 0.0192, Val Loss: 0.0213\n",
      "Fold 1, Epoch 50/50, Train Loss: 0.0189, Val Loss: 0.0211\n",
      "Finished fold 1\n",
      "Fold 2, Epoch 1/50, Train Loss: 0.9691, Val Loss: 0.9248\n",
      "Fold 2, Epoch 2/50, Train Loss: 0.7778, Val Loss: 0.5910\n",
      "Fold 2, Epoch 3/50, Train Loss: 0.4084, Val Loss: 0.2867\n",
      "Fold 2, Epoch 4/50, Train Loss: 0.2210, Val Loss: 0.1683\n",
      "Fold 2, Epoch 5/50, Train Loss: 0.1399, Val Loss: 0.1216\n",
      "Fold 2, Epoch 6/50, Train Loss: 0.1081, Val Loss: 0.1001\n",
      "Fold 2, Epoch 7/50, Train Loss: 0.0905, Val Loss: 0.0856\n",
      "Fold 2, Epoch 8/50, Train Loss: 0.0784, Val Loss: 0.0746\n",
      "Fold 2, Epoch 9/50, Train Loss: 0.0691, Val Loss: 0.0660\n",
      "Fold 2, Epoch 10/50, Train Loss: 0.0619, Val Loss: 0.0595\n",
      "Fold 2, Epoch 11/50, Train Loss: 0.0559, Val Loss: 0.0538\n",
      "Fold 2, Epoch 12/50, Train Loss: 0.0509, Val Loss: 0.0492\n",
      "Fold 2, Epoch 13/50, Train Loss: 0.0469, Val Loss: 0.0456\n",
      "Fold 2, Epoch 14/50, Train Loss: 0.0436, Val Loss: 0.0426\n",
      "Fold 2, Epoch 15/50, Train Loss: 0.0407, Val Loss: 0.0399\n",
      "Fold 2, Epoch 16/50, Train Loss: 0.0381, Val Loss: 0.0377\n",
      "Fold 2, Epoch 17/50, Train Loss: 0.0359, Val Loss: 0.0357\n",
      "Fold 2, Epoch 18/50, Train Loss: 0.0339, Val Loss: 0.0340\n",
      "Fold 2, Epoch 19/50, Train Loss: 0.0323, Val Loss: 0.0325\n",
      "Fold 2, Epoch 20/50, Train Loss: 0.0309, Val Loss: 0.0312\n",
      "Fold 2, Epoch 21/50, Train Loss: 0.0296, Val Loss: 0.0300\n",
      "Fold 2, Epoch 22/50, Train Loss: 0.0286, Val Loss: 0.0289\n",
      "Fold 2, Epoch 23/50, Train Loss: 0.0276, Val Loss: 0.0279\n",
      "Fold 2, Epoch 24/50, Train Loss: 0.0267, Val Loss: 0.0270\n",
      "Fold 2, Epoch 25/50, Train Loss: 0.0259, Val Loss: 0.0261\n",
      "Fold 2, Epoch 26/50, Train Loss: 0.0251, Val Loss: 0.0254\n",
      "Fold 2, Epoch 27/50, Train Loss: 0.0244, Val Loss: 0.0248\n",
      "Fold 2, Epoch 28/50, Train Loss: 0.0237, Val Loss: 0.0240\n",
      "Fold 2, Epoch 29/50, Train Loss: 0.0230, Val Loss: 0.0233\n",
      "Fold 2, Epoch 30/50, Train Loss: 0.0224, Val Loss: 0.0227\n",
      "Fold 2, Epoch 31/50, Train Loss: 0.0218, Val Loss: 0.0223\n",
      "Fold 2, Epoch 32/50, Train Loss: 0.0213, Val Loss: 0.0219\n",
      "Fold 2, Epoch 33/50, Train Loss: 0.0207, Val Loss: 0.0214\n",
      "Fold 2, Epoch 34/50, Train Loss: 0.0203, Val Loss: 0.0211\n",
      "Fold 2, Epoch 35/50, Train Loss: 0.0198, Val Loss: 0.0207\n",
      "Fold 2, Epoch 36/50, Train Loss: 0.0194, Val Loss: 0.0202\n",
      "Fold 2, Epoch 37/50, Train Loss: 0.0191, Val Loss: 0.0200\n",
      "Fold 2, Epoch 38/50, Train Loss: 0.0187, Val Loss: 0.0195\n",
      "Fold 2, Epoch 39/50, Train Loss: 0.0184, Val Loss: 0.0193\n",
      "Fold 2, Epoch 40/50, Train Loss: 0.0181, Val Loss: 0.0189\n",
      "Fold 2, Epoch 41/50, Train Loss: 0.0177, Val Loss: 0.0186\n",
      "Fold 2, Epoch 42/50, Train Loss: 0.0175, Val Loss: 0.0182\n",
      "Fold 2, Epoch 43/50, Train Loss: 0.0172, Val Loss: 0.0179\n",
      "Fold 2, Epoch 44/50, Train Loss: 0.0169, Val Loss: 0.0176\n",
      "Fold 2, Epoch 45/50, Train Loss: 0.0167, Val Loss: 0.0173\n",
      "Fold 2, Epoch 46/50, Train Loss: 0.0165, Val Loss: 0.0172\n",
      "Fold 2, Epoch 47/50, Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Fold 2, Epoch 48/50, Train Loss: 0.0161, Val Loss: 0.0168\n",
      "Fold 2, Epoch 49/50, Train Loss: 0.0158, Val Loss: 0.0164\n",
      "Fold 2, Epoch 50/50, Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Finished fold 2\n",
      "Fold 3, Epoch 1/50, Train Loss: 0.9709, Val Loss: 0.9111\n",
      "Fold 3, Epoch 2/50, Train Loss: 0.7701, Val Loss: 0.5804\n",
      "Fold 3, Epoch 3/50, Train Loss: 0.4010, Val Loss: 0.2685\n",
      "Fold 3, Epoch 4/50, Train Loss: 0.2152, Val Loss: 0.1697\n",
      "Fold 3, Epoch 5/50, Train Loss: 0.1511, Val Loss: 0.1304\n",
      "Fold 3, Epoch 6/50, Train Loss: 0.1210, Val Loss: 0.1075\n",
      "Fold 3, Epoch 7/50, Train Loss: 0.1007, Val Loss: 0.0903\n",
      "Fold 3, Epoch 8/50, Train Loss: 0.0852, Val Loss: 0.0775\n",
      "Fold 3, Epoch 9/50, Train Loss: 0.0737, Val Loss: 0.0675\n",
      "Fold 3, Epoch 10/50, Train Loss: 0.0642, Val Loss: 0.0598\n",
      "Fold 3, Epoch 11/50, Train Loss: 0.0572, Val Loss: 0.0539\n",
      "Fold 3, Epoch 12/50, Train Loss: 0.0513, Val Loss: 0.0495\n",
      "Fold 3, Epoch 13/50, Train Loss: 0.0470, Val Loss: 0.0456\n",
      "Fold 3, Epoch 14/50, Train Loss: 0.0436, Val Loss: 0.0420\n",
      "Fold 3, Epoch 15/50, Train Loss: 0.0399, Val Loss: 0.0389\n",
      "Fold 3, Epoch 16/50, Train Loss: 0.0371, Val Loss: 0.0362\n",
      "Fold 3, Epoch 17/50, Train Loss: 0.0347, Val Loss: 0.0338\n",
      "Fold 3, Epoch 18/50, Train Loss: 0.0327, Val Loss: 0.0319\n",
      "Fold 3, Epoch 19/50, Train Loss: 0.0310, Val Loss: 0.0302\n",
      "Fold 3, Epoch 20/50, Train Loss: 0.0296, Val Loss: 0.0288\n",
      "Fold 3, Epoch 21/50, Train Loss: 0.0282, Val Loss: 0.0275\n",
      "Fold 3, Epoch 22/50, Train Loss: 0.0271, Val Loss: 0.0265\n",
      "Fold 3, Epoch 23/50, Train Loss: 0.0261, Val Loss: 0.0256\n",
      "Fold 3, Epoch 24/50, Train Loss: 0.0252, Val Loss: 0.0247\n",
      "Fold 3, Epoch 25/50, Train Loss: 0.0244, Val Loss: 0.0240\n",
      "Fold 3, Epoch 26/50, Train Loss: 0.0238, Val Loss: 0.0234\n",
      "Fold 3, Epoch 27/50, Train Loss: 0.0232, Val Loss: 0.0229\n",
      "Fold 3, Epoch 28/50, Train Loss: 0.0224, Val Loss: 0.0222\n",
      "Fold 3, Epoch 29/50, Train Loss: 0.0220, Val Loss: 0.0218\n",
      "Fold 3, Epoch 30/50, Train Loss: 0.0217, Val Loss: 0.0212\n",
      "Fold 3, Epoch 31/50, Train Loss: 0.0217, Val Loss: 0.0208\n",
      "Fold 3, Epoch 32/50, Train Loss: 0.0203, Val Loss: 0.0201\n",
      "Fold 3, Epoch 33/50, Train Loss: 0.0199, Val Loss: 0.0198\n",
      "Fold 3, Epoch 34/50, Train Loss: 0.0195, Val Loss: 0.0194\n",
      "Fold 3, Epoch 35/50, Train Loss: 0.0191, Val Loss: 0.0192\n",
      "Fold 3, Epoch 36/50, Train Loss: 0.0188, Val Loss: 0.0189\n",
      "Fold 3, Epoch 37/50, Train Loss: 0.0185, Val Loss: 0.0186\n",
      "Fold 3, Epoch 38/50, Train Loss: 0.0181, Val Loss: 0.0183\n",
      "Fold 3, Epoch 39/50, Train Loss: 0.0178, Val Loss: 0.0179\n",
      "Fold 3, Epoch 40/50, Train Loss: 0.0175, Val Loss: 0.0177\n",
      "Fold 3, Epoch 41/50, Train Loss: 0.0172, Val Loss: 0.0174\n",
      "Fold 3, Epoch 42/50, Train Loss: 0.0169, Val Loss: 0.0172\n",
      "Fold 3, Epoch 43/50, Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Fold 3, Epoch 44/50, Train Loss: 0.0164, Val Loss: 0.0167\n",
      "Fold 3, Epoch 45/50, Train Loss: 0.0162, Val Loss: 0.0165\n",
      "Fold 3, Epoch 46/50, Train Loss: 0.0160, Val Loss: 0.0163\n",
      "Fold 3, Epoch 47/50, Train Loss: 0.0158, Val Loss: 0.0161\n",
      "Fold 3, Epoch 48/50, Train Loss: 0.0156, Val Loss: 0.0159\n",
      "Fold 3, Epoch 49/50, Train Loss: 0.0153, Val Loss: 0.0157\n",
      "Fold 3, Epoch 50/50, Train Loss: 0.0151, Val Loss: 0.0154\n",
      "Finished fold 3\n",
      "Fold 1, Epoch 1/50, Train Loss: 0.8432, Val Loss: 0.6687\n",
      "Fold 1, Epoch 2/50, Train Loss: 0.5048, Val Loss: 0.4076\n",
      "Fold 1, Epoch 3/50, Train Loss: 0.3721, Val Loss: 0.3441\n",
      "Fold 1, Epoch 4/50, Train Loss: 0.3223, Val Loss: 0.3035\n",
      "Fold 1, Epoch 5/50, Train Loss: 0.2843, Val Loss: 0.2690\n",
      "Fold 1, Epoch 6/50, Train Loss: 0.2500, Val Loss: 0.2357\n",
      "Fold 1, Epoch 7/50, Train Loss: 0.2163, Val Loss: 0.2045\n",
      "Fold 1, Epoch 8/50, Train Loss: 0.1846, Val Loss: 0.1742\n",
      "Fold 1, Epoch 9/50, Train Loss: 0.1571, Val Loss: 0.1482\n",
      "Fold 1, Epoch 10/50, Train Loss: 0.1339, Val Loss: 0.1268\n",
      "Fold 1, Epoch 11/50, Train Loss: 0.1144, Val Loss: 0.1096\n",
      "Fold 1, Epoch 12/50, Train Loss: 0.0985, Val Loss: 0.0956\n",
      "Fold 1, Epoch 13/50, Train Loss: 0.0866, Val Loss: 0.0851\n",
      "Fold 1, Epoch 14/50, Train Loss: 0.0780, Val Loss: 0.0772\n",
      "Fold 1, Epoch 15/50, Train Loss: 0.0710, Val Loss: 0.0709\n",
      "Fold 1, Epoch 16/50, Train Loss: 0.0653, Val Loss: 0.0653\n",
      "Fold 1, Epoch 17/50, Train Loss: 0.0604, Val Loss: 0.0608\n",
      "Fold 1, Epoch 18/50, Train Loss: 0.0562, Val Loss: 0.0568\n",
      "Fold 1, Epoch 19/50, Train Loss: 0.0526, Val Loss: 0.0534\n",
      "Fold 1, Epoch 20/50, Train Loss: 0.0495, Val Loss: 0.0505\n",
      "Fold 1, Epoch 21/50, Train Loss: 0.0467, Val Loss: 0.0479\n",
      "Fold 1, Epoch 22/50, Train Loss: 0.0443, Val Loss: 0.0456\n",
      "Fold 1, Epoch 23/50, Train Loss: 0.0422, Val Loss: 0.0436\n",
      "Fold 1, Epoch 24/50, Train Loss: 0.0404, Val Loss: 0.0418\n",
      "Fold 1, Epoch 25/50, Train Loss: 0.0387, Val Loss: 0.0402\n",
      "Fold 1, Epoch 26/50, Train Loss: 0.0372, Val Loss: 0.0387\n",
      "Fold 1, Epoch 27/50, Train Loss: 0.0358, Val Loss: 0.0373\n",
      "Fold 1, Epoch 28/50, Train Loss: 0.0346, Val Loss: 0.0361\n",
      "Fold 1, Epoch 29/50, Train Loss: 0.0335, Val Loss: 0.0350\n",
      "Fold 1, Epoch 30/50, Train Loss: 0.0325, Val Loss: 0.0339\n",
      "Fold 1, Epoch 31/50, Train Loss: 0.0315, Val Loss: 0.0329\n",
      "Fold 1, Epoch 32/50, Train Loss: 0.0307, Val Loss: 0.0320\n",
      "Fold 1, Epoch 33/50, Train Loss: 0.0299, Val Loss: 0.0312\n",
      "Fold 1, Epoch 34/50, Train Loss: 0.0291, Val Loss: 0.0304\n",
      "Fold 1, Epoch 35/50, Train Loss: 0.0284, Val Loss: 0.0297\n",
      "Fold 1, Epoch 36/50, Train Loss: 0.0277, Val Loss: 0.0290\n",
      "Fold 1, Epoch 37/50, Train Loss: 0.0271, Val Loss: 0.0284\n",
      "Fold 1, Epoch 38/50, Train Loss: 0.0265, Val Loss: 0.0278\n",
      "Fold 1, Epoch 39/50, Train Loss: 0.0260, Val Loss: 0.0273\n",
      "Fold 1, Epoch 40/50, Train Loss: 0.0254, Val Loss: 0.0267\n",
      "Fold 1, Epoch 41/50, Train Loss: 0.0249, Val Loss: 0.0262\n",
      "Fold 1, Epoch 42/50, Train Loss: 0.0244, Val Loss: 0.0258\n",
      "Fold 1, Epoch 43/50, Train Loss: 0.0240, Val Loss: 0.0253\n",
      "Fold 1, Epoch 44/50, Train Loss: 0.0235, Val Loss: 0.0249\n",
      "Fold 1, Epoch 45/50, Train Loss: 0.0231, Val Loss: 0.0246\n",
      "Fold 1, Epoch 46/50, Train Loss: 0.0227, Val Loss: 0.0242\n",
      "Fold 1, Epoch 47/50, Train Loss: 0.0223, Val Loss: 0.0238\n",
      "Fold 1, Epoch 48/50, Train Loss: 0.0219, Val Loss: 0.0235\n",
      "Fold 1, Epoch 49/50, Train Loss: 0.0216, Val Loss: 0.0232\n",
      "Fold 1, Epoch 50/50, Train Loss: 0.0212, Val Loss: 0.0229\n",
      "Finished fold 1\n",
      "Fold 2, Epoch 1/50, Train Loss: 0.8522, Val Loss: 0.7031\n",
      "Fold 2, Epoch 2/50, Train Loss: 0.5099, Val Loss: 0.3862\n",
      "Fold 2, Epoch 3/50, Train Loss: 0.3498, Val Loss: 0.3266\n",
      "Fold 2, Epoch 4/50, Train Loss: 0.3088, Val Loss: 0.2939\n",
      "Fold 2, Epoch 5/50, Train Loss: 0.2772, Val Loss: 0.2634\n",
      "Fold 2, Epoch 6/50, Train Loss: 0.2459, Val Loss: 0.2313\n",
      "Fold 2, Epoch 7/50, Train Loss: 0.2118, Val Loss: 0.1963\n",
      "Fold 2, Epoch 8/50, Train Loss: 0.1779, Val Loss: 0.1632\n",
      "Fold 2, Epoch 9/50, Train Loss: 0.1483, Val Loss: 0.1342\n",
      "Fold 2, Epoch 10/50, Train Loss: 0.1210, Val Loss: 0.1088\n",
      "Fold 2, Epoch 11/50, Train Loss: 0.0980, Val Loss: 0.0896\n",
      "Fold 2, Epoch 12/50, Train Loss: 0.0814, Val Loss: 0.0769\n",
      "Fold 2, Epoch 13/50, Train Loss: 0.0700, Val Loss: 0.0679\n",
      "Fold 2, Epoch 14/50, Train Loss: 0.0618, Val Loss: 0.0608\n",
      "Fold 2, Epoch 15/50, Train Loss: 0.0554, Val Loss: 0.0551\n",
      "Fold 2, Epoch 16/50, Train Loss: 0.0504, Val Loss: 0.0506\n",
      "Fold 2, Epoch 17/50, Train Loss: 0.0462, Val Loss: 0.0469\n",
      "Fold 2, Epoch 18/50, Train Loss: 0.0429, Val Loss: 0.0438\n",
      "Fold 2, Epoch 19/50, Train Loss: 0.0400, Val Loss: 0.0412\n",
      "Fold 2, Epoch 20/50, Train Loss: 0.0377, Val Loss: 0.0390\n",
      "Fold 2, Epoch 21/50, Train Loss: 0.0356, Val Loss: 0.0371\n",
      "Fold 2, Epoch 22/50, Train Loss: 0.0338, Val Loss: 0.0353\n",
      "Fold 2, Epoch 23/50, Train Loss: 0.0322, Val Loss: 0.0338\n",
      "Fold 2, Epoch 24/50, Train Loss: 0.0308, Val Loss: 0.0324\n",
      "Fold 2, Epoch 25/50, Train Loss: 0.0296, Val Loss: 0.0311\n",
      "Fold 2, Epoch 26/50, Train Loss: 0.0284, Val Loss: 0.0300\n",
      "Fold 2, Epoch 27/50, Train Loss: 0.0274, Val Loss: 0.0289\n",
      "Fold 2, Epoch 28/50, Train Loss: 0.0265, Val Loss: 0.0279\n",
      "Fold 2, Epoch 29/50, Train Loss: 0.0256, Val Loss: 0.0270\n",
      "Fold 2, Epoch 30/50, Train Loss: 0.0248, Val Loss: 0.0261\n",
      "Fold 2, Epoch 31/50, Train Loss: 0.0241, Val Loss: 0.0253\n",
      "Fold 2, Epoch 32/50, Train Loss: 0.0234, Val Loss: 0.0245\n",
      "Fold 2, Epoch 33/50, Train Loss: 0.0228, Val Loss: 0.0238\n",
      "Fold 2, Epoch 34/50, Train Loss: 0.0222, Val Loss: 0.0232\n",
      "Fold 2, Epoch 35/50, Train Loss: 0.0216, Val Loss: 0.0226\n",
      "Fold 2, Epoch 36/50, Train Loss: 0.0210, Val Loss: 0.0220\n",
      "Fold 2, Epoch 37/50, Train Loss: 0.0205, Val Loss: 0.0215\n",
      "Fold 2, Epoch 38/50, Train Loss: 0.0201, Val Loss: 0.0210\n",
      "Fold 2, Epoch 39/50, Train Loss: 0.0196, Val Loss: 0.0205\n",
      "Fold 2, Epoch 40/50, Train Loss: 0.0192, Val Loss: 0.0201\n",
      "Fold 2, Epoch 41/50, Train Loss: 0.0189, Val Loss: 0.0197\n",
      "Fold 2, Epoch 42/50, Train Loss: 0.0186, Val Loss: 0.0194\n",
      "Fold 2, Epoch 43/50, Train Loss: 0.0183, Val Loss: 0.0190\n",
      "Fold 2, Epoch 44/50, Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Fold 2, Epoch 45/50, Train Loss: 0.0177, Val Loss: 0.0183\n",
      "Fold 2, Epoch 46/50, Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Fold 2, Epoch 47/50, Train Loss: 0.0171, Val Loss: 0.0176\n",
      "Fold 2, Epoch 48/50, Train Loss: 0.0169, Val Loss: 0.0172\n",
      "Fold 2, Epoch 49/50, Train Loss: 0.0166, Val Loss: 0.0169\n",
      "Fold 2, Epoch 50/50, Train Loss: 0.0164, Val Loss: 0.0165\n",
      "Finished fold 2\n",
      "Fold 3, Epoch 1/50, Train Loss: 0.8654, Val Loss: 0.7296\n",
      "Fold 3, Epoch 2/50, Train Loss: 0.5461, Val Loss: 0.4091\n",
      "Fold 3, Epoch 3/50, Train Loss: 0.3718, Val Loss: 0.3437\n",
      "Fold 3, Epoch 4/50, Train Loss: 0.3254, Val Loss: 0.3031\n",
      "Fold 3, Epoch 5/50, Train Loss: 0.2864, Val Loss: 0.2648\n",
      "Fold 3, Epoch 6/50, Train Loss: 0.2502, Val Loss: 0.2303\n",
      "Fold 3, Epoch 7/50, Train Loss: 0.2179, Val Loss: 0.1999\n",
      "Fold 3, Epoch 8/50, Train Loss: 0.1895, Val Loss: 0.1730\n",
      "Fold 3, Epoch 9/50, Train Loss: 0.1634, Val Loss: 0.1457\n",
      "Fold 3, Epoch 10/50, Train Loss: 0.1370, Val Loss: 0.1204\n",
      "Fold 3, Epoch 11/50, Train Loss: 0.1132, Val Loss: 0.0999\n",
      "Fold 3, Epoch 12/50, Train Loss: 0.0949, Val Loss: 0.0858\n",
      "Fold 3, Epoch 13/50, Train Loss: 0.0818, Val Loss: 0.0759\n",
      "Fold 3, Epoch 14/50, Train Loss: 0.0723, Val Loss: 0.0679\n",
      "Fold 3, Epoch 15/50, Train Loss: 0.0651, Val Loss: 0.0618\n",
      "Fold 3, Epoch 16/50, Train Loss: 0.0596, Val Loss: 0.0572\n",
      "Fold 3, Epoch 17/50, Train Loss: 0.0553, Val Loss: 0.0535\n",
      "Fold 3, Epoch 18/50, Train Loss: 0.0518, Val Loss: 0.0505\n",
      "Fold 3, Epoch 19/50, Train Loss: 0.0488, Val Loss: 0.0479\n",
      "Fold 3, Epoch 20/50, Train Loss: 0.0463, Val Loss: 0.0456\n",
      "Fold 3, Epoch 21/50, Train Loss: 0.0441, Val Loss: 0.0436\n",
      "Fold 3, Epoch 22/50, Train Loss: 0.0422, Val Loss: 0.0418\n",
      "Fold 3, Epoch 23/50, Train Loss: 0.0404, Val Loss: 0.0401\n",
      "Fold 3, Epoch 24/50, Train Loss: 0.0387, Val Loss: 0.0385\n",
      "Fold 3, Epoch 25/50, Train Loss: 0.0372, Val Loss: 0.0369\n",
      "Fold 3, Epoch 26/50, Train Loss: 0.0358, Val Loss: 0.0354\n",
      "Fold 3, Epoch 27/50, Train Loss: 0.0344, Val Loss: 0.0339\n",
      "Fold 3, Epoch 28/50, Train Loss: 0.0331, Val Loss: 0.0324\n",
      "Fold 3, Epoch 29/50, Train Loss: 0.0318, Val Loss: 0.0310\n",
      "Fold 3, Epoch 30/50, Train Loss: 0.0307, Val Loss: 0.0298\n",
      "Fold 3, Epoch 31/50, Train Loss: 0.0296, Val Loss: 0.0286\n",
      "Fold 3, Epoch 32/50, Train Loss: 0.0287, Val Loss: 0.0276\n",
      "Fold 3, Epoch 33/50, Train Loss: 0.0278, Val Loss: 0.0267\n",
      "Fold 3, Epoch 34/50, Train Loss: 0.0269, Val Loss: 0.0258\n",
      "Fold 3, Epoch 35/50, Train Loss: 0.0261, Val Loss: 0.0251\n",
      "Fold 3, Epoch 36/50, Train Loss: 0.0254, Val Loss: 0.0243\n",
      "Fold 3, Epoch 37/50, Train Loss: 0.0247, Val Loss: 0.0237\n",
      "Fold 3, Epoch 38/50, Train Loss: 0.0241, Val Loss: 0.0231\n",
      "Fold 3, Epoch 39/50, Train Loss: 0.0235, Val Loss: 0.0226\n",
      "Fold 3, Epoch 40/50, Train Loss: 0.0229, Val Loss: 0.0221\n",
      "Fold 3, Epoch 41/50, Train Loss: 0.0225, Val Loss: 0.0217\n",
      "Fold 3, Epoch 42/50, Train Loss: 0.0219, Val Loss: 0.0212\n",
      "Fold 3, Epoch 43/50, Train Loss: 0.0215, Val Loss: 0.0210\n",
      "Fold 3, Epoch 44/50, Train Loss: 0.0212, Val Loss: 0.0204\n",
      "Fold 3, Epoch 45/50, Train Loss: 0.0206, Val Loss: 0.0202\n",
      "Fold 3, Epoch 46/50, Train Loss: 0.0203, Val Loss: 0.0198\n",
      "Fold 3, Epoch 47/50, Train Loss: 0.0198, Val Loss: 0.0194\n",
      "Fold 3, Epoch 48/50, Train Loss: 0.0194, Val Loss: 0.0191\n",
      "Fold 3, Epoch 49/50, Train Loss: 0.0191, Val Loss: 0.0188\n",
      "Fold 3, Epoch 50/50, Train Loss: 0.0188, Val Loss: 0.0185\n",
      "Finished fold 3\n",
      "Fold 1, Epoch 1/50, Train Loss: 1.0003, Val Loss: 0.9984\n",
      "Fold 1, Epoch 2/50, Train Loss: 0.9984, Val Loss: 0.9950\n",
      "Fold 1, Epoch 3/50, Train Loss: 0.9893, Val Loss: 0.9749\n",
      "Fold 1, Epoch 4/50, Train Loss: 0.9427, Val Loss: 0.8897\n",
      "Fold 1, Epoch 5/50, Train Loss: 0.8403, Val Loss: 0.7832\n",
      "Fold 1, Epoch 6/50, Train Loss: 0.7366, Val Loss: 0.6635\n",
      "Fold 1, Epoch 7/50, Train Loss: 0.5724, Val Loss: 0.4570\n",
      "Fold 1, Epoch 8/50, Train Loss: 0.3935, Val Loss: 0.3532\n",
      "Fold 1, Epoch 9/50, Train Loss: 0.3421, Val Loss: 0.3327\n",
      "Fold 1, Epoch 10/50, Train Loss: 0.3247, Val Loss: 0.3187\n",
      "Fold 1, Epoch 11/50, Train Loss: 0.3118, Val Loss: 0.3076\n",
      "Fold 1, Epoch 12/50, Train Loss: 0.3011, Val Loss: 0.2979\n",
      "Fold 1, Epoch 13/50, Train Loss: 0.2916, Val Loss: 0.2893\n",
      "Fold 1, Epoch 14/50, Train Loss: 0.2829, Val Loss: 0.2812\n",
      "Fold 1, Epoch 15/50, Train Loss: 0.2748, Val Loss: 0.2736\n",
      "Fold 1, Epoch 16/50, Train Loss: 0.2672, Val Loss: 0.2665\n",
      "Fold 1, Epoch 17/50, Train Loss: 0.2600, Val Loss: 0.2597\n",
      "Fold 1, Epoch 18/50, Train Loss: 0.2532, Val Loss: 0.2533\n",
      "Fold 1, Epoch 19/50, Train Loss: 0.2468, Val Loss: 0.2471\n",
      "Fold 1, Epoch 20/50, Train Loss: 0.2405, Val Loss: 0.2412\n",
      "Fold 1, Epoch 21/50, Train Loss: 0.2346, Val Loss: 0.2355\n",
      "Fold 1, Epoch 22/50, Train Loss: 0.2289, Val Loss: 0.2301\n",
      "Fold 1, Epoch 23/50, Train Loss: 0.2234, Val Loss: 0.2249\n",
      "Fold 1, Epoch 24/50, Train Loss: 0.2181, Val Loss: 0.2200\n",
      "Fold 1, Epoch 25/50, Train Loss: 0.2131, Val Loss: 0.2153\n",
      "Fold 1, Epoch 26/50, Train Loss: 0.2084, Val Loss: 0.2108\n",
      "Fold 1, Epoch 27/50, Train Loss: 0.2039, Val Loss: 0.2065\n",
      "Fold 1, Epoch 28/50, Train Loss: 0.1996, Val Loss: 0.2025\n",
      "Fold 1, Epoch 29/50, Train Loss: 0.1957, Val Loss: 0.1988\n",
      "Fold 1, Epoch 30/50, Train Loss: 0.1919, Val Loss: 0.1953\n",
      "Fold 1, Epoch 31/50, Train Loss: 0.1885, Val Loss: 0.1921\n",
      "Fold 1, Epoch 32/50, Train Loss: 0.1853, Val Loss: 0.1892\n",
      "Fold 1, Epoch 33/50, Train Loss: 0.1825, Val Loss: 0.1866\n",
      "Fold 1, Epoch 34/50, Train Loss: 0.1799, Val Loss: 0.1842\n",
      "Fold 1, Epoch 35/50, Train Loss: 0.1775, Val Loss: 0.1820\n",
      "Fold 1, Epoch 36/50, Train Loss: 0.1753, Val Loss: 0.1799\n",
      "Fold 1, Epoch 37/50, Train Loss: 0.1732, Val Loss: 0.1779\n",
      "Fold 1, Epoch 38/50, Train Loss: 0.1713, Val Loss: 0.1761\n",
      "Fold 1, Epoch 39/50, Train Loss: 0.1695, Val Loss: 0.1743\n",
      "Fold 1, Epoch 40/50, Train Loss: 0.1678, Val Loss: 0.1726\n",
      "Fold 1, Epoch 41/50, Train Loss: 0.1661, Val Loss: 0.1710\n",
      "Fold 1, Epoch 42/50, Train Loss: 0.1646, Val Loss: 0.1695\n",
      "Fold 1, Epoch 43/50, Train Loss: 0.1630, Val Loss: 0.1679\n",
      "Fold 1, Epoch 44/50, Train Loss: 0.1616, Val Loss: 0.1664\n",
      "Fold 1, Epoch 45/50, Train Loss: 0.1601, Val Loss: 0.1650\n",
      "Fold 1, Epoch 46/50, Train Loss: 0.1587, Val Loss: 0.1635\n",
      "Fold 1, Epoch 47/50, Train Loss: 0.1573, Val Loss: 0.1620\n",
      "Fold 1, Epoch 48/50, Train Loss: 0.1558, Val Loss: 0.1606\n",
      "Fold 1, Epoch 49/50, Train Loss: 0.1544, Val Loss: 0.1591\n",
      "Fold 1, Epoch 50/50, Train Loss: 0.1530, Val Loss: 0.1577\n",
      "Finished fold 1\n",
      "Fold 2, Epoch 1/50, Train Loss: 1.0004, Val Loss: 1.0073\n",
      "Fold 2, Epoch 2/50, Train Loss: 0.9962, Val Loss: 1.0052\n",
      "Fold 2, Epoch 3/50, Train Loss: 0.9929, Val Loss: 0.9992\n",
      "Fold 2, Epoch 4/50, Train Loss: 0.9794, Val Loss: 0.9723\n",
      "Fold 2, Epoch 5/50, Train Loss: 0.9241, Val Loss: 0.8808\n",
      "Fold 2, Epoch 6/50, Train Loss: 0.8171, Val Loss: 0.7737\n",
      "Fold 2, Epoch 7/50, Train Loss: 0.7108, Val Loss: 0.6475\n",
      "Fold 2, Epoch 8/50, Train Loss: 0.5406, Val Loss: 0.4335\n",
      "Fold 2, Epoch 9/50, Train Loss: 0.3716, Val Loss: 0.3425\n",
      "Fold 2, Epoch 10/50, Train Loss: 0.3324, Val Loss: 0.3247\n",
      "Fold 2, Epoch 11/50, Train Loss: 0.3182, Val Loss: 0.3128\n",
      "Fold 2, Epoch 12/50, Train Loss: 0.3076, Val Loss: 0.3033\n",
      "Fold 2, Epoch 13/50, Train Loss: 0.2988, Val Loss: 0.2951\n",
      "Fold 2, Epoch 14/50, Train Loss: 0.2909, Val Loss: 0.2876\n",
      "Fold 2, Epoch 15/50, Train Loss: 0.2835, Val Loss: 0.2805\n",
      "Fold 2, Epoch 16/50, Train Loss: 0.2764, Val Loss: 0.2735\n",
      "Fold 2, Epoch 17/50, Train Loss: 0.2694, Val Loss: 0.2667\n",
      "Fold 2, Epoch 18/50, Train Loss: 0.2625, Val Loss: 0.2600\n",
      "Fold 2, Epoch 19/50, Train Loss: 0.2556, Val Loss: 0.2534\n",
      "Fold 2, Epoch 20/50, Train Loss: 0.2489, Val Loss: 0.2469\n",
      "Fold 2, Epoch 21/50, Train Loss: 0.2423, Val Loss: 0.2406\n",
      "Fold 2, Epoch 22/50, Train Loss: 0.2359, Val Loss: 0.2344\n",
      "Fold 2, Epoch 23/50, Train Loss: 0.2296, Val Loss: 0.2284\n",
      "Fold 2, Epoch 24/50, Train Loss: 0.2236, Val Loss: 0.2225\n",
      "Fold 2, Epoch 25/50, Train Loss: 0.2177, Val Loss: 0.2167\n",
      "Fold 2, Epoch 26/50, Train Loss: 0.2119, Val Loss: 0.2110\n",
      "Fold 2, Epoch 27/50, Train Loss: 0.2065, Val Loss: 0.2057\n",
      "Fold 2, Epoch 28/50, Train Loss: 0.2013, Val Loss: 0.2006\n",
      "Fold 2, Epoch 29/50, Train Loss: 0.1964, Val Loss: 0.1958\n",
      "Fold 2, Epoch 30/50, Train Loss: 0.1919, Val Loss: 0.1913\n",
      "Fold 2, Epoch 31/50, Train Loss: 0.1877, Val Loss: 0.1872\n",
      "Fold 2, Epoch 32/50, Train Loss: 0.1840, Val Loss: 0.1835\n",
      "Fold 2, Epoch 33/50, Train Loss: 0.1805, Val Loss: 0.1801\n",
      "Fold 2, Epoch 34/50, Train Loss: 0.1774, Val Loss: 0.1770\n",
      "Fold 2, Epoch 35/50, Train Loss: 0.1746, Val Loss: 0.1742\n",
      "Fold 2, Epoch 36/50, Train Loss: 0.1719, Val Loss: 0.1715\n",
      "Fold 2, Epoch 37/50, Train Loss: 0.1694, Val Loss: 0.1690\n",
      "Fold 2, Epoch 38/50, Train Loss: 0.1670, Val Loss: 0.1666\n",
      "Fold 2, Epoch 39/50, Train Loss: 0.1646, Val Loss: 0.1642\n",
      "Fold 2, Epoch 40/50, Train Loss: 0.1623, Val Loss: 0.1619\n",
      "Fold 2, Epoch 41/50, Train Loss: 0.1600, Val Loss: 0.1595\n",
      "Fold 2, Epoch 42/50, Train Loss: 0.1577, Val Loss: 0.1571\n",
      "Fold 2, Epoch 43/50, Train Loss: 0.1553, Val Loss: 0.1547\n",
      "Fold 2, Epoch 44/50, Train Loss: 0.1529, Val Loss: 0.1523\n",
      "Fold 2, Epoch 45/50, Train Loss: 0.1505, Val Loss: 0.1498\n",
      "Fold 2, Epoch 46/50, Train Loss: 0.1480, Val Loss: 0.1472\n",
      "Fold 2, Epoch 47/50, Train Loss: 0.1455, Val Loss: 0.1446\n",
      "Fold 2, Epoch 48/50, Train Loss: 0.1428, Val Loss: 0.1419\n",
      "Fold 2, Epoch 49/50, Train Loss: 0.1402, Val Loss: 0.1392\n",
      "Fold 2, Epoch 50/50, Train Loss: 0.1374, Val Loss: 0.1364\n",
      "Finished fold 2\n",
      "Fold 3, Epoch 1/50, Train Loss: 1.0834, Val Loss: 0.9966\n",
      "Fold 3, Epoch 2/50, Train Loss: 0.9980, Val Loss: 0.9947\n",
      "Fold 3, Epoch 3/50, Train Loss: 0.9961, Val Loss: 0.9926\n",
      "Fold 3, Epoch 4/50, Train Loss: 0.9925, Val Loss: 0.9870\n",
      "Fold 3, Epoch 5/50, Train Loss: 0.9831, Val Loss: 0.9727\n",
      "Fold 3, Epoch 6/50, Train Loss: 0.9602, Val Loss: 0.9395\n",
      "Fold 3, Epoch 7/50, Train Loss: 0.9136, Val Loss: 0.8812\n",
      "Fold 3, Epoch 8/50, Train Loss: 0.8475, Val Loss: 0.8119\n",
      "Fold 3, Epoch 9/50, Train Loss: 0.7719, Val Loss: 0.7260\n",
      "Fold 3, Epoch 10/50, Train Loss: 0.6572, Val Loss: 0.5700\n",
      "Fold 3, Epoch 11/50, Train Loss: 0.4776, Val Loss: 0.3935\n",
      "Fold 3, Epoch 12/50, Train Loss: 0.3551, Val Loss: 0.3296\n",
      "Fold 3, Epoch 13/50, Train Loss: 0.3223, Val Loss: 0.3145\n",
      "Fold 3, Epoch 14/50, Train Loss: 0.3108, Val Loss: 0.3046\n",
      "Fold 3, Epoch 15/50, Train Loss: 0.3020, Val Loss: 0.2961\n",
      "Fold 3, Epoch 16/50, Train Loss: 0.2942, Val Loss: 0.2882\n",
      "Fold 3, Epoch 17/50, Train Loss: 0.2868, Val Loss: 0.2806\n",
      "Fold 3, Epoch 18/50, Train Loss: 0.2796, Val Loss: 0.2732\n",
      "Fold 3, Epoch 19/50, Train Loss: 0.2726, Val Loss: 0.2659\n",
      "Fold 3, Epoch 20/50, Train Loss: 0.2656, Val Loss: 0.2587\n",
      "Fold 3, Epoch 21/50, Train Loss: 0.2587, Val Loss: 0.2517\n",
      "Fold 3, Epoch 22/50, Train Loss: 0.2519, Val Loss: 0.2449\n",
      "Fold 3, Epoch 23/50, Train Loss: 0.2453, Val Loss: 0.2382\n",
      "Fold 3, Epoch 24/50, Train Loss: 0.2388, Val Loss: 0.2318\n",
      "Fold 3, Epoch 25/50, Train Loss: 0.2325, Val Loss: 0.2255\n",
      "Fold 3, Epoch 26/50, Train Loss: 0.2263, Val Loss: 0.2193\n",
      "Fold 3, Epoch 27/50, Train Loss: 0.2203, Val Loss: 0.2134\n",
      "Fold 3, Epoch 28/50, Train Loss: 0.2145, Val Loss: 0.2076\n",
      "Fold 3, Epoch 29/50, Train Loss: 0.2089, Val Loss: 0.2021\n",
      "Fold 3, Epoch 30/50, Train Loss: 0.2034, Val Loss: 0.1968\n",
      "Fold 3, Epoch 31/50, Train Loss: 0.1982, Val Loss: 0.1918\n",
      "Fold 3, Epoch 32/50, Train Loss: 0.1933, Val Loss: 0.1870\n",
      "Fold 3, Epoch 33/50, Train Loss: 0.1886, Val Loss: 0.1824\n",
      "Fold 3, Epoch 34/50, Train Loss: 0.1842, Val Loss: 0.1782\n",
      "Fold 3, Epoch 35/50, Train Loss: 0.1801, Val Loss: 0.1742\n",
      "Fold 3, Epoch 36/50, Train Loss: 0.1762, Val Loss: 0.1705\n",
      "Fold 3, Epoch 37/50, Train Loss: 0.1725, Val Loss: 0.1670\n",
      "Fold 3, Epoch 38/50, Train Loss: 0.1690, Val Loss: 0.1636\n",
      "Fold 3, Epoch 39/50, Train Loss: 0.1656, Val Loss: 0.1603\n",
      "Fold 3, Epoch 40/50, Train Loss: 0.1623, Val Loss: 0.1570\n",
      "Fold 3, Epoch 41/50, Train Loss: 0.1590, Val Loss: 0.1538\n",
      "Fold 3, Epoch 42/50, Train Loss: 0.1557, Val Loss: 0.1506\n",
      "Fold 3, Epoch 43/50, Train Loss: 0.1524, Val Loss: 0.1473\n",
      "Fold 3, Epoch 44/50, Train Loss: 0.1490, Val Loss: 0.1440\n",
      "Fold 3, Epoch 45/50, Train Loss: 0.1456, Val Loss: 0.1406\n",
      "Fold 3, Epoch 46/50, Train Loss: 0.1421, Val Loss: 0.1372\n",
      "Fold 3, Epoch 47/50, Train Loss: 0.1385, Val Loss: 0.1336\n",
      "Fold 3, Epoch 48/50, Train Loss: 0.1349, Val Loss: 0.1300\n",
      "Fold 3, Epoch 49/50, Train Loss: 0.1312, Val Loss: 0.1264\n",
      "Fold 3, Epoch 50/50, Train Loss: 0.1274, Val Loss: 0.1227\n",
      "Finished fold 3\n",
      "Fold 1, Epoch 1/50, Train Loss: 0.9717, Val Loss: 0.9157\n",
      "Fold 1, Epoch 2/50, Train Loss: 0.8033, Val Loss: 0.6390\n",
      "Fold 1, Epoch 3/50, Train Loss: 0.4661, Val Loss: 0.3311\n",
      "Fold 1, Epoch 4/50, Train Loss: 0.2818, Val Loss: 0.2441\n",
      "Fold 1, Epoch 5/50, Train Loss: 0.2152, Val Loss: 0.1883\n",
      "Fold 1, Epoch 6/50, Train Loss: 0.1636, Val Loss: 0.1417\n",
      "Fold 1, Epoch 7/50, Train Loss: 0.1254, Val Loss: 0.1136\n",
      "Fold 1, Epoch 8/50, Train Loss: 0.1024, Val Loss: 0.0958\n",
      "Fold 1, Epoch 9/50, Train Loss: 0.0878, Val Loss: 0.0843\n",
      "Fold 1, Epoch 10/50, Train Loss: 0.0773, Val Loss: 0.0752\n",
      "Fold 1, Epoch 11/50, Train Loss: 0.0692, Val Loss: 0.0682\n",
      "Fold 1, Epoch 12/50, Train Loss: 0.0622, Val Loss: 0.0618\n",
      "Fold 1, Epoch 13/50, Train Loss: 0.0564, Val Loss: 0.0556\n",
      "Fold 1, Epoch 14/50, Train Loss: 0.0515, Val Loss: 0.0515\n",
      "Fold 1, Epoch 15/50, Train Loss: 0.0474, Val Loss: 0.0471\n",
      "Fold 1, Epoch 16/50, Train Loss: 0.0439, Val Loss: 0.0449\n",
      "Fold 1, Epoch 17/50, Train Loss: 0.0409, Val Loss: 0.0415\n",
      "Fold 1, Epoch 18/50, Train Loss: 0.0384, Val Loss: 0.0400\n",
      "Fold 1, Epoch 19/50, Train Loss: 0.0362, Val Loss: 0.0381\n",
      "Fold 1, Epoch 20/50, Train Loss: 0.0343, Val Loss: 0.0376\n",
      "Fold 1, Epoch 21/50, Train Loss: 0.0327, Val Loss: 0.0332\n",
      "Fold 1, Epoch 22/50, Train Loss: 0.0313, Val Loss: 0.0318\n",
      "Fold 1, Epoch 23/50, Train Loss: 0.0299, Val Loss: 0.0313\n",
      "Fold 1, Epoch 24/50, Train Loss: 0.0288, Val Loss: 0.0312\n",
      "Fold 1, Epoch 25/50, Train Loss: 0.0277, Val Loss: 0.0305\n",
      "Fold 1, Epoch 26/50, Train Loss: 0.0268, Val Loss: 0.0309\n",
      "Fold 1, Epoch 27/50, Train Loss: 0.0259, Val Loss: 0.0291\n",
      "Fold 1, Epoch 28/50, Train Loss: 0.0252, Val Loss: 0.0301\n",
      "Fold 1, Epoch 29/50, Train Loss: 0.0245, Val Loss: 0.0336\n",
      "Fold 1, Epoch 30/50, Train Loss: 0.0239, Val Loss: 0.0252\n",
      "Fold 1, Epoch 31/50, Train Loss: 0.0234, Val Loss: 0.0248\n",
      "Fold 1, Epoch 32/50, Train Loss: 0.0228, Val Loss: 0.0249\n",
      "Fold 1, Epoch 33/50, Train Loss: 0.0223, Val Loss: 0.0256\n",
      "Fold 1, Epoch 34/50, Train Loss: 0.0218, Val Loss: 0.0270\n",
      "Fold 1, Epoch 35/50, Train Loss: 0.0213, Val Loss: 0.0293\n",
      "Fold 1, Epoch 36/50, Train Loss: 0.0209, Val Loss: 0.0313\n",
      "Fold 1, Epoch 37/50, Train Loss: 0.0205, Val Loss: 0.0305\n",
      "Fold 1, Epoch 38/50, Train Loss: 0.0201, Val Loss: 0.0329\n",
      "Fold 1, Epoch 39/50, Train Loss: 0.0198, Val Loss: 0.0313\n",
      "Fold 1, Epoch 40/50, Train Loss: 0.0195, Val Loss: 0.0359\n",
      "Fold 1, Epoch 41/50, Train Loss: 0.0191, Val Loss: 0.0220\n",
      "Fold 1, Epoch 42/50, Train Loss: 0.0189, Val Loss: 0.0218\n",
      "Fold 1, Epoch 43/50, Train Loss: 0.0186, Val Loss: 0.0215\n",
      "Fold 1, Epoch 44/50, Train Loss: 0.0183, Val Loss: 0.0215\n",
      "Fold 1, Epoch 45/50, Train Loss: 0.0180, Val Loss: 0.0217\n",
      "Fold 1, Epoch 46/50, Train Loss: 0.0177, Val Loss: 0.0219\n",
      "Fold 1, Epoch 47/50, Train Loss: 0.0174, Val Loss: 0.0227\n",
      "Fold 1, Epoch 48/50, Train Loss: 0.0172, Val Loss: 0.0232\n",
      "Fold 1, Epoch 49/50, Train Loss: 0.0170, Val Loss: 0.0251\n",
      "Fold 1, Epoch 50/50, Train Loss: 0.0167, Val Loss: 0.0256\n",
      "Finished fold 1\n",
      "Fold 2, Epoch 1/50, Train Loss: 0.9569, Val Loss: 0.8985\n",
      "Fold 2, Epoch 2/50, Train Loss: 0.7689, Val Loss: 0.6007\n",
      "Fold 2, Epoch 3/50, Train Loss: 0.4047, Val Loss: 0.2661\n",
      "Fold 2, Epoch 4/50, Train Loss: 0.2055, Val Loss: 0.1664\n",
      "Fold 2, Epoch 5/50, Train Loss: 0.1464, Val Loss: 0.1336\n",
      "Fold 2, Epoch 6/50, Train Loss: 0.1198, Val Loss: 0.1125\n",
      "Fold 2, Epoch 7/50, Train Loss: 0.1015, Val Loss: 0.0970\n",
      "Fold 2, Epoch 8/50, Train Loss: 0.0880, Val Loss: 0.0851\n",
      "Fold 2, Epoch 9/50, Train Loss: 0.0776, Val Loss: 0.0760\n",
      "Fold 2, Epoch 10/50, Train Loss: 0.0689, Val Loss: 0.0667\n",
      "Fold 2, Epoch 11/50, Train Loss: 0.0611, Val Loss: 0.0592\n",
      "Fold 2, Epoch 12/50, Train Loss: 0.0548, Val Loss: 0.0533\n",
      "Fold 2, Epoch 13/50, Train Loss: 0.0497, Val Loss: 0.0483\n",
      "Fold 2, Epoch 14/50, Train Loss: 0.0453, Val Loss: 0.0443\n",
      "Fold 2, Epoch 15/50, Train Loss: 0.0417, Val Loss: 0.0410\n",
      "Fold 2, Epoch 16/50, Train Loss: 0.0387, Val Loss: 0.0384\n",
      "Fold 2, Epoch 17/50, Train Loss: 0.0362, Val Loss: 0.0363\n",
      "Fold 2, Epoch 18/50, Train Loss: 0.0341, Val Loss: 0.0343\n",
      "Fold 2, Epoch 19/50, Train Loss: 0.0324, Val Loss: 0.0327\n",
      "Fold 2, Epoch 20/50, Train Loss: 0.0308, Val Loss: 0.0313\n",
      "Fold 2, Epoch 21/50, Train Loss: 0.0294, Val Loss: 0.0305\n",
      "Fold 2, Epoch 22/50, Train Loss: 0.0284, Val Loss: 0.0294\n",
      "Fold 2, Epoch 23/50, Train Loss: 0.0273, Val Loss: 0.0285\n",
      "Fold 2, Epoch 24/50, Train Loss: 0.0264, Val Loss: 0.0279\n",
      "Fold 2, Epoch 25/50, Train Loss: 0.0255, Val Loss: 0.0270\n",
      "Fold 2, Epoch 26/50, Train Loss: 0.0247, Val Loss: 0.0263\n",
      "Fold 2, Epoch 27/50, Train Loss: 0.0239, Val Loss: 0.0256\n",
      "Fold 2, Epoch 28/50, Train Loss: 0.0232, Val Loss: 0.0249\n",
      "Fold 2, Epoch 29/50, Train Loss: 0.0226, Val Loss: 0.0243\n",
      "Fold 2, Epoch 30/50, Train Loss: 0.0221, Val Loss: 0.0239\n",
      "Fold 2, Epoch 31/50, Train Loss: 0.0215, Val Loss: 0.0235\n",
      "Fold 2, Epoch 32/50, Train Loss: 0.0210, Val Loss: 0.0230\n",
      "Fold 2, Epoch 33/50, Train Loss: 0.0205, Val Loss: 0.0226\n",
      "Fold 2, Epoch 34/50, Train Loss: 0.0201, Val Loss: 0.0222\n",
      "Fold 2, Epoch 35/50, Train Loss: 0.0196, Val Loss: 0.0219\n",
      "Fold 2, Epoch 36/50, Train Loss: 0.0192, Val Loss: 0.0214\n",
      "Fold 2, Epoch 37/50, Train Loss: 0.0188, Val Loss: 0.0213\n",
      "Fold 2, Epoch 38/50, Train Loss: 0.0184, Val Loss: 0.0211\n",
      "Fold 2, Epoch 39/50, Train Loss: 0.0181, Val Loss: 0.0208\n",
      "Fold 2, Epoch 40/50, Train Loss: 0.0178, Val Loss: 0.0206\n",
      "Fold 2, Epoch 41/50, Train Loss: 0.0175, Val Loss: 0.0204\n",
      "Fold 2, Epoch 42/50, Train Loss: 0.0171, Val Loss: 0.0200\n",
      "Fold 2, Epoch 43/50, Train Loss: 0.0168, Val Loss: 0.0199\n",
      "Fold 2, Epoch 44/50, Train Loss: 0.0166, Val Loss: 0.0195\n",
      "Fold 2, Epoch 45/50, Train Loss: 0.0164, Val Loss: 0.0193\n",
      "Fold 2, Epoch 46/50, Train Loss: 0.0161, Val Loss: 0.0190\n",
      "Fold 2, Epoch 47/50, Train Loss: 0.0158, Val Loss: 0.0189\n",
      "Fold 2, Epoch 48/50, Train Loss: 0.0156, Val Loss: 0.0186\n",
      "Fold 2, Epoch 49/50, Train Loss: 0.0154, Val Loss: 0.0184\n",
      "Fold 2, Epoch 50/50, Train Loss: 0.0151, Val Loss: 0.0182\n",
      "Finished fold 2\n",
      "Fold 3, Epoch 1/50, Train Loss: 0.9648, Val Loss: 0.9041\n",
      "Fold 3, Epoch 2/50, Train Loss: 0.7630, Val Loss: 0.5588\n",
      "Fold 3, Epoch 3/50, Train Loss: 0.3718, Val Loss: 0.2425\n",
      "Fold 3, Epoch 4/50, Train Loss: 0.1943, Val Loss: 0.1553\n",
      "Fold 3, Epoch 5/50, Train Loss: 0.1410, Val Loss: 0.1245\n",
      "Fold 3, Epoch 6/50, Train Loss: 0.1167, Val Loss: 0.1059\n",
      "Fold 3, Epoch 7/50, Train Loss: 0.1006, Val Loss: 0.0927\n",
      "Fold 3, Epoch 8/50, Train Loss: 0.0880, Val Loss: 0.0816\n",
      "Fold 3, Epoch 9/50, Train Loss: 0.0774, Val Loss: 0.0722\n",
      "Fold 3, Epoch 10/50, Train Loss: 0.0685, Val Loss: 0.0646\n",
      "Fold 3, Epoch 11/50, Train Loss: 0.0613, Val Loss: 0.0583\n",
      "Fold 3, Epoch 12/50, Train Loss: 0.0554, Val Loss: 0.0527\n",
      "Fold 3, Epoch 13/50, Train Loss: 0.0506, Val Loss: 0.0481\n",
      "Fold 3, Epoch 14/50, Train Loss: 0.0466, Val Loss: 0.0447\n",
      "Fold 3, Epoch 15/50, Train Loss: 0.0433, Val Loss: 0.0419\n",
      "Fold 3, Epoch 16/50, Train Loss: 0.0404, Val Loss: 0.0396\n",
      "Fold 3, Epoch 17/50, Train Loss: 0.0380, Val Loss: 0.0374\n",
      "Fold 3, Epoch 18/50, Train Loss: 0.0359, Val Loss: 0.0354\n",
      "Fold 3, Epoch 19/50, Train Loss: 0.0340, Val Loss: 0.0337\n",
      "Fold 3, Epoch 20/50, Train Loss: 0.0323, Val Loss: 0.0321\n",
      "Fold 3, Epoch 21/50, Train Loss: 0.0308, Val Loss: 0.0305\n",
      "Fold 3, Epoch 22/50, Train Loss: 0.0294, Val Loss: 0.0293\n",
      "Fold 3, Epoch 23/50, Train Loss: 0.0282, Val Loss: 0.0280\n",
      "Fold 3, Epoch 24/50, Train Loss: 0.0271, Val Loss: 0.0270\n",
      "Fold 3, Epoch 25/50, Train Loss: 0.0261, Val Loss: 0.0260\n",
      "Fold 3, Epoch 26/50, Train Loss: 0.0252, Val Loss: 0.0252\n",
      "Fold 3, Epoch 27/50, Train Loss: 0.0244, Val Loss: 0.0245\n",
      "Fold 3, Epoch 28/50, Train Loss: 0.0237, Val Loss: 0.0239\n",
      "Fold 3, Epoch 29/50, Train Loss: 0.0230, Val Loss: 0.0231\n",
      "Fold 3, Epoch 30/50, Train Loss: 0.0224, Val Loss: 0.0225\n",
      "Fold 3, Epoch 31/50, Train Loss: 0.0218, Val Loss: 0.0221\n",
      "Fold 3, Epoch 32/50, Train Loss: 0.0213, Val Loss: 0.0217\n",
      "Fold 3, Epoch 33/50, Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Fold 3, Epoch 34/50, Train Loss: 0.0203, Val Loss: 0.0207\n",
      "Fold 3, Epoch 35/50, Train Loss: 0.0199, Val Loss: 0.0203\n",
      "Fold 3, Epoch 36/50, Train Loss: 0.0194, Val Loss: 0.0199\n",
      "Fold 3, Epoch 37/50, Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Fold 3, Epoch 38/50, Train Loss: 0.0187, Val Loss: 0.0192\n",
      "Fold 3, Epoch 39/50, Train Loss: 0.0183, Val Loss: 0.0190\n",
      "Fold 3, Epoch 40/50, Train Loss: 0.0181, Val Loss: 0.0187\n",
      "Fold 3, Epoch 41/50, Train Loss: 0.0177, Val Loss: 0.0184\n",
      "Fold 3, Epoch 42/50, Train Loss: 0.0174, Val Loss: 0.0182\n",
      "Fold 3, Epoch 43/50, Train Loss: 0.0171, Val Loss: 0.0180\n",
      "Fold 3, Epoch 44/50, Train Loss: 0.0169, Val Loss: 0.0177\n",
      "Fold 3, Epoch 45/50, Train Loss: 0.0166, Val Loss: 0.0174\n",
      "Fold 3, Epoch 46/50, Train Loss: 0.0163, Val Loss: 0.0172\n",
      "Fold 3, Epoch 47/50, Train Loss: 0.0161, Val Loss: 0.0169\n",
      "Fold 3, Epoch 48/50, Train Loss: 0.0158, Val Loss: 0.0165\n",
      "Fold 3, Epoch 49/50, Train Loss: 0.0156, Val Loss: 0.0164\n",
      "Fold 3, Epoch 50/50, Train Loss: 0.0155, Val Loss: 0.0163\n",
      "Finished fold 3\n",
      "Fold 1, Epoch 1/50, Train Loss: 0.9352, Val Loss: 0.8365\n",
      "Fold 1, Epoch 2/50, Train Loss: 0.7106, Val Loss: 0.5318\n",
      "Fold 1, Epoch 3/50, Train Loss: 0.3676, Val Loss: 0.2687\n",
      "Fold 1, Epoch 4/50, Train Loss: 0.2220, Val Loss: 0.1882\n",
      "Fold 1, Epoch 5/50, Train Loss: 0.1603, Val Loss: 0.1427\n",
      "Fold 1, Epoch 6/50, Train Loss: 0.1261, Val Loss: 0.1179\n",
      "Fold 1, Epoch 7/50, Train Loss: 0.1052, Val Loss: 0.0998\n",
      "Fold 1, Epoch 8/50, Train Loss: 0.0900, Val Loss: 0.0865\n",
      "Fold 1, Epoch 9/50, Train Loss: 0.0784, Val Loss: 0.0763\n",
      "Fold 1, Epoch 10/50, Train Loss: 0.0692, Val Loss: 0.0680\n",
      "Fold 1, Epoch 11/50, Train Loss: 0.0617, Val Loss: 0.0613\n",
      "Fold 1, Epoch 12/50, Train Loss: 0.0557, Val Loss: 0.0558\n",
      "Fold 1, Epoch 13/50, Train Loss: 0.0509, Val Loss: 0.0513\n",
      "Fold 1, Epoch 14/50, Train Loss: 0.0470, Val Loss: 0.0476\n",
      "Fold 1, Epoch 15/50, Train Loss: 0.0438, Val Loss: 0.0442\n",
      "Fold 1, Epoch 16/50, Train Loss: 0.0410, Val Loss: 0.0412\n",
      "Fold 1, Epoch 17/50, Train Loss: 0.0385, Val Loss: 0.0385\n",
      "Fold 1, Epoch 18/50, Train Loss: 0.0363, Val Loss: 0.0361\n",
      "Fold 1, Epoch 19/50, Train Loss: 0.0343, Val Loss: 0.0341\n",
      "Fold 1, Epoch 20/50, Train Loss: 0.0324, Val Loss: 0.0324\n",
      "Fold 1, Epoch 21/50, Train Loss: 0.0306, Val Loss: 0.0307\n",
      "Fold 1, Epoch 22/50, Train Loss: 0.0290, Val Loss: 0.0292\n",
      "Fold 1, Epoch 23/50, Train Loss: 0.0275, Val Loss: 0.0280\n",
      "Fold 1, Epoch 24/50, Train Loss: 0.0263, Val Loss: 0.0270\n",
      "Fold 1, Epoch 25/50, Train Loss: 0.0252, Val Loss: 0.0261\n",
      "Fold 1, Epoch 26/50, Train Loss: 0.0242, Val Loss: 0.0254\n",
      "Fold 1, Epoch 27/50, Train Loss: 0.0234, Val Loss: 0.0248\n",
      "Fold 1, Epoch 28/50, Train Loss: 0.0226, Val Loss: 0.0241\n",
      "Fold 1, Epoch 29/50, Train Loss: 0.0219, Val Loss: 0.0234\n",
      "Fold 1, Epoch 30/50, Train Loss: 0.0212, Val Loss: 0.0229\n",
      "Fold 1, Epoch 31/50, Train Loss: 0.0206, Val Loss: 0.0222\n",
      "Fold 1, Epoch 32/50, Train Loss: 0.0200, Val Loss: 0.0217\n",
      "Fold 1, Epoch 33/50, Train Loss: 0.0195, Val Loss: 0.0212\n",
      "Fold 1, Epoch 34/50, Train Loss: 0.0190, Val Loss: 0.0208\n",
      "Fold 1, Epoch 35/50, Train Loss: 0.0186, Val Loss: 0.0203\n",
      "Fold 1, Epoch 36/50, Train Loss: 0.0181, Val Loss: 0.0199\n",
      "Fold 1, Epoch 37/50, Train Loss: 0.0177, Val Loss: 0.0194\n",
      "Fold 1, Epoch 38/50, Train Loss: 0.0173, Val Loss: 0.0189\n",
      "Fold 1, Epoch 39/50, Train Loss: 0.0169, Val Loss: 0.0187\n",
      "Fold 1, Epoch 40/50, Train Loss: 0.0166, Val Loss: 0.0184\n",
      "Fold 1, Epoch 41/50, Train Loss: 0.0162, Val Loss: 0.0181\n",
      "Fold 1, Epoch 42/50, Train Loss: 0.0159, Val Loss: 0.0179\n",
      "Fold 1, Epoch 43/50, Train Loss: 0.0157, Val Loss: 0.0176\n",
      "Fold 1, Epoch 44/50, Train Loss: 0.0154, Val Loss: 0.0175\n",
      "Fold 1, Epoch 45/50, Train Loss: 0.0151, Val Loss: 0.0172\n",
      "Fold 1, Epoch 46/50, Train Loss: 0.0149, Val Loss: 0.0171\n",
      "Fold 1, Epoch 47/50, Train Loss: 0.0147, Val Loss: 0.0170\n",
      "Fold 1, Epoch 48/50, Train Loss: 0.0145, Val Loss: 0.0168\n",
      "Fold 1, Epoch 49/50, Train Loss: 0.0143, Val Loss: 0.0168\n",
      "Fold 1, Epoch 50/50, Train Loss: 0.0141, Val Loss: 0.0167\n",
      "Finished fold 1\n",
      "Fold 2, Epoch 1/50, Train Loss: 0.9634, Val Loss: 0.8955\n",
      "Fold 2, Epoch 2/50, Train Loss: 0.7381, Val Loss: 0.5522\n",
      "Fold 2, Epoch 3/50, Train Loss: 0.3689, Val Loss: 0.2544\n",
      "Fold 2, Epoch 4/50, Train Loss: 0.1988, Val Loss: 0.1613\n",
      "Fold 2, Epoch 5/50, Train Loss: 0.1391, Val Loss: 0.1264\n",
      "Fold 2, Epoch 6/50, Train Loss: 0.1141, Val Loss: 0.1086\n",
      "Fold 2, Epoch 7/50, Train Loss: 0.0989, Val Loss: 0.0955\n",
      "Fold 2, Epoch 8/50, Train Loss: 0.0877, Val Loss: 0.0852\n",
      "Fold 2, Epoch 9/50, Train Loss: 0.0784, Val Loss: 0.0764\n",
      "Fold 2, Epoch 10/50, Train Loss: 0.0704, Val Loss: 0.0684\n",
      "Fold 2, Epoch 11/50, Train Loss: 0.0634, Val Loss: 0.0615\n",
      "Fold 2, Epoch 12/50, Train Loss: 0.0574, Val Loss: 0.0558\n",
      "Fold 2, Epoch 13/50, Train Loss: 0.0524, Val Loss: 0.0510\n",
      "Fold 2, Epoch 14/50, Train Loss: 0.0481, Val Loss: 0.0467\n",
      "Fold 2, Epoch 15/50, Train Loss: 0.0443, Val Loss: 0.0432\n",
      "Fold 2, Epoch 16/50, Train Loss: 0.0410, Val Loss: 0.0403\n",
      "Fold 2, Epoch 17/50, Train Loss: 0.0382, Val Loss: 0.0380\n",
      "Fold 2, Epoch 18/50, Train Loss: 0.0359, Val Loss: 0.0359\n",
      "Fold 2, Epoch 19/50, Train Loss: 0.0340, Val Loss: 0.0340\n",
      "Fold 2, Epoch 20/50, Train Loss: 0.0322, Val Loss: 0.0325\n",
      "Fold 2, Epoch 21/50, Train Loss: 0.0307, Val Loss: 0.0311\n",
      "Fold 2, Epoch 22/50, Train Loss: 0.0294, Val Loss: 0.0299\n",
      "Fold 2, Epoch 23/50, Train Loss: 0.0282, Val Loss: 0.0290\n",
      "Fold 2, Epoch 24/50, Train Loss: 0.0272, Val Loss: 0.0282\n",
      "Fold 2, Epoch 25/50, Train Loss: 0.0263, Val Loss: 0.0276\n",
      "Fold 2, Epoch 26/50, Train Loss: 0.0256, Val Loss: 0.0268\n",
      "Fold 2, Epoch 27/50, Train Loss: 0.0249, Val Loss: 0.0268\n",
      "Fold 2, Epoch 28/50, Train Loss: 0.0242, Val Loss: 0.0266\n",
      "Fold 2, Epoch 29/50, Train Loss: 0.0237, Val Loss: 0.0264\n",
      "Fold 2, Epoch 30/50, Train Loss: 0.0231, Val Loss: 0.0261\n",
      "Fold 2, Epoch 31/50, Train Loss: 0.0226, Val Loss: 0.0259\n",
      "Fold 2, Epoch 32/50, Train Loss: 0.0221, Val Loss: 0.0256\n",
      "Fold 2, Epoch 33/50, Train Loss: 0.0216, Val Loss: 0.0251\n",
      "Fold 2, Epoch 34/50, Train Loss: 0.0212, Val Loss: 0.0248\n",
      "Fold 2, Epoch 35/50, Train Loss: 0.0208, Val Loss: 0.0245\n",
      "Fold 2, Epoch 36/50, Train Loss: 0.0204, Val Loss: 0.0241\n",
      "Fold 2, Epoch 37/50, Train Loss: 0.0200, Val Loss: 0.0237\n",
      "Fold 2, Epoch 38/50, Train Loss: 0.0197, Val Loss: 0.0233\n",
      "Fold 2, Epoch 39/50, Train Loss: 0.0193, Val Loss: 0.0231\n",
      "Fold 2, Epoch 40/50, Train Loss: 0.0190, Val Loss: 0.0226\n",
      "Fold 2, Epoch 41/50, Train Loss: 0.0187, Val Loss: 0.0224\n",
      "Fold 2, Epoch 42/50, Train Loss: 0.0184, Val Loss: 0.0222\n",
      "Fold 2, Epoch 43/50, Train Loss: 0.0182, Val Loss: 0.0220\n",
      "Fold 2, Epoch 44/50, Train Loss: 0.0179, Val Loss: 0.0220\n",
      "Fold 2, Epoch 45/50, Train Loss: 0.0177, Val Loss: 0.0215\n",
      "Fold 2, Epoch 46/50, Train Loss: 0.0174, Val Loss: 0.0211\n",
      "Fold 2, Epoch 47/50, Train Loss: 0.0172, Val Loss: 0.0209\n",
      "Fold 2, Epoch 48/50, Train Loss: 0.0169, Val Loss: 0.0209\n",
      "Fold 2, Epoch 49/50, Train Loss: 0.0167, Val Loss: 0.0205\n",
      "Fold 2, Epoch 50/50, Train Loss: 0.0165, Val Loss: 0.0202\n",
      "Finished fold 2\n",
      "Fold 3, Epoch 1/50, Train Loss: 0.9631, Val Loss: 0.8852\n",
      "Fold 3, Epoch 2/50, Train Loss: 0.7400, Val Loss: 0.5456\n",
      "Fold 3, Epoch 3/50, Train Loss: 0.3704, Val Loss: 0.2551\n",
      "Fold 3, Epoch 4/50, Train Loss: 0.2111, Val Loss: 0.1709\n",
      "Fold 3, Epoch 5/50, Train Loss: 0.1533, Val Loss: 0.1331\n",
      "Fold 3, Epoch 6/50, Train Loss: 0.1225, Val Loss: 0.1096\n",
      "Fold 3, Epoch 7/50, Train Loss: 0.1014, Val Loss: 0.0923\n",
      "Fold 3, Epoch 8/50, Train Loss: 0.0858, Val Loss: 0.0794\n",
      "Fold 3, Epoch 9/50, Train Loss: 0.0740, Val Loss: 0.0697\n",
      "Fold 3, Epoch 10/50, Train Loss: 0.0651, Val Loss: 0.0623\n",
      "Fold 3, Epoch 11/50, Train Loss: 0.0582, Val Loss: 0.0565\n",
      "Fold 3, Epoch 12/50, Train Loss: 0.0528, Val Loss: 0.0515\n",
      "Fold 3, Epoch 13/50, Train Loss: 0.0484, Val Loss: 0.0471\n",
      "Fold 3, Epoch 14/50, Train Loss: 0.0446, Val Loss: 0.0432\n",
      "Fold 3, Epoch 15/50, Train Loss: 0.0414, Val Loss: 0.0402\n",
      "Fold 3, Epoch 16/50, Train Loss: 0.0387, Val Loss: 0.0378\n",
      "Fold 3, Epoch 17/50, Train Loss: 0.0363, Val Loss: 0.0359\n",
      "Fold 3, Epoch 18/50, Train Loss: 0.0344, Val Loss: 0.0342\n",
      "Fold 3, Epoch 19/50, Train Loss: 0.0327, Val Loss: 0.0329\n",
      "Fold 3, Epoch 20/50, Train Loss: 0.0312, Val Loss: 0.0317\n",
      "Fold 3, Epoch 21/50, Train Loss: 0.0298, Val Loss: 0.0306\n",
      "Fold 3, Epoch 22/50, Train Loss: 0.0286, Val Loss: 0.0295\n",
      "Fold 3, Epoch 23/50, Train Loss: 0.0275, Val Loss: 0.0286\n",
      "Fold 3, Epoch 24/50, Train Loss: 0.0265, Val Loss: 0.0275\n",
      "Fold 3, Epoch 25/50, Train Loss: 0.0256, Val Loss: 0.0268\n",
      "Fold 3, Epoch 26/50, Train Loss: 0.0248, Val Loss: 0.0260\n",
      "Fold 3, Epoch 27/50, Train Loss: 0.0240, Val Loss: 0.0253\n",
      "Fold 3, Epoch 28/50, Train Loss: 0.0233, Val Loss: 0.0248\n",
      "Fold 3, Epoch 29/50, Train Loss: 0.0227, Val Loss: 0.0240\n",
      "Fold 3, Epoch 30/50, Train Loss: 0.0221, Val Loss: 0.0237\n",
      "Fold 3, Epoch 31/50, Train Loss: 0.0216, Val Loss: 0.0230\n",
      "Fold 3, Epoch 32/50, Train Loss: 0.0212, Val Loss: 0.0227\n",
      "Fold 3, Epoch 33/50, Train Loss: 0.0205, Val Loss: 0.0220\n",
      "Fold 3, Epoch 34/50, Train Loss: 0.0200, Val Loss: 0.0218\n",
      "Fold 3, Epoch 35/50, Train Loss: 0.0196, Val Loss: 0.0213\n",
      "Fold 3, Epoch 36/50, Train Loss: 0.0192, Val Loss: 0.0209\n",
      "Fold 3, Epoch 37/50, Train Loss: 0.0189, Val Loss: 0.0206\n",
      "Fold 3, Epoch 38/50, Train Loss: 0.0185, Val Loss: 0.0203\n",
      "Fold 3, Epoch 39/50, Train Loss: 0.0182, Val Loss: 0.0199\n",
      "Fold 3, Epoch 40/50, Train Loss: 0.0178, Val Loss: 0.0196\n",
      "Fold 3, Epoch 41/50, Train Loss: 0.0175, Val Loss: 0.0193\n",
      "Fold 3, Epoch 42/50, Train Loss: 0.0172, Val Loss: 0.0189\n",
      "Fold 3, Epoch 43/50, Train Loss: 0.0169, Val Loss: 0.0186\n",
      "Fold 3, Epoch 44/50, Train Loss: 0.0167, Val Loss: 0.0184\n",
      "Fold 3, Epoch 45/50, Train Loss: 0.0165, Val Loss: 0.0182\n",
      "Fold 3, Epoch 46/50, Train Loss: 0.0162, Val Loss: 0.0179\n",
      "Fold 3, Epoch 47/50, Train Loss: 0.0160, Val Loss: 0.0177\n",
      "Fold 3, Epoch 48/50, Train Loss: 0.0157, Val Loss: 0.0174\n",
      "Fold 3, Epoch 49/50, Train Loss: 0.0155, Val Loss: 0.0173\n",
      "Fold 3, Epoch 50/50, Train Loss: 0.0153, Val Loss: 0.0169\n",
      "Finished fold 3\n",
      "Fold 1, Epoch 1/50, Train Loss: 0.9724, Val Loss: 0.9014\n",
      "Fold 1, Epoch 2/50, Train Loss: 0.7880, Val Loss: 0.6581\n",
      "Fold 1, Epoch 3/50, Train Loss: 0.4637, Val Loss: 0.3090\n",
      "Fold 1, Epoch 4/50, Train Loss: 0.2501, Val Loss: 0.2106\n",
      "Fold 1, Epoch 5/50, Train Loss: 0.1841, Val Loss: 0.1667\n",
      "Fold 1, Epoch 6/50, Train Loss: 0.1480, Val Loss: 0.1362\n",
      "Fold 1, Epoch 7/50, Train Loss: 0.1215, Val Loss: 0.1148\n",
      "Fold 1, Epoch 8/50, Train Loss: 0.1036, Val Loss: 0.1003\n",
      "Fold 1, Epoch 9/50, Train Loss: 0.0916, Val Loss: 0.0901\n",
      "Fold 1, Epoch 10/50, Train Loss: 0.0831, Val Loss: 0.0820\n",
      "Fold 1, Epoch 11/50, Train Loss: 0.0761, Val Loss: 0.0753\n",
      "Fold 1, Epoch 12/50, Train Loss: 0.0702, Val Loss: 0.0700\n",
      "Fold 1, Epoch 13/50, Train Loss: 0.0653, Val Loss: 0.0652\n",
      "Fold 1, Epoch 14/50, Train Loss: 0.0609, Val Loss: 0.0613\n",
      "Fold 1, Epoch 15/50, Train Loss: 0.0573, Val Loss: 0.0577\n",
      "Fold 1, Epoch 16/50, Train Loss: 0.0540, Val Loss: 0.0535\n",
      "Fold 1, Epoch 17/50, Train Loss: 0.0509, Val Loss: 0.0505\n",
      "Fold 1, Epoch 18/50, Train Loss: 0.0483, Val Loss: 0.0480\n",
      "Fold 1, Epoch 19/50, Train Loss: 0.0459, Val Loss: 0.0457\n",
      "Fold 1, Epoch 20/50, Train Loss: 0.0436, Val Loss: 0.0438\n",
      "Fold 1, Epoch 21/50, Train Loss: 0.0417, Val Loss: 0.0422\n",
      "Fold 1, Epoch 22/50, Train Loss: 0.0399, Val Loss: 0.0417\n",
      "Fold 1, Epoch 23/50, Train Loss: 0.0384, Val Loss: 0.0401\n",
      "Fold 1, Epoch 24/50, Train Loss: 0.0367, Val Loss: 0.0390\n",
      "Fold 1, Epoch 25/50, Train Loss: 0.0359, Val Loss: 0.0358\n",
      "Fold 1, Epoch 26/50, Train Loss: 0.0341, Val Loss: 0.0343\n",
      "Fold 1, Epoch 27/50, Train Loss: 0.0328, Val Loss: 0.0330\n",
      "Fold 1, Epoch 28/50, Train Loss: 0.0316, Val Loss: 0.0321\n",
      "Fold 1, Epoch 29/50, Train Loss: 0.0306, Val Loss: 0.0313\n",
      "Fold 1, Epoch 30/50, Train Loss: 0.0296, Val Loss: 0.0308\n",
      "Fold 1, Epoch 31/50, Train Loss: 0.0287, Val Loss: 0.0305\n",
      "Fold 1, Epoch 32/50, Train Loss: 0.0278, Val Loss: 0.0305\n",
      "Fold 1, Epoch 33/50, Train Loss: 0.0272, Val Loss: 0.0314\n",
      "Fold 1, Epoch 34/50, Train Loss: 0.0264, Val Loss: 0.0310\n",
      "Fold 1, Epoch 35/50, Train Loss: 0.0260, Val Loss: 0.0339\n",
      "Fold 1, Epoch 36/50, Train Loss: 0.0256, Val Loss: 0.0277\n",
      "Fold 1, Epoch 37/50, Train Loss: 0.0248, Val Loss: 0.0268\n",
      "Fold 1, Epoch 38/50, Train Loss: 0.0242, Val Loss: 0.0264\n",
      "Fold 1, Epoch 39/50, Train Loss: 0.0236, Val Loss: 0.0262\n",
      "Fold 1, Epoch 40/50, Train Loss: 0.0232, Val Loss: 0.0258\n",
      "Fold 1, Epoch 41/50, Train Loss: 0.0227, Val Loss: 0.0255\n",
      "Fold 1, Epoch 42/50, Train Loss: 0.0223, Val Loss: 0.0252\n",
      "Fold 1, Epoch 43/50, Train Loss: 0.0219, Val Loss: 0.0249\n",
      "Fold 1, Epoch 44/50, Train Loss: 0.0215, Val Loss: 0.0247\n",
      "Fold 1, Epoch 45/50, Train Loss: 0.0212, Val Loss: 0.0247\n",
      "Fold 1, Epoch 46/50, Train Loss: 0.0209, Val Loss: 0.0244\n",
      "Fold 1, Epoch 47/50, Train Loss: 0.0206, Val Loss: 0.0242\n",
      "Fold 1, Epoch 48/50, Train Loss: 0.0203, Val Loss: 0.0239\n",
      "Fold 1, Epoch 49/50, Train Loss: 0.0199, Val Loss: 0.0235\n",
      "Fold 1, Epoch 50/50, Train Loss: 0.0196, Val Loss: 0.0232\n",
      "Finished fold 1\n",
      "Fold 2, Epoch 1/50, Train Loss: 0.9311, Val Loss: 0.8367\n",
      "Fold 2, Epoch 2/50, Train Loss: 0.6940, Val Loss: 0.5285\n",
      "Fold 2, Epoch 3/50, Train Loss: 0.3684, Val Loss: 0.2568\n",
      "Fold 2, Epoch 4/50, Train Loss: 0.2022, Val Loss: 0.1659\n",
      "Fold 2, Epoch 5/50, Train Loss: 0.1451, Val Loss: 0.1312\n",
      "Fold 2, Epoch 6/50, Train Loss: 0.1192, Val Loss: 0.1115\n",
      "Fold 2, Epoch 7/50, Train Loss: 0.1013, Val Loss: 0.0960\n",
      "Fold 2, Epoch 8/50, Train Loss: 0.0869, Val Loss: 0.0834\n",
      "Fold 2, Epoch 9/50, Train Loss: 0.0757, Val Loss: 0.0732\n",
      "Fold 2, Epoch 10/50, Train Loss: 0.0667, Val Loss: 0.0644\n",
      "Fold 2, Epoch 11/50, Train Loss: 0.0593, Val Loss: 0.0575\n",
      "Fold 2, Epoch 12/50, Train Loss: 0.0534, Val Loss: 0.0519\n",
      "Fold 2, Epoch 13/50, Train Loss: 0.0486, Val Loss: 0.0474\n",
      "Fold 2, Epoch 14/50, Train Loss: 0.0447, Val Loss: 0.0437\n",
      "Fold 2, Epoch 15/50, Train Loss: 0.0416, Val Loss: 0.0408\n",
      "Fold 2, Epoch 16/50, Train Loss: 0.0390, Val Loss: 0.0384\n",
      "Fold 2, Epoch 17/50, Train Loss: 0.0370, Val Loss: 0.0364\n",
      "Fold 2, Epoch 18/50, Train Loss: 0.0352, Val Loss: 0.0347\n",
      "Fold 2, Epoch 19/50, Train Loss: 0.0335, Val Loss: 0.0332\n",
      "Fold 2, Epoch 20/50, Train Loss: 0.0321, Val Loss: 0.0319\n",
      "Fold 2, Epoch 21/50, Train Loss: 0.0308, Val Loss: 0.0306\n",
      "Fold 2, Epoch 22/50, Train Loss: 0.0296, Val Loss: 0.0295\n",
      "Fold 2, Epoch 23/50, Train Loss: 0.0286, Val Loss: 0.0286\n",
      "Fold 2, Epoch 24/50, Train Loss: 0.0276, Val Loss: 0.0278\n",
      "Fold 2, Epoch 25/50, Train Loss: 0.0268, Val Loss: 0.0270\n",
      "Fold 2, Epoch 26/50, Train Loss: 0.0260, Val Loss: 0.0264\n",
      "Fold 2, Epoch 27/50, Train Loss: 0.0252, Val Loss: 0.0257\n",
      "Fold 2, Epoch 28/50, Train Loss: 0.0245, Val Loss: 0.0250\n",
      "Fold 2, Epoch 29/50, Train Loss: 0.0239, Val Loss: 0.0244\n",
      "Fold 2, Epoch 30/50, Train Loss: 0.0232, Val Loss: 0.0238\n",
      "Fold 2, Epoch 31/50, Train Loss: 0.0227, Val Loss: 0.0234\n",
      "Fold 2, Epoch 32/50, Train Loss: 0.0221, Val Loss: 0.0228\n",
      "Fold 2, Epoch 33/50, Train Loss: 0.0216, Val Loss: 0.0223\n",
      "Fold 2, Epoch 34/50, Train Loss: 0.0212, Val Loss: 0.0218\n",
      "Fold 2, Epoch 35/50, Train Loss: 0.0207, Val Loss: 0.0215\n",
      "Fold 2, Epoch 36/50, Train Loss: 0.0203, Val Loss: 0.0210\n",
      "Fold 2, Epoch 37/50, Train Loss: 0.0199, Val Loss: 0.0206\n",
      "Fold 2, Epoch 38/50, Train Loss: 0.0195, Val Loss: 0.0202\n",
      "Fold 2, Epoch 39/50, Train Loss: 0.0191, Val Loss: 0.0198\n",
      "Fold 2, Epoch 40/50, Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Fold 2, Epoch 41/50, Train Loss: 0.0184, Val Loss: 0.0191\n",
      "Fold 2, Epoch 42/50, Train Loss: 0.0180, Val Loss: 0.0186\n",
      "Fold 2, Epoch 43/50, Train Loss: 0.0177, Val Loss: 0.0183\n",
      "Fold 2, Epoch 44/50, Train Loss: 0.0173, Val Loss: 0.0180\n",
      "Fold 2, Epoch 45/50, Train Loss: 0.0170, Val Loss: 0.0176\n",
      "Fold 2, Epoch 46/50, Train Loss: 0.0167, Val Loss: 0.0172\n",
      "Fold 2, Epoch 47/50, Train Loss: 0.0163, Val Loss: 0.0169\n",
      "Fold 2, Epoch 48/50, Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Fold 2, Epoch 49/50, Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Fold 2, Epoch 50/50, Train Loss: 0.0155, Val Loss: 0.0159\n",
      "Finished fold 2\n",
      "Fold 3, Epoch 1/50, Train Loss: 0.9522, Val Loss: 0.8705\n",
      "Fold 3, Epoch 2/50, Train Loss: 0.7595, Val Loss: 0.6280\n",
      "Fold 3, Epoch 3/50, Train Loss: 0.4484, Val Loss: 0.2911\n",
      "Fold 3, Epoch 4/50, Train Loss: 0.2186, Val Loss: 0.1625\n",
      "Fold 3, Epoch 5/50, Train Loss: 0.1387, Val Loss: 0.1175\n",
      "Fold 3, Epoch 6/50, Train Loss: 0.1065, Val Loss: 0.0954\n",
      "Fold 3, Epoch 7/50, Train Loss: 0.0889, Val Loss: 0.0815\n",
      "Fold 3, Epoch 8/50, Train Loss: 0.0769, Val Loss: 0.0710\n",
      "Fold 3, Epoch 9/50, Train Loss: 0.0673, Val Loss: 0.0622\n",
      "Fold 3, Epoch 10/50, Train Loss: 0.0593, Val Loss: 0.0549\n",
      "Fold 3, Epoch 11/50, Train Loss: 0.0529, Val Loss: 0.0492\n",
      "Fold 3, Epoch 12/50, Train Loss: 0.0478, Val Loss: 0.0448\n",
      "Fold 3, Epoch 13/50, Train Loss: 0.0437, Val Loss: 0.0414\n",
      "Fold 3, Epoch 14/50, Train Loss: 0.0403, Val Loss: 0.0385\n",
      "Fold 3, Epoch 15/50, Train Loss: 0.0375, Val Loss: 0.0359\n",
      "Fold 3, Epoch 16/50, Train Loss: 0.0350, Val Loss: 0.0339\n",
      "Fold 3, Epoch 17/50, Train Loss: 0.0330, Val Loss: 0.0322\n",
      "Fold 3, Epoch 18/50, Train Loss: 0.0312, Val Loss: 0.0308\n",
      "Fold 3, Epoch 19/50, Train Loss: 0.0297, Val Loss: 0.0297\n",
      "Fold 3, Epoch 20/50, Train Loss: 0.0283, Val Loss: 0.0285\n",
      "Fold 3, Epoch 21/50, Train Loss: 0.0271, Val Loss: 0.0274\n",
      "Fold 3, Epoch 22/50, Train Loss: 0.0260, Val Loss: 0.0263\n",
      "Fold 3, Epoch 23/50, Train Loss: 0.0250, Val Loss: 0.0252\n",
      "Fold 3, Epoch 24/50, Train Loss: 0.0242, Val Loss: 0.0243\n",
      "Fold 3, Epoch 25/50, Train Loss: 0.0234, Val Loss: 0.0236\n",
      "Fold 3, Epoch 26/50, Train Loss: 0.0227, Val Loss: 0.0230\n",
      "Fold 3, Epoch 27/50, Train Loss: 0.0220, Val Loss: 0.0222\n",
      "Fold 3, Epoch 28/50, Train Loss: 0.0214, Val Loss: 0.0216\n",
      "Fold 3, Epoch 29/50, Train Loss: 0.0209, Val Loss: 0.0211\n",
      "Fold 3, Epoch 30/50, Train Loss: 0.0204, Val Loss: 0.0206\n",
      "Fold 3, Epoch 31/50, Train Loss: 0.0199, Val Loss: 0.0201\n",
      "Fold 3, Epoch 32/50, Train Loss: 0.0195, Val Loss: 0.0197\n",
      "Fold 3, Epoch 33/50, Train Loss: 0.0191, Val Loss: 0.0192\n",
      "Fold 3, Epoch 34/50, Train Loss: 0.0187, Val Loss: 0.0189\n",
      "Fold 3, Epoch 35/50, Train Loss: 0.0183, Val Loss: 0.0186\n",
      "Fold 3, Epoch 36/50, Train Loss: 0.0180, Val Loss: 0.0182\n",
      "Fold 3, Epoch 37/50, Train Loss: 0.0177, Val Loss: 0.0179\n",
      "Fold 3, Epoch 38/50, Train Loss: 0.0174, Val Loss: 0.0175\n",
      "Fold 3, Epoch 39/50, Train Loss: 0.0171, Val Loss: 0.0173\n",
      "Fold 3, Epoch 40/50, Train Loss: 0.0168, Val Loss: 0.0167\n",
      "Fold 3, Epoch 41/50, Train Loss: 0.0165, Val Loss: 0.0166\n",
      "Fold 3, Epoch 42/50, Train Loss: 0.0163, Val Loss: 0.0163\n",
      "Fold 3, Epoch 43/50, Train Loss: 0.0161, Val Loss: 0.0161\n",
      "Fold 3, Epoch 44/50, Train Loss: 0.0158, Val Loss: 0.0157\n",
      "Fold 3, Epoch 45/50, Train Loss: 0.0156, Val Loss: 0.0156\n",
      "Fold 3, Epoch 46/50, Train Loss: 0.0154, Val Loss: 0.0155\n",
      "Fold 3, Epoch 47/50, Train Loss: 0.0152, Val Loss: 0.0154\n",
      "Fold 3, Epoch 48/50, Train Loss: 0.0150, Val Loss: 0.0153\n",
      "Fold 3, Epoch 49/50, Train Loss: 0.0148, Val Loss: 0.0150\n",
      "Fold 3, Epoch 50/50, Train Loss: 0.0146, Val Loss: 0.0149\n",
      "Finished fold 3\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Final training",
   "id": "6cd34b35dae09ed4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:19:45.540850Z",
     "start_time": "2025-01-20T15:19:45.529906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_final(\n",
    "        tb_writer,\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        elems,\n",
    "        supports,\n",
    "        out_size=1,\n",
    "        hidden_size=70,\n",
    "        n_hidden=3,\n",
    "        activation=nn.ReLU,\n",
    "        activation_params={},\n",
    "        learning_rate=1e-3,\n",
    "        n_epochs=30,\n",
    "        save_path=None\n",
    "):\n",
    "    in_size = len(train_dataset[0][0])\n",
    "    N_EPOCHS = n_epochs\n",
    "    BATCH_SIZE = 2048  # Not an hyperparameter\n",
    "    LEARNING_RATE = learning_rate\n",
    "\n",
    "    train_losses = []\n",
    "    train_data_losses = []\n",
    "    train_physics_losses = []\n",
    "\n",
    "    test_losses = []\n",
    "    test_data_losses = []\n",
    "    test_physics_losses = []\n",
    "\n",
    "    # Dataloader\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, )\n",
    "\n",
    "    # Training scaler\n",
    "    x_scaler = StandardScaler(in_size).to(device)\n",
    "    y_scaler = StandardScaler(out_size).to(device)\n",
    "    for x, y, _, _, _ in train_loader:\n",
    "        x_scaler.partial_fit(x.to(device))\n",
    "        y_scaler.partial_fit(y.to(device))\n",
    "\n",
    "    # Model setup\n",
    "    model = MultiLayerPerceptron(in_size, out_size, hidden_size, n_hidden, activation,\n",
    "                                 activation_params).to(device)\n",
    "\n",
    "    data_criterion = nn.MSELoss()\n",
    "    physics_criterion = StiffnessToLoadLoss()\n",
    "\n",
    "    physics_loss_scale = None\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
    "    best_test_loss = float('inf')\n",
    "    # Model training\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        # Train\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_data_loss = 0.0\n",
    "        total_physics_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            data, ea, nodes, u, q = batch\n",
    "\n",
    "            data, ea = data.to(device), ea.to(device)\n",
    "            nodes, u, q = nodes.to(device), u.to(device), q.to(device)\n",
    "\n",
    "            z_data = x_scaler.transform(data)\n",
    "            z_ea = y_scaler.transform(ea)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            z_pred_ea = model(z_data)\n",
    "\n",
    "            ea_pred = y_scaler.inverse_transform(z_pred_ea)\n",
    "            stiffness_pred = construct_k_from_ea(ea_pred, nodes, elems, supports, device=device)\n",
    "\n",
    "            data_loss = data_criterion(z_pred_ea, z_ea)\n",
    "            physics_loss = physics_criterion(stiffness_pred, u, q)\n",
    "\n",
    "            if physics_loss_scale is None: physics_loss_scale = physics_loss.item()\n",
    "\n",
    "            physics_loss /= physics_loss_scale\n",
    "\n",
    "            loss = data_loss + physics_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_data_loss += data_loss.item()\n",
    "            total_physics_loss += physics_loss.item()\n",
    "\n",
    "        # Log\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_physics_loss = total_physics_loss / len(train_loader)\n",
    "        train_data_loss = total_data_loss / len(train_loader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_physics_losses.append(train_physics_loss)\n",
    "        train_data_losses.append(train_data_loss)\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_data_loss = 0.0\n",
    "        total_physics_loss = 0.0\n",
    "        total_MAE = 0.0\n",
    "        total_MAER = 0.0\n",
    "        total_MSE = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                data, ea, nodes, u, q = batch\n",
    "\n",
    "                data, ea = data.to(device), ea.to(device)\n",
    "                nodes, u, q = nodes.to(device), u.to(device), q.to(device)\n",
    "\n",
    "                z_data = x_scaler.transform(data)\n",
    "                z_ea = y_scaler.transform(ea)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                z_pred_ea = model(z_data)\n",
    "\n",
    "                ea_pred = y_scaler.inverse_transform(z_pred_ea)\n",
    "                stiffness_pred = construct_k_from_ea(ea_pred, nodes, elems, supports, device=device)\n",
    "\n",
    "                data_loss = data_criterion(z_pred_ea, z_ea)\n",
    "                physics_loss = physics_criterion(stiffness_pred, u, q) / physics_loss_scale\n",
    "\n",
    "                loss = data_loss + physics_loss\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_data_loss += data_loss.item()\n",
    "                total_physics_loss += physics_loss.item()\n",
    "\n",
    "                MAE = torch.abs(ea-ea_pred).mean()\n",
    "                MAER = torch.abs((ea-ea_pred)/ea).mean()\n",
    "                MSE = data_criterion(ea, ea_pred)\n",
    "\n",
    "                total_MAE += MAE.item()\n",
    "                total_MAER += MAER.item()\n",
    "                total_MSE += MSE.item()\n",
    "\n",
    "\n",
    "        # Log\n",
    "        test_loss = total_loss / len(test_loader)\n",
    "        test_physics_loss = total_physics_loss / len(test_loader)\n",
    "        test_data_loss = total_data_loss / len(test_loader)\n",
    "\n",
    "        test_mse = total_MSE / len(test_loader)\n",
    "        test_mae = total_MAE / len(test_loader)\n",
    "        test_maer = total_MAER / len(test_loader)\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "        test_physics_losses.append(test_physics_loss)\n",
    "        test_data_losses.append(test_data_loss)\n",
    "\n",
    "        tb_writer.add_scalar(f'Loss/total/train', train_loss, epoch)\n",
    "        tb_writer.add_scalar(f'Loss/total/test', test_loss, epoch)\n",
    "        tb_writer.add_scalar(f'Loss/data/train', train_data_loss, epoch)\n",
    "        tb_writer.add_scalar(f'Loss/data/test', test_data_loss, epoch)\n",
    "        tb_writer.add_scalar(f'Loss/physics/train', train_physics_loss, epoch)\n",
    "        tb_writer.add_scalar(f'Loss/physics/test', test_physics_loss, epoch)\n",
    "\n",
    "        tb_writer.add_scalar(f'Loss/MAE/test', test_mae, epoch)\n",
    "        tb_writer.add_scalar(f'Loss/MAER/test', test_maer, epoch)\n",
    "        tb_writer.add_scalar(f'Loss/MSE/test', test_mse, epoch)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{N_EPOCHS}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test MAER: {test_maer * 100:.4f} %, Test MAE: {test_mae * 1e-6:.4f} MN, Test MSE: {test_mse * 1e-12:.4f} MN^2\",\n",
    "            end=' ')\n",
    "\n",
    "        if test_loss < best_test_loss and save_path is not None and epoch > 50:\n",
    "            best_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), f\"{save_path}/best_model_{test_loss}.pth\")\n",
    "            print(\"-> MODEL SAVED\")\n",
    "        else:\n",
    "            print(\"\")\n"
   ],
   "id": "db7089e3d5d96992",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:52:16.897007Z",
     "start_time": "2025-01-20T15:19:47.368270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = 'beam'\n",
    "writer = SummaryWriter(f\"./runs/final/beam/FEI_MLP/4/60/ReLU/lr_0.0004\")\n",
    "save_path = \"./runs/final/beam/FEI_MLP/4/60/ReLU/lr_0.0004/SAVE\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model_beam = main_final(writer, _train_ds[dataset_name], _test_ds[dataset_name], connectivity[dataset_name], support[dataset_name], hidden_size=60, n_hidden=4, activation=nn.ReLU,\n",
    "                        activation_params={}, learning_rate=0.0004, n_epochs=200, save_path=save_path)\n",
    "writer.close()"
   ],
   "id": "270f4903508d10ba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aflamant/miniconda3/envs/memoire/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 1.5929, Test Loss: 1.2005, Test MAER: 69.6500 %, Test MAE: 379.4090 MN, Test MSE: 206471.9044 MN^2 \n",
      "Epoch 2/200, Train Loss: 0.6806, Test Loss: 0.2310, Test MAER: 26.2230 %, Test MAE: 147.1438 MN, Test MSE: 48211.1430 MN^2 \n",
      "Epoch 3/200, Train Loss: 0.1603, Test Loss: 0.1302, Test MAER: 18.8771 %, Test MAE: 106.4445 MN, Test MSE: 28280.7455 MN^2 \n",
      "Epoch 4/200, Train Loss: 0.1158, Test Loss: 0.1050, Test MAER: 16.5011 %, Test MAE: 92.9427 MN, Test MSE: 23160.4766 MN^2 \n",
      "Epoch 5/200, Train Loss: 0.0938, Test Loss: 0.0870, Test MAER: 14.6564 %, Test MAE: 81.9032 MN, Test MSE: 19444.6719 MN^2 \n",
      "Epoch 6/200, Train Loss: 0.0777, Test Loss: 0.0717, Test MAER: 12.5897 %, Test MAE: 70.7119 MN, Test MSE: 16166.5147 MN^2 \n",
      "Epoch 7/200, Train Loss: 0.0659, Test Loss: 0.0604, Test MAER: 11.0212 %, Test MAE: 62.0086 MN, Test MSE: 13704.4182 MN^2 \n",
      "Epoch 8/200, Train Loss: 0.0562, Test Loss: 0.0523, Test MAER: 9.4743 %, Test MAE: 54.9516 MN, Test MSE: 11898.7221 MN^2 \n",
      "Epoch 9/200, Train Loss: 0.0491, Test Loss: 0.0469, Test MAER: 8.5142 %, Test MAE: 50.5734 MN, Test MSE: 10764.1007 MN^2 \n",
      "Epoch 10/200, Train Loss: 0.0434, Test Loss: 0.0429, Test MAER: 7.8566 %, Test MAE: 47.3470 MN, Test MSE: 9909.7168 MN^2 \n",
      "Epoch 11/200, Train Loss: 0.0388, Test Loss: 0.0403, Test MAER: 7.5933 %, Test MAE: 45.7730 MN, Test MSE: 9244.4466 MN^2 \n",
      "Epoch 12/200, Train Loss: 0.0371, Test Loss: 0.0372, Test MAER: 6.9084 %, Test MAE: 42.2485 MN, Test MSE: 8669.0673 MN^2 \n",
      "Epoch 13/200, Train Loss: 0.0327, Test Loss: 0.0345, Test MAER: 6.5783 %, Test MAE: 40.5854 MN, Test MSE: 8124.3303 MN^2 \n",
      "Epoch 14/200, Train Loss: 0.0302, Test Loss: 0.0326, Test MAER: 6.3139 %, Test MAE: 39.1820 MN, Test MSE: 7675.0008 MN^2 \n",
      "Epoch 15/200, Train Loss: 0.0282, Test Loss: 0.0312, Test MAER: 6.0871 %, Test MAE: 38.1050 MN, Test MSE: 7372.1282 MN^2 \n",
      "Epoch 16/200, Train Loss: 0.0266, Test Loss: 0.0301, Test MAER: 5.8515 %, Test MAE: 37.0601 MN, Test MSE: 7136.1327 MN^2 \n",
      "Epoch 17/200, Train Loss: 0.0252, Test Loss: 0.0291, Test MAER: 5.6790 %, Test MAE: 36.3639 MN, Test MSE: 6935.3410 MN^2 \n",
      "Epoch 18/200, Train Loss: 0.0239, Test Loss: 0.0283, Test MAER: 5.5345 %, Test MAE: 35.7309 MN, Test MSE: 6745.7815 MN^2 \n",
      "Epoch 19/200, Train Loss: 0.0228, Test Loss: 0.0275, Test MAER: 5.4276 %, Test MAE: 35.1836 MN, Test MSE: 6554.8382 MN^2 \n",
      "Epoch 20/200, Train Loss: 0.0219, Test Loss: 0.0268, Test MAER: 5.3339 %, Test MAE: 34.8425 MN, Test MSE: 6373.7866 MN^2 \n",
      "Epoch 21/200, Train Loss: 0.0210, Test Loss: 0.0260, Test MAER: 5.2137 %, Test MAE: 34.3151 MN, Test MSE: 6218.9273 MN^2 \n",
      "Epoch 22/200, Train Loss: 0.0202, Test Loss: 0.0254, Test MAER: 5.1141 %, Test MAE: 33.7714 MN, Test MSE: 6078.0713 MN^2 \n",
      "Epoch 23/200, Train Loss: 0.0195, Test Loss: 0.0247, Test MAER: 5.0164 %, Test MAE: 33.3734 MN, Test MSE: 5930.8072 MN^2 \n",
      "Epoch 24/200, Train Loss: 0.0189, Test Loss: 0.0241, Test MAER: 4.8827 %, Test MAE: 32.7691 MN, Test MSE: 5814.2238 MN^2 \n",
      "Epoch 25/200, Train Loss: 0.0184, Test Loss: 0.0235, Test MAER: 4.8976 %, Test MAE: 31.9270 MN, Test MSE: 5627.5179 MN^2 \n",
      "Epoch 26/200, Train Loss: 0.0180, Test Loss: 0.0229, Test MAER: 4.7981 %, Test MAE: 31.4996 MN, Test MSE: 5460.6674 MN^2 \n",
      "Epoch 27/200, Train Loss: 0.0174, Test Loss: 0.0225, Test MAER: 4.8085 %, Test MAE: 32.0574 MN, Test MSE: 5337.2760 MN^2 \n",
      "Epoch 28/200, Train Loss: 0.0171, Test Loss: 0.0215, Test MAER: 4.5844 %, Test MAE: 30.1978 MN, Test MSE: 5144.0997 MN^2 \n",
      "Epoch 29/200, Train Loss: 0.0165, Test Loss: 0.0204, Test MAER: 4.3455 %, Test MAE: 28.5202 MN, Test MSE: 4937.5763 MN^2 \n",
      "Epoch 30/200, Train Loss: 0.0158, Test Loss: 0.0193, Test MAER: 4.2674 %, Test MAE: 28.2424 MN, Test MSE: 4623.3469 MN^2 \n",
      "Epoch 31/200, Train Loss: 0.0154, Test Loss: 0.0178, Test MAER: 4.0700 %, Test MAE: 26.6489 MN, Test MSE: 4291.8763 MN^2 \n",
      "Epoch 32/200, Train Loss: 0.0148, Test Loss: 0.0164, Test MAER: 3.9067 %, Test MAE: 24.9632 MN, Test MSE: 3957.0597 MN^2 \n",
      "Epoch 33/200, Train Loss: 0.0144, Test Loss: 0.0152, Test MAER: 3.7895 %, Test MAE: 23.7490 MN, Test MSE: 3662.2601 MN^2 \n",
      "Epoch 34/200, Train Loss: 0.0139, Test Loss: 0.0145, Test MAER: 3.6867 %, Test MAE: 22.9956 MN, Test MSE: 3488.8520 MN^2 \n",
      "Epoch 35/200, Train Loss: 0.0136, Test Loss: 0.0141, Test MAER: 3.6315 %, Test MAE: 22.5998 MN, Test MSE: 3383.8876 MN^2 \n",
      "Epoch 36/200, Train Loss: 0.0133, Test Loss: 0.0138, Test MAER: 3.5973 %, Test MAE: 22.3764 MN, Test MSE: 3325.7191 MN^2 \n",
      "Epoch 37/200, Train Loss: 0.0131, Test Loss: 0.0137, Test MAER: 3.5565 %, Test MAE: 22.2908 MN, Test MSE: 3299.6742 MN^2 \n",
      "Epoch 38/200, Train Loss: 0.0129, Test Loss: 0.0135, Test MAER: 3.5078 %, Test MAE: 22.0581 MN, Test MSE: 3241.9632 MN^2 \n",
      "Epoch 39/200, Train Loss: 0.0127, Test Loss: 0.0132, Test MAER: 3.4767 %, Test MAE: 21.8582 MN, Test MSE: 3192.7187 MN^2 \n",
      "Epoch 40/200, Train Loss: 0.0125, Test Loss: 0.0129, Test MAER: 3.4426 %, Test MAE: 21.5458 MN, Test MSE: 3115.7580 MN^2 \n",
      "Epoch 41/200, Train Loss: 0.0122, Test Loss: 0.0126, Test MAER: 3.3855 %, Test MAE: 21.1658 MN, Test MSE: 3033.9839 MN^2 \n",
      "Epoch 42/200, Train Loss: 0.0120, Test Loss: 0.0124, Test MAER: 3.3356 %, Test MAE: 20.8727 MN, Test MSE: 2971.2946 MN^2 \n",
      "Epoch 43/200, Train Loss: 0.0118, Test Loss: 0.0120, Test MAER: 3.3158 %, Test MAE: 20.5775 MN, Test MSE: 2873.6209 MN^2 \n",
      "Epoch 44/200, Train Loss: 0.0115, Test Loss: 0.0116, Test MAER: 3.2850 %, Test MAE: 20.2009 MN, Test MSE: 2761.8556 MN^2 \n",
      "Epoch 45/200, Train Loss: 0.0115, Test Loss: 0.0115, Test MAER: 3.3931 %, Test MAE: 20.5273 MN, Test MSE: 2672.8573 MN^2 \n",
      "Epoch 46/200, Train Loss: 0.0114, Test Loss: 0.0108, Test MAER: 3.1393 %, Test MAE: 19.0109 MN, Test MSE: 2560.5524 MN^2 \n",
      "Epoch 47/200, Train Loss: 0.0110, Test Loss: 0.0102, Test MAER: 3.1044 %, Test MAE: 18.5596 MN, Test MSE: 2434.0509 MN^2 \n",
      "Epoch 48/200, Train Loss: 0.0109, Test Loss: 0.0098, Test MAER: 3.0522 %, Test MAE: 18.1566 MN, Test MSE: 2349.5192 MN^2 \n",
      "Epoch 49/200, Train Loss: 0.0107, Test Loss: 0.0096, Test MAER: 3.0387 %, Test MAE: 18.0129 MN, Test MSE: 2291.2257 MN^2 \n",
      "Epoch 50/200, Train Loss: 0.0106, Test Loss: 0.0094, Test MAER: 3.0303 %, Test MAE: 17.8167 MN, Test MSE: 2247.2914 MN^2 \n",
      "Epoch 51/200, Train Loss: 0.0103, Test Loss: 0.0093, Test MAER: 3.0378 %, Test MAE: 17.7628 MN, Test MSE: 2213.1216 MN^2 \n",
      "Epoch 52/200, Train Loss: 0.0102, Test Loss: 0.0092, Test MAER: 3.0693 %, Test MAE: 17.7957 MN, Test MSE: 2190.5728 MN^2 -> MODEL SAVED\n",
      "Epoch 53/200, Train Loss: 0.0101, Test Loss: 0.0092, Test MAER: 3.0973 %, Test MAE: 17.8313 MN, Test MSE: 2178.9602 MN^2 -> MODEL SAVED\n",
      "Epoch 54/200, Train Loss: 0.0100, Test Loss: 0.0091, Test MAER: 3.0900 %, Test MAE: 17.8357 MN, Test MSE: 2168.4616 MN^2 -> MODEL SAVED\n",
      "Epoch 55/200, Train Loss: 0.0099, Test Loss: 0.0091, Test MAER: 3.0874 %, Test MAE: 17.8414 MN, Test MSE: 2161.6943 MN^2 -> MODEL SAVED\n",
      "Epoch 56/200, Train Loss: 0.0097, Test Loss: 0.0091, Test MAER: 3.0986 %, Test MAE: 17.8402 MN, Test MSE: 2161.1226 MN^2 \n",
      "Epoch 57/200, Train Loss: 0.0097, Test Loss: 0.0091, Test MAER: 3.1148 %, Test MAE: 17.9324 MN, Test MSE: 2150.5221 MN^2 -> MODEL SAVED\n",
      "Epoch 58/200, Train Loss: 0.0095, Test Loss: 0.0091, Test MAER: 3.1166 %, Test MAE: 17.9758 MN, Test MSE: 2154.1174 MN^2 \n",
      "Epoch 59/200, Train Loss: 0.0095, Test Loss: 0.0092, Test MAER: 3.1420 %, Test MAE: 18.1194 MN, Test MSE: 2171.1614 MN^2 \n",
      "Epoch 60/200, Train Loss: 0.0093, Test Loss: 0.0092, Test MAER: 3.1448 %, Test MAE: 18.1296 MN, Test MSE: 2171.4312 MN^2 \n",
      "Epoch 61/200, Train Loss: 0.0092, Test Loss: 0.0093, Test MAER: 3.1848 %, Test MAE: 18.3588 MN, Test MSE: 2198.6142 MN^2 \n",
      "Epoch 62/200, Train Loss: 0.0091, Test Loss: 0.0094, Test MAER: 3.1880 %, Test MAE: 18.4462 MN, Test MSE: 2223.0502 MN^2 \n",
      "Epoch 63/200, Train Loss: 0.0090, Test Loss: 0.0095, Test MAER: 3.2012 %, Test MAE: 18.6358 MN, Test MSE: 2267.6092 MN^2 \n",
      "Epoch 64/200, Train Loss: 0.0090, Test Loss: 0.0095, Test MAER: 3.1993 %, Test MAE: 18.5547 MN, Test MSE: 2254.5750 MN^2 \n",
      "Epoch 65/200, Train Loss: 0.0088, Test Loss: 0.0096, Test MAER: 3.2370 %, Test MAE: 18.7541 MN, Test MSE: 2286.6520 MN^2 \n",
      "Epoch 66/200, Train Loss: 0.0087, Test Loss: 0.0097, Test MAER: 3.2212 %, Test MAE: 18.7431 MN, Test MSE: 2306.9625 MN^2 \n",
      "Epoch 67/200, Train Loss: 0.0086, Test Loss: 0.0097, Test MAER: 3.1801 %, Test MAE: 18.8065 MN, Test MSE: 2318.2097 MN^2 \n",
      "Epoch 68/200, Train Loss: 0.0083, Test Loss: 0.0097, Test MAER: 3.1722 %, Test MAE: 18.7308 MN, Test MSE: 2338.0105 MN^2 \n",
      "Epoch 69/200, Train Loss: 0.0081, Test Loss: 0.0094, Test MAER: 3.1899 %, Test MAE: 18.3680 MN, Test MSE: 2203.0740 MN^2 \n",
      "Epoch 70/200, Train Loss: 0.0078, Test Loss: 0.0083, Test MAER: 2.8859 %, Test MAE: 17.0771 MN, Test MSE: 1975.4490 MN^2 -> MODEL SAVED\n",
      "Epoch 71/200, Train Loss: 0.0079, Test Loss: 0.0076, Test MAER: 2.6782 %, Test MAE: 16.2320 MN, Test MSE: 1814.3376 MN^2 -> MODEL SAVED\n",
      "Epoch 72/200, Train Loss: 0.0076, Test Loss: 0.0074, Test MAER: 2.6416 %, Test MAE: 15.9400 MN, Test MSE: 1779.3910 MN^2 -> MODEL SAVED\n",
      "Epoch 73/200, Train Loss: 0.0078, Test Loss: 0.0075, Test MAER: 2.6464 %, Test MAE: 16.0886 MN, Test MSE: 1788.6222 MN^2 \n",
      "Epoch 74/200, Train Loss: 0.0077, Test Loss: 0.0074, Test MAER: 2.5782 %, Test MAE: 16.0155 MN, Test MSE: 1788.7678 MN^2 -> MODEL SAVED\n",
      "Epoch 75/200, Train Loss: 0.0077, Test Loss: 0.0074, Test MAER: 2.5549 %, Test MAE: 15.8408 MN, Test MSE: 1786.5394 MN^2 -> MODEL SAVED\n",
      "Epoch 76/200, Train Loss: 0.0078, Test Loss: 0.0073, Test MAER: 2.5566 %, Test MAE: 15.7359 MN, Test MSE: 1758.1033 MN^2 -> MODEL SAVED\n",
      "Epoch 77/200, Train Loss: 0.0078, Test Loss: 0.0074, Test MAER: 2.6072 %, Test MAE: 15.8191 MN, Test MSE: 1748.5861 MN^2 \n",
      "Epoch 78/200, Train Loss: 0.0077, Test Loss: 0.0074, Test MAER: 2.5599 %, Test MAE: 16.0538 MN, Test MSE: 1753.0686 MN^2 \n",
      "Epoch 79/200, Train Loss: 0.0064, Test Loss: 0.0072, Test MAER: 2.5567 %, Test MAE: 15.3259 MN, Test MSE: 1692.9844 MN^2 -> MODEL SAVED\n",
      "Epoch 80/200, Train Loss: 0.0071, Test Loss: 0.0073, Test MAER: 2.4604 %, Test MAE: 15.1001 MN, Test MSE: 1727.0281 MN^2 \n",
      "Epoch 81/200, Train Loss: 0.0071, Test Loss: 0.0071, Test MAER: 2.4210 %, Test MAE: 14.8651 MN, Test MSE: 1696.9233 MN^2 -> MODEL SAVED\n",
      "Epoch 82/200, Train Loss: 0.0070, Test Loss: 0.0070, Test MAER: 2.4169 %, Test MAE: 14.7959 MN, Test MSE: 1681.9272 MN^2 -> MODEL SAVED\n",
      "Epoch 83/200, Train Loss: 0.0070, Test Loss: 0.0070, Test MAER: 2.4220 %, Test MAE: 14.7720 MN, Test MSE: 1668.0958 MN^2 -> MODEL SAVED\n",
      "Epoch 84/200, Train Loss: 0.0070, Test Loss: 0.0069, Test MAER: 2.4239 %, Test MAE: 14.7378 MN, Test MSE: 1652.4601 MN^2 -> MODEL SAVED\n",
      "Epoch 85/200, Train Loss: 0.0069, Test Loss: 0.0069, Test MAER: 2.4180 %, Test MAE: 14.6645 MN, Test MSE: 1627.9216 MN^2 -> MODEL SAVED\n",
      "Epoch 86/200, Train Loss: 0.0057, Test Loss: 0.0078, Test MAER: 2.6896 %, Test MAE: 16.0779 MN, Test MSE: 1890.0281 MN^2 \n",
      "Epoch 87/200, Train Loss: 0.0058, Test Loss: 0.0080, Test MAER: 2.7083 %, Test MAE: 16.2933 MN, Test MSE: 1943.7699 MN^2 \n",
      "Epoch 88/200, Train Loss: 0.0057, Test Loss: 0.0079, Test MAER: 2.7039 %, Test MAE: 16.2605 MN, Test MSE: 1937.0469 MN^2 \n",
      "Epoch 89/200, Train Loss: 0.0057, Test Loss: 0.0078, Test MAER: 2.6878 %, Test MAE: 16.1542 MN, Test MSE: 1915.4044 MN^2 \n",
      "Epoch 90/200, Train Loss: 0.0056, Test Loss: 0.0077, Test MAER: 2.6674 %, Test MAE: 16.0091 MN, Test MSE: 1886.2754 MN^2 \n",
      "Epoch 91/200, Train Loss: 0.0056, Test Loss: 0.0076, Test MAER: 2.6405 %, Test MAE: 15.8267 MN, Test MSE: 1849.2698 MN^2 \n",
      "Epoch 92/200, Train Loss: 0.0055, Test Loss: 0.0075, Test MAER: 2.6162 %, Test MAE: 15.6766 MN, Test MSE: 1817.8350 MN^2 \n",
      "Epoch 93/200, Train Loss: 0.0055, Test Loss: 0.0074, Test MAER: 2.5943 %, Test MAE: 15.5394 MN, Test MSE: 1793.3742 MN^2 \n",
      "Epoch 94/200, Train Loss: 0.0055, Test Loss: 0.0073, Test MAER: 2.5745 %, Test MAE: 15.4256 MN, Test MSE: 1771.2811 MN^2 \n",
      "Epoch 95/200, Train Loss: 0.0054, Test Loss: 0.0072, Test MAER: 2.5655 %, Test MAE: 15.3852 MN, Test MSE: 1765.1869 MN^2 \n",
      "Epoch 96/200, Train Loss: 0.0054, Test Loss: 0.0072, Test MAER: 2.5560 %, Test MAE: 15.3248 MN, Test MSE: 1754.9035 MN^2 \n",
      "Epoch 97/200, Train Loss: 0.0054, Test Loss: 0.0072, Test MAER: 2.5525 %, Test MAE: 15.3006 MN, Test MSE: 1749.4957 MN^2 \n",
      "Epoch 98/200, Train Loss: 0.0054, Test Loss: 0.0072, Test MAER: 2.5516 %, Test MAE: 15.3249 MN, Test MSE: 1752.4038 MN^2 \n",
      "Epoch 99/200, Train Loss: 0.0054, Test Loss: 0.0072, Test MAER: 2.5524 %, Test MAE: 15.3264 MN, Test MSE: 1752.7636 MN^2 \n",
      "Epoch 100/200, Train Loss: 0.0054, Test Loss: 0.0072, Test MAER: 2.5573 %, Test MAE: 15.3662 MN, Test MSE: 1761.3734 MN^2 \n",
      "Epoch 101/200, Train Loss: 0.0054, Test Loss: 0.0073, Test MAER: 2.5654 %, Test MAE: 15.4353 MN, Test MSE: 1775.1876 MN^2 \n",
      "Epoch 102/200, Train Loss: 0.0054, Test Loss: 0.0073, Test MAER: 2.5733 %, Test MAE: 15.4846 MN, Test MSE: 1787.6655 MN^2 \n",
      "Epoch 103/200, Train Loss: 0.0054, Test Loss: 0.0074, Test MAER: 2.5789 %, Test MAE: 15.5291 MN, Test MSE: 1797.0807 MN^2 \n",
      "Epoch 104/200, Train Loss: 0.0054, Test Loss: 0.0074, Test MAER: 2.5851 %, Test MAE: 15.5679 MN, Test MSE: 1806.4320 MN^2 \n",
      "Epoch 105/200, Train Loss: 0.0054, Test Loss: 0.0074, Test MAER: 2.5908 %, Test MAE: 15.6131 MN, Test MSE: 1815.3955 MN^2 \n",
      "Epoch 106/200, Train Loss: 0.0054, Test Loss: 0.0059, Test MAER: 2.3444 %, Test MAE: 13.8886 MN, Test MSE: 1415.1546 MN^2 -> MODEL SAVED\n",
      "Epoch 107/200, Train Loss: 0.0052, Test Loss: 0.0058, Test MAER: 2.3249 %, Test MAE: 13.7584 MN, Test MSE: 1396.6161 MN^2 -> MODEL SAVED\n",
      "Epoch 108/200, Train Loss: 0.0052, Test Loss: 0.0058, Test MAER: 2.3280 %, Test MAE: 13.7967 MN, Test MSE: 1400.4980 MN^2 \n",
      "Epoch 109/200, Train Loss: 0.0052, Test Loss: 0.0058, Test MAER: 2.3314 %, Test MAE: 13.8276 MN, Test MSE: 1404.3728 MN^2 \n",
      "Epoch 110/200, Train Loss: 0.0052, Test Loss: 0.0059, Test MAER: 2.3366 %, Test MAE: 13.8736 MN, Test MSE: 1410.0277 MN^2 \n",
      "Epoch 111/200, Train Loss: 0.0052, Test Loss: 0.0059, Test MAER: 2.3422 %, Test MAE: 13.9233 MN, Test MSE: 1416.1509 MN^2 \n",
      "Epoch 112/200, Train Loss: 0.0052, Test Loss: 0.0059, Test MAER: 2.3453 %, Test MAE: 13.9450 MN, Test MSE: 1418.5508 MN^2 \n",
      "Epoch 113/200, Train Loss: 0.0051, Test Loss: 0.0059, Test MAER: 2.3502 %, Test MAE: 13.9811 MN, Test MSE: 1423.2866 MN^2 \n",
      "Epoch 114/200, Train Loss: 0.0051, Test Loss: 0.0059, Test MAER: 2.3505 %, Test MAE: 13.9899 MN, Test MSE: 1423.5672 MN^2 \n",
      "Epoch 115/200, Train Loss: 0.0051, Test Loss: 0.0059, Test MAER: 2.3529 %, Test MAE: 14.0151 MN, Test MSE: 1426.6087 MN^2 \n",
      "Epoch 116/200, Train Loss: 0.0051, Test Loss: 0.0059, Test MAER: 2.3565 %, Test MAE: 14.0372 MN, Test MSE: 1429.8380 MN^2 \n",
      "Epoch 117/200, Train Loss: 0.0051, Test Loss: 0.0059, Test MAER: 2.3552 %, Test MAE: 14.0518 MN, Test MSE: 1430.4260 MN^2 \n",
      "Epoch 118/200, Train Loss: 0.0051, Test Loss: 0.0059, Test MAER: 2.3560 %, Test MAE: 14.0577 MN, Test MSE: 1431.1664 MN^2 \n",
      "Epoch 119/200, Train Loss: 0.0051, Test Loss: 0.0059, Test MAER: 2.3589 %, Test MAE: 14.0789 MN, Test MSE: 1434.2453 MN^2 \n",
      "Epoch 120/200, Train Loss: 0.0051, Test Loss: 0.0059, Test MAER: 2.3571 %, Test MAE: 14.0732 MN, Test MSE: 1432.5216 MN^2 \n",
      "Epoch 121/200, Train Loss: 0.0051, Test Loss: 0.0059, Test MAER: 2.3545 %, Test MAE: 14.0652 MN, Test MSE: 1430.0817 MN^2 \n",
      "Epoch 122/200, Train Loss: 0.0051, Test Loss: 0.0059, Test MAER: 2.3563 %, Test MAE: 14.0810 MN, Test MSE: 1431.8828 MN^2 \n",
      "Epoch 123/200, Train Loss: 0.0050, Test Loss: 0.0059, Test MAER: 2.3552 %, Test MAE: 14.0785 MN, Test MSE: 1430.3978 MN^2 \n",
      "Epoch 124/200, Train Loss: 0.0050, Test Loss: 0.0059, Test MAER: 2.3548 %, Test MAE: 14.0801 MN, Test MSE: 1429.5809 MN^2 \n",
      "Epoch 125/200, Train Loss: 0.0050, Test Loss: 0.0059, Test MAER: 2.3535 %, Test MAE: 14.0747 MN, Test MSE: 1428.8849 MN^2 \n",
      "Epoch 126/200, Train Loss: 0.0050, Test Loss: 0.0059, Test MAER: 2.3539 %, Test MAE: 14.0846 MN, Test MSE: 1429.5435 MN^2 \n",
      "Epoch 127/200, Train Loss: 0.0050, Test Loss: 0.0059, Test MAER: 2.3535 %, Test MAE: 14.0866 MN, Test MSE: 1428.5889 MN^2 \n",
      "Epoch 128/200, Train Loss: 0.0050, Test Loss: 0.0059, Test MAER: 2.3506 %, Test MAE: 14.0801 MN, Test MSE: 1426.8356 MN^2 \n",
      "Epoch 129/200, Train Loss: 0.0050, Test Loss: 0.0059, Test MAER: 2.3480 %, Test MAE: 14.0692 MN, Test MSE: 1425.0352 MN^2 \n",
      "Epoch 130/200, Train Loss: 0.0050, Test Loss: 0.0059, Test MAER: 2.3467 %, Test MAE: 14.0606 MN, Test MSE: 1423.5561 MN^2 \n",
      "Epoch 131/200, Train Loss: 0.0050, Test Loss: 0.0059, Test MAER: 2.3480 %, Test MAE: 14.0751 MN, Test MSE: 1425.4098 MN^2 \n",
      "Epoch 132/200, Train Loss: 0.0049, Test Loss: 0.0059, Test MAER: 2.3452 %, Test MAE: 14.0604 MN, Test MSE: 1422.5403 MN^2 \n",
      "Epoch 133/200, Train Loss: 0.0049, Test Loss: 0.0059, Test MAER: 2.3442 %, Test MAE: 14.0596 MN, Test MSE: 1422.0752 MN^2 \n",
      "Epoch 134/200, Train Loss: 0.0049, Test Loss: 0.0059, Test MAER: 2.3438 %, Test MAE: 14.0641 MN, Test MSE: 1422.4002 MN^2 \n",
      "Epoch 135/200, Train Loss: 0.0049, Test Loss: 0.0059, Test MAER: 2.3412 %, Test MAE: 14.0484 MN, Test MSE: 1420.2910 MN^2 \n",
      "Epoch 136/200, Train Loss: 0.0049, Test Loss: 0.0059, Test MAER: 2.3425 %, Test MAE: 14.0520 MN, Test MSE: 1420.1995 MN^2 \n",
      "Epoch 137/200, Train Loss: 0.0049, Test Loss: 0.0059, Test MAER: 2.3416 %, Test MAE: 14.0518 MN, Test MSE: 1419.5592 MN^2 \n",
      "Epoch 138/200, Train Loss: 0.0049, Test Loss: 0.0059, Test MAER: 2.3384 %, Test MAE: 14.0360 MN, Test MSE: 1417.5157 MN^2 \n",
      "Epoch 139/200, Train Loss: 0.0049, Test Loss: 0.0058, Test MAER: 2.3335 %, Test MAE: 14.0119 MN, Test MSE: 1412.7403 MN^2 \n",
      "Epoch 140/200, Train Loss: 0.0048, Test Loss: 0.0058, Test MAER: 2.3322 %, Test MAE: 14.0048 MN, Test MSE: 1411.6504 MN^2 \n",
      "Epoch 141/200, Train Loss: 0.0048, Test Loss: 0.0058, Test MAER: 2.3266 %, Test MAE: 13.9748 MN, Test MSE: 1407.0987 MN^2 \n",
      "Epoch 142/200, Train Loss: 0.0048, Test Loss: 0.0058, Test MAER: 2.3237 %, Test MAE: 13.9546 MN, Test MSE: 1402.9994 MN^2 \n",
      "Epoch 143/200, Train Loss: 0.0048, Test Loss: 0.0058, Test MAER: 2.3200 %, Test MAE: 13.9226 MN, Test MSE: 1398.7749 MN^2 -> MODEL SAVED\n",
      "Epoch 144/200, Train Loss: 0.0048, Test Loss: 0.0058, Test MAER: 2.3159 %, Test MAE: 13.9118 MN, Test MSE: 1396.6089 MN^2 -> MODEL SAVED\n",
      "Epoch 145/200, Train Loss: 0.0048, Test Loss: 0.0058, Test MAER: 2.3127 %, Test MAE: 13.8843 MN, Test MSE: 1392.1973 MN^2 -> MODEL SAVED\n",
      "Epoch 146/200, Train Loss: 0.0048, Test Loss: 0.0058, Test MAER: 2.3111 %, Test MAE: 13.8793 MN, Test MSE: 1389.2154 MN^2 -> MODEL SAVED\n",
      "Epoch 147/200, Train Loss: 0.0048, Test Loss: 0.0057, Test MAER: 2.3053 %, Test MAE: 13.8415 MN, Test MSE: 1384.1591 MN^2 -> MODEL SAVED\n",
      "Epoch 148/200, Train Loss: 0.0047, Test Loss: 0.0057, Test MAER: 2.3031 %, Test MAE: 13.8270 MN, Test MSE: 1381.2696 MN^2 -> MODEL SAVED\n",
      "Epoch 149/200, Train Loss: 0.0047, Test Loss: 0.0057, Test MAER: 2.2991 %, Test MAE: 13.8081 MN, Test MSE: 1378.5021 MN^2 -> MODEL SAVED\n",
      "Epoch 150/200, Train Loss: 0.0047, Test Loss: 0.0057, Test MAER: 2.2957 %, Test MAE: 13.7963 MN, Test MSE: 1376.3820 MN^2 -> MODEL SAVED\n",
      "Epoch 151/200, Train Loss: 0.0047, Test Loss: 0.0057, Test MAER: 2.2903 %, Test MAE: 13.7578 MN, Test MSE: 1369.9849 MN^2 -> MODEL SAVED\n",
      "Epoch 152/200, Train Loss: 0.0047, Test Loss: 0.0057, Test MAER: 2.2890 %, Test MAE: 13.7430 MN, Test MSE: 1368.4095 MN^2 -> MODEL SAVED\n",
      "Epoch 153/200, Train Loss: 0.0047, Test Loss: 0.0056, Test MAER: 2.2844 %, Test MAE: 13.7270 MN, Test MSE: 1364.5894 MN^2 -> MODEL SAVED\n",
      "Epoch 154/200, Train Loss: 0.0047, Test Loss: 0.0056, Test MAER: 2.2813 %, Test MAE: 13.7104 MN, Test MSE: 1361.5115 MN^2 -> MODEL SAVED\n",
      "Epoch 155/200, Train Loss: 0.0046, Test Loss: 0.0056, Test MAER: 2.2765 %, Test MAE: 13.6804 MN, Test MSE: 1357.6687 MN^2 -> MODEL SAVED\n",
      "Epoch 156/200, Train Loss: 0.0046, Test Loss: 0.0056, Test MAER: 2.2730 %, Test MAE: 13.6479 MN, Test MSE: 1351.4938 MN^2 -> MODEL SAVED\n",
      "Epoch 157/200, Train Loss: 0.0046, Test Loss: 0.0056, Test MAER: 2.2694 %, Test MAE: 13.6389 MN, Test MSE: 1351.0055 MN^2 -> MODEL SAVED\n",
      "Epoch 158/200, Train Loss: 0.0046, Test Loss: 0.0056, Test MAER: 2.2639 %, Test MAE: 13.6188 MN, Test MSE: 1346.4906 MN^2 -> MODEL SAVED\n",
      "Epoch 159/200, Train Loss: 0.0046, Test Loss: 0.0056, Test MAER: 2.2602 %, Test MAE: 13.5846 MN, Test MSE: 1342.1088 MN^2 -> MODEL SAVED\n",
      "Epoch 160/200, Train Loss: 0.0046, Test Loss: 0.0055, Test MAER: 2.2538 %, Test MAE: 13.5475 MN, Test MSE: 1335.6479 MN^2 -> MODEL SAVED\n",
      "Epoch 161/200, Train Loss: 0.0046, Test Loss: 0.0055, Test MAER: 2.2512 %, Test MAE: 13.5376 MN, Test MSE: 1334.6944 MN^2 -> MODEL SAVED\n",
      "Epoch 162/200, Train Loss: 0.0045, Test Loss: 0.0055, Test MAER: 2.2456 %, Test MAE: 13.5020 MN, Test MSE: 1327.0834 MN^2 -> MODEL SAVED\n",
      "Epoch 163/200, Train Loss: 0.0045, Test Loss: 0.0055, Test MAER: 2.2390 %, Test MAE: 13.4649 MN, Test MSE: 1324.0488 MN^2 -> MODEL SAVED\n",
      "Epoch 164/200, Train Loss: 0.0045, Test Loss: 0.0054, Test MAER: 2.2307 %, Test MAE: 13.4172 MN, Test MSE: 1315.2227 MN^2 -> MODEL SAVED\n",
      "Epoch 165/200, Train Loss: 0.0045, Test Loss: 0.0055, Test MAER: 2.2313 %, Test MAE: 13.4198 MN, Test MSE: 1315.9725 MN^2 \n",
      "Epoch 166/200, Train Loss: 0.0045, Test Loss: 0.0054, Test MAER: 2.2220 %, Test MAE: 13.3692 MN, Test MSE: 1306.9977 MN^2 -> MODEL SAVED\n",
      "Epoch 167/200, Train Loss: 0.0045, Test Loss: 0.0054, Test MAER: 2.2197 %, Test MAE: 13.3548 MN, Test MSE: 1306.5341 MN^2 -> MODEL SAVED\n",
      "Epoch 168/200, Train Loss: 0.0045, Test Loss: 0.0054, Test MAER: 2.2156 %, Test MAE: 13.3318 MN, Test MSE: 1302.0908 MN^2 -> MODEL SAVED\n",
      "Epoch 169/200, Train Loss: 0.0045, Test Loss: 0.0054, Test MAER: 2.2135 %, Test MAE: 13.3212 MN, Test MSE: 1300.6040 MN^2 -> MODEL SAVED\n",
      "Epoch 170/200, Train Loss: 0.0045, Test Loss: 0.0054, Test MAER: 2.2028 %, Test MAE: 13.2635 MN, Test MSE: 1292.1081 MN^2 -> MODEL SAVED\n",
      "Epoch 171/200, Train Loss: 0.0044, Test Loss: 0.0054, Test MAER: 2.2071 %, Test MAE: 13.2800 MN, Test MSE: 1294.5436 MN^2 \n",
      "Epoch 172/200, Train Loss: 0.0044, Test Loss: 0.0053, Test MAER: 2.1996 %, Test MAE: 13.2455 MN, Test MSE: 1287.3046 MN^2 -> MODEL SAVED\n",
      "Epoch 173/200, Train Loss: 0.0044, Test Loss: 0.0053, Test MAER: 2.1949 %, Test MAE: 13.2075 MN, Test MSE: 1282.9405 MN^2 -> MODEL SAVED\n",
      "Epoch 174/200, Train Loss: 0.0044, Test Loss: 0.0053, Test MAER: 2.1913 %, Test MAE: 13.1963 MN, Test MSE: 1280.1112 MN^2 -> MODEL SAVED\n",
      "Epoch 175/200, Train Loss: 0.0044, Test Loss: 0.0053, Test MAER: 2.1900 %, Test MAE: 13.1897 MN, Test MSE: 1280.3885 MN^2 \n",
      "Epoch 176/200, Train Loss: 0.0044, Test Loss: 0.0053, Test MAER: 2.1883 %, Test MAE: 13.1730 MN, Test MSE: 1276.5976 MN^2 -> MODEL SAVED\n",
      "Epoch 177/200, Train Loss: 0.0044, Test Loss: 0.0053, Test MAER: 2.1838 %, Test MAE: 13.1581 MN, Test MSE: 1275.4270 MN^2 -> MODEL SAVED\n",
      "Epoch 178/200, Train Loss: 0.0044, Test Loss: 0.0053, Test MAER: 2.1788 %, Test MAE: 13.1222 MN, Test MSE: 1268.4250 MN^2 -> MODEL SAVED\n",
      "Epoch 179/200, Train Loss: 0.0043, Test Loss: 0.0052, Test MAER: 2.1724 %, Test MAE: 13.0923 MN, Test MSE: 1263.1102 MN^2 -> MODEL SAVED\n",
      "Epoch 180/200, Train Loss: 0.0043, Test Loss: 0.0052, Test MAER: 2.1712 %, Test MAE: 13.0829 MN, Test MSE: 1261.1267 MN^2 -> MODEL SAVED\n",
      "Epoch 181/200, Train Loss: 0.0043, Test Loss: 0.0052, Test MAER: 2.1638 %, Test MAE: 13.0534 MN, Test MSE: 1255.3008 MN^2 -> MODEL SAVED\n",
      "Epoch 182/200, Train Loss: 0.0043, Test Loss: 0.0052, Test MAER: 2.1630 %, Test MAE: 13.0477 MN, Test MSE: 1253.7620 MN^2 -> MODEL SAVED\n",
      "Epoch 183/200, Train Loss: 0.0043, Test Loss: 0.0052, Test MAER: 2.1571 %, Test MAE: 13.0116 MN, Test MSE: 1248.0036 MN^2 -> MODEL SAVED\n",
      "Epoch 184/200, Train Loss: 0.0043, Test Loss: 0.0052, Test MAER: 2.1540 %, Test MAE: 12.9974 MN, Test MSE: 1246.4234 MN^2 -> MODEL SAVED\n",
      "Epoch 185/200, Train Loss: 0.0043, Test Loss: 0.0052, Test MAER: 2.1531 %, Test MAE: 12.9923 MN, Test MSE: 1244.9558 MN^2 -> MODEL SAVED\n",
      "Epoch 186/200, Train Loss: 0.0043, Test Loss: 0.0051, Test MAER: 2.1468 %, Test MAE: 12.9472 MN, Test MSE: 1238.6179 MN^2 -> MODEL SAVED\n",
      "Epoch 187/200, Train Loss: 0.0043, Test Loss: 0.0051, Test MAER: 2.1485 %, Test MAE: 12.9612 MN, Test MSE: 1239.9507 MN^2 \n",
      "Epoch 188/200, Train Loss: 0.0042, Test Loss: 0.0051, Test MAER: 2.1407 %, Test MAE: 12.9089 MN, Test MSE: 1232.4734 MN^2 -> MODEL SAVED\n",
      "Epoch 189/200, Train Loss: 0.0042, Test Loss: 0.0051, Test MAER: 2.1415 %, Test MAE: 12.9222 MN, Test MSE: 1233.6501 MN^2 \n",
      "Epoch 190/200, Train Loss: 0.0042, Test Loss: 0.0051, Test MAER: 2.1357 %, Test MAE: 12.8752 MN, Test MSE: 1227.3618 MN^2 -> MODEL SAVED\n",
      "Epoch 191/200, Train Loss: 0.0042, Test Loss: 0.0051, Test MAER: 2.1369 %, Test MAE: 12.8887 MN, Test MSE: 1229.1740 MN^2 \n",
      "Epoch 192/200, Train Loss: 0.0042, Test Loss: 0.0051, Test MAER: 2.1329 %, Test MAE: 12.8676 MN, Test MSE: 1225.1045 MN^2 -> MODEL SAVED\n",
      "Epoch 193/200, Train Loss: 0.0042, Test Loss: 0.0051, Test MAER: 2.1305 %, Test MAE: 12.8481 MN, Test MSE: 1223.2937 MN^2 -> MODEL SAVED\n",
      "Epoch 194/200, Train Loss: 0.0042, Test Loss: 0.0051, Test MAER: 2.1290 %, Test MAE: 12.8479 MN, Test MSE: 1221.6203 MN^2 -> MODEL SAVED\n",
      "Epoch 195/200, Train Loss: 0.0042, Test Loss: 0.0051, Test MAER: 2.1288 %, Test MAE: 12.8438 MN, Test MSE: 1221.5600 MN^2 -> MODEL SAVED\n",
      "Epoch 196/200, Train Loss: 0.0042, Test Loss: 0.0050, Test MAER: 2.1243 %, Test MAE: 12.8202 MN, Test MSE: 1216.1313 MN^2 -> MODEL SAVED\n",
      "Epoch 197/200, Train Loss: 0.0042, Test Loss: 0.0050, Test MAER: 2.1251 %, Test MAE: 12.8258 MN, Test MSE: 1217.4234 MN^2 \n",
      "Epoch 198/200, Train Loss: 0.0042, Test Loss: 0.0050, Test MAER: 2.1205 %, Test MAE: 12.7972 MN, Test MSE: 1212.3949 MN^2 -> MODEL SAVED\n",
      "Epoch 199/200, Train Loss: 0.0041, Test Loss: 0.0050, Test MAER: 2.1178 %, Test MAE: 12.7803 MN, Test MSE: 1209.1044 MN^2 -> MODEL SAVED\n",
      "Epoch 200/200, Train Loss: 0.0041, Test Loss: 0.0050, Test MAER: 2.1154 %, Test MAE: 12.7686 MN, Test MSE: 1207.1802 MN^2 -> MODEL SAVED\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T16:09:10.627912Z",
     "start_time": "2025-01-20T16:00:40.551623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = 'triangle'\n",
    "writer = SummaryWriter(f\"./runs/final/{dataset_name}/FEI_MLP/3/50/ReLU/lr_0.0004\")\n",
    "save_path = f\"./runs/final/{dataset_name}/FEI_MLP/3/50/ReLU/lr_0.0004/SAVE\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model_beam = main_final(writer, _train_ds[dataset_name], _test_ds[dataset_name], connectivity[dataset_name], support[dataset_name], hidden_size=50, n_hidden=3, activation=nn.ReLU,\n",
    "                        activation_params={}, learning_rate=0.0004, n_epochs=200, save_path=save_path)\n",
    "writer.close()"
   ],
   "id": "77b46535dfd97639",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 1.5903, Test Loss: 1.1098, Test MAER: 50.1340 %, Test MAE: 471.4121 MN, Test MSE: 438297.1955 MN^2 \n",
      "Epoch 2/200, Train Loss: 0.6744, Test Loss: 0.3946, Test MAER: 27.4776 %, Test MAE: 273.8418 MN, Test MSE: 186131.0951 MN^2 \n",
      "Epoch 3/200, Train Loss: 0.2915, Test Loss: 0.2230, Test MAER: 19.3542 %, Test MAE: 197.9738 MN, Test MSE: 114708.1546 MN^2 \n",
      "Epoch 4/200, Train Loss: 0.1836, Test Loss: 0.1606, Test MAER: 16.2856 %, Test MAE: 163.5788 MN, Test MSE: 85654.6319 MN^2 \n",
      "Epoch 5/200, Train Loss: 0.1391, Test Loss: 0.1283, Test MAER: 14.3286 %, Test MAE: 142.6707 MN, Test MSE: 69966.4858 MN^2 \n",
      "Epoch 6/200, Train Loss: 0.1133, Test Loss: 0.1080, Test MAER: 12.9125 %, Test MAE: 128.0267 MN, Test MSE: 59501.9861 MN^2 \n",
      "Epoch 7/200, Train Loss: 0.0961, Test Loss: 0.0935, Test MAER: 11.5638 %, Test MAE: 115.2963 MN, Test MSE: 51865.9037 MN^2 \n",
      "Epoch 8/200, Train Loss: 0.0837, Test Loss: 0.0824, Test MAER: 10.3803 %, Test MAE: 104.5032 MN, Test MSE: 45979.8027 MN^2 \n",
      "Epoch 9/200, Train Loss: 0.0741, Test Loss: 0.0734, Test MAER: 9.4729 %, Test MAE: 95.8895 MN, Test MSE: 41148.2605 MN^2 \n",
      "Epoch 10/200, Train Loss: 0.0662, Test Loss: 0.0660, Test MAER: 8.7625 %, Test MAE: 88.9259 MN, Test MSE: 37122.5938 MN^2 \n",
      "Epoch 11/200, Train Loss: 0.0598, Test Loss: 0.0599, Test MAER: 8.1725 %, Test MAE: 82.9171 MN, Test MSE: 33782.2802 MN^2 \n",
      "Epoch 12/200, Train Loss: 0.0544, Test Loss: 0.0546, Test MAER: 7.6321 %, Test MAE: 77.5198 MN, Test MSE: 30917.3543 MN^2 \n",
      "Epoch 13/200, Train Loss: 0.0497, Test Loss: 0.0499, Test MAER: 7.1931 %, Test MAE: 72.8274 MN, Test MSE: 28426.1282 MN^2 \n",
      "Epoch 14/200, Train Loss: 0.0457, Test Loss: 0.0460, Test MAER: 6.8680 %, Test MAE: 69.0524 MN, Test MSE: 26299.6415 MN^2 \n",
      "Epoch 15/200, Train Loss: 0.0423, Test Loss: 0.0428, Test MAER: 6.5959 %, Test MAE: 65.9281 MN, Test MSE: 24511.9978 MN^2 \n",
      "Epoch 16/200, Train Loss: 0.0395, Test Loss: 0.0401, Test MAER: 6.3523 %, Test MAE: 63.1737 MN, Test MSE: 22979.6811 MN^2 \n",
      "Epoch 17/200, Train Loss: 0.0371, Test Loss: 0.0377, Test MAER: 6.1143 %, Test MAE: 60.6079 MN, Test MSE: 21622.3478 MN^2 \n",
      "Epoch 18/200, Train Loss: 0.0350, Test Loss: 0.0357, Test MAER: 5.8961 %, Test MAE: 58.3177 MN, Test MSE: 20444.0556 MN^2 \n",
      "Epoch 19/200, Train Loss: 0.0331, Test Loss: 0.0339, Test MAER: 5.7054 %, Test MAE: 56.3408 MN, Test MSE: 19395.4818 MN^2 \n",
      "Epoch 20/200, Train Loss: 0.0314, Test Loss: 0.0323, Test MAER: 5.5269 %, Test MAE: 54.4821 MN, Test MSE: 18457.1935 MN^2 \n",
      "Epoch 21/200, Train Loss: 0.0300, Test Loss: 0.0308, Test MAER: 5.3649 %, Test MAE: 52.7716 MN, Test MSE: 17596.0590 MN^2 \n",
      "Epoch 22/200, Train Loss: 0.0286, Test Loss: 0.0294, Test MAER: 5.2229 %, Test MAE: 51.2151 MN, Test MSE: 16804.8715 MN^2 \n",
      "Epoch 23/200, Train Loss: 0.0274, Test Loss: 0.0282, Test MAER: 5.0856 %, Test MAE: 49.8225 MN, Test MSE: 16102.0753 MN^2 \n",
      "Epoch 24/200, Train Loss: 0.0263, Test Loss: 0.0271, Test MAER: 4.9693 %, Test MAE: 48.5669 MN, Test MSE: 15467.7924 MN^2 \n",
      "Epoch 25/200, Train Loss: 0.0253, Test Loss: 0.0261, Test MAER: 4.8585 %, Test MAE: 47.4128 MN, Test MSE: 14885.4062 MN^2 \n",
      "Epoch 26/200, Train Loss: 0.0244, Test Loss: 0.0252, Test MAER: 4.7659 %, Test MAE: 46.3710 MN, Test MSE: 14352.3331 MN^2 \n",
      "Epoch 27/200, Train Loss: 0.0235, Test Loss: 0.0244, Test MAER: 4.6675 %, Test MAE: 45.3571 MN, Test MSE: 13858.4180 MN^2 \n",
      "Epoch 28/200, Train Loss: 0.0227, Test Loss: 0.0237, Test MAER: 4.5841 %, Test MAE: 44.4758 MN, Test MSE: 13406.2243 MN^2 \n",
      "Epoch 29/200, Train Loss: 0.0220, Test Loss: 0.0230, Test MAER: 4.5058 %, Test MAE: 43.6136 MN, Test MSE: 12989.4772 MN^2 \n",
      "Epoch 30/200, Train Loss: 0.0213, Test Loss: 0.0223, Test MAER: 4.4496 %, Test MAE: 42.8774 MN, Test MSE: 12599.5932 MN^2 \n",
      "Epoch 31/200, Train Loss: 0.0207, Test Loss: 0.0217, Test MAER: 4.3609 %, Test MAE: 42.0397 MN, Test MSE: 12234.3802 MN^2 \n",
      "Epoch 32/200, Train Loss: 0.0201, Test Loss: 0.0211, Test MAER: 4.3017 %, Test MAE: 41.3378 MN, Test MSE: 11898.1203 MN^2 \n",
      "Epoch 33/200, Train Loss: 0.0196, Test Loss: 0.0206, Test MAER: 4.2243 %, Test MAE: 40.6202 MN, Test MSE: 11579.3869 MN^2 \n",
      "Epoch 34/200, Train Loss: 0.0191, Test Loss: 0.0200, Test MAER: 4.1681 %, Test MAE: 39.9848 MN, Test MSE: 11276.9912 MN^2 \n",
      "Epoch 35/200, Train Loss: 0.0185, Test Loss: 0.0197, Test MAER: 4.1018 %, Test MAE: 39.3893 MN, Test MSE: 11008.0494 MN^2 \n",
      "Epoch 36/200, Train Loss: 0.0181, Test Loss: 0.0191, Test MAER: 4.0764 %, Test MAE: 38.9340 MN, Test MSE: 10745.3503 MN^2 \n",
      "Epoch 37/200, Train Loss: 0.0176, Test Loss: 0.0188, Test MAER: 4.0078 %, Test MAE: 38.3650 MN, Test MSE: 10505.1380 MN^2 \n",
      "Epoch 38/200, Train Loss: 0.0173, Test Loss: 0.0183, Test MAER: 3.9700 %, Test MAE: 37.8881 MN, Test MSE: 10270.0579 MN^2 \n",
      "Epoch 39/200, Train Loss: 0.0169, Test Loss: 0.0180, Test MAER: 3.9139 %, Test MAE: 37.3745 MN, Test MSE: 10056.6703 MN^2 \n",
      "Epoch 40/200, Train Loss: 0.0165, Test Loss: 0.0176, Test MAER: 3.8561 %, Test MAE: 36.8690 MN, Test MSE: 9847.0746 MN^2 \n",
      "Epoch 41/200, Train Loss: 0.0161, Test Loss: 0.0173, Test MAER: 3.8050 %, Test MAE: 36.3818 MN, Test MSE: 9650.6809 MN^2 \n",
      "Epoch 42/200, Train Loss: 0.0158, Test Loss: 0.0170, Test MAER: 3.7549 %, Test MAE: 35.9401 MN, Test MSE: 9460.5566 MN^2 \n",
      "Epoch 43/200, Train Loss: 0.0155, Test Loss: 0.0167, Test MAER: 3.7159 %, Test MAE: 35.5304 MN, Test MSE: 9278.6637 MN^2 \n",
      "Epoch 44/200, Train Loss: 0.0152, Test Loss: 0.0164, Test MAER: 3.6800 %, Test MAE: 35.1931 MN, Test MSE: 9104.6197 MN^2 \n",
      "Epoch 45/200, Train Loss: 0.0149, Test Loss: 0.0161, Test MAER: 3.6341 %, Test MAE: 34.8033 MN, Test MSE: 8933.4324 MN^2 \n",
      "Epoch 46/200, Train Loss: 0.0146, Test Loss: 0.0158, Test MAER: 3.5879 %, Test MAE: 34.4332 MN, Test MSE: 8774.2012 MN^2 \n",
      "Epoch 47/200, Train Loss: 0.0143, Test Loss: 0.0156, Test MAER: 3.5510 %, Test MAE: 34.1329 MN, Test MSE: 8629.2378 MN^2 \n",
      "Epoch 48/200, Train Loss: 0.0141, Test Loss: 0.0152, Test MAER: 3.5114 %, Test MAE: 33.8052 MN, Test MSE: 8482.0360 MN^2 \n",
      "Epoch 49/200, Train Loss: 0.0138, Test Loss: 0.0151, Test MAER: 3.4841 %, Test MAE: 33.6087 MN, Test MSE: 8359.8215 MN^2 \n",
      "Epoch 50/200, Train Loss: 0.0136, Test Loss: 0.0147, Test MAER: 3.4485 %, Test MAE: 33.1879 MN, Test MSE: 8215.5162 MN^2 \n",
      "Epoch 51/200, Train Loss: 0.0134, Test Loss: 0.0147, Test MAER: 3.4457 %, Test MAE: 33.3052 MN, Test MSE: 8130.8211 MN^2 \n",
      "Epoch 52/200, Train Loss: 0.0132, Test Loss: 0.0143, Test MAER: 3.4066 %, Test MAE: 32.7208 MN, Test MSE: 7987.5426 MN^2 -> MODEL SAVED\n",
      "Epoch 53/200, Train Loss: 0.0130, Test Loss: 0.0139, Test MAER: 3.3679 %, Test MAE: 32.3856 MN, Test MSE: 7849.3325 MN^2 -> MODEL SAVED\n",
      "Epoch 54/200, Train Loss: 0.0127, Test Loss: 0.0137, Test MAER: 3.3264 %, Test MAE: 32.1419 MN, Test MSE: 7744.5129 MN^2 -> MODEL SAVED\n",
      "Epoch 55/200, Train Loss: 0.0125, Test Loss: 0.0135, Test MAER: 3.2891 %, Test MAE: 31.8926 MN, Test MSE: 7641.0811 MN^2 -> MODEL SAVED\n",
      "Epoch 56/200, Train Loss: 0.0123, Test Loss: 0.0133, Test MAER: 3.2637 %, Test MAE: 31.7314 MN, Test MSE: 7542.6206 MN^2 -> MODEL SAVED\n",
      "Epoch 57/200, Train Loss: 0.0122, Test Loss: 0.0138, Test MAER: 3.3570 %, Test MAE: 32.6503 MN, Test MSE: 7535.2933 MN^2 \n",
      "Epoch 58/200, Train Loss: 0.0120, Test Loss: 0.0131, Test MAER: 3.2243 %, Test MAE: 31.4165 MN, Test MSE: 7393.9537 MN^2 -> MODEL SAVED\n",
      "Epoch 59/200, Train Loss: 0.0119, Test Loss: 0.0129, Test MAER: 3.1924 %, Test MAE: 31.1808 MN, Test MSE: 7281.9057 MN^2 -> MODEL SAVED\n",
      "Epoch 60/200, Train Loss: 0.0117, Test Loss: 0.0128, Test MAER: 3.1737 %, Test MAE: 31.0888 MN, Test MSE: 7209.0840 MN^2 -> MODEL SAVED\n",
      "Epoch 61/200, Train Loss: 0.0115, Test Loss: 0.0126, Test MAER: 3.1563 %, Test MAE: 30.9665 MN, Test MSE: 7134.5702 MN^2 -> MODEL SAVED\n",
      "Epoch 62/200, Train Loss: 0.0113, Test Loss: 0.0125, Test MAER: 3.1331 %, Test MAE: 30.7617 MN, Test MSE: 7043.8855 MN^2 -> MODEL SAVED\n",
      "Epoch 63/200, Train Loss: 0.0112, Test Loss: 0.0123, Test MAER: 3.1144 %, Test MAE: 30.6313 MN, Test MSE: 6975.2963 MN^2 -> MODEL SAVED\n",
      "Epoch 64/200, Train Loss: 0.0110, Test Loss: 0.0122, Test MAER: 3.0942 %, Test MAE: 30.4817 MN, Test MSE: 6899.0409 MN^2 -> MODEL SAVED\n",
      "Epoch 65/200, Train Loss: 0.0109, Test Loss: 0.0121, Test MAER: 3.0704 %, Test MAE: 30.2836 MN, Test MSE: 6815.1231 MN^2 -> MODEL SAVED\n",
      "Epoch 66/200, Train Loss: 0.0108, Test Loss: 0.0120, Test MAER: 3.0611 %, Test MAE: 30.2412 MN, Test MSE: 6749.1181 MN^2 -> MODEL SAVED\n",
      "Epoch 67/200, Train Loss: 0.0106, Test Loss: 0.0120, Test MAER: 3.0681 %, Test MAE: 30.3174 MN, Test MSE: 6695.9160 MN^2 \n",
      "Epoch 68/200, Train Loss: 0.0105, Test Loss: 0.0117, Test MAER: 3.0317 %, Test MAE: 29.9831 MN, Test MSE: 6621.1625 MN^2 -> MODEL SAVED\n",
      "Epoch 69/200, Train Loss: 0.0104, Test Loss: 0.0118, Test MAER: 3.0485 %, Test MAE: 30.1522 MN, Test MSE: 6572.0344 MN^2 \n",
      "Epoch 70/200, Train Loss: 0.0103, Test Loss: 0.0115, Test MAER: 2.9951 %, Test MAE: 29.6349 MN, Test MSE: 6481.5220 MN^2 -> MODEL SAVED\n",
      "Epoch 71/200, Train Loss: 0.0102, Test Loss: 0.0117, Test MAER: 3.0430 %, Test MAE: 30.0286 MN, Test MSE: 6437.9755 MN^2 \n",
      "Epoch 72/200, Train Loss: 0.0101, Test Loss: 0.0112, Test MAER: 2.9612 %, Test MAE: 29.2438 MN, Test MSE: 6344.1913 MN^2 -> MODEL SAVED\n",
      "Epoch 73/200, Train Loss: 0.0100, Test Loss: 0.0116, Test MAER: 3.0530 %, Test MAE: 30.0291 MN, Test MSE: 6329.0877 MN^2 \n",
      "Epoch 74/200, Train Loss: 0.0098, Test Loss: 0.0110, Test MAER: 2.9320 %, Test MAE: 28.9227 MN, Test MSE: 6215.1016 MN^2 -> MODEL SAVED\n",
      "Epoch 75/200, Train Loss: 0.0098, Test Loss: 0.0114, Test MAER: 3.0050 %, Test MAE: 29.5982 MN, Test MSE: 6196.2053 MN^2 \n",
      "Epoch 76/200, Train Loss: 0.0096, Test Loss: 0.0108, Test MAER: 2.9096 %, Test MAE: 28.6395 MN, Test MSE: 6095.0890 MN^2 -> MODEL SAVED\n",
      "Epoch 77/200, Train Loss: 0.0096, Test Loss: 0.0112, Test MAER: 3.0006 %, Test MAE: 29.4900 MN, Test MSE: 6083.8998 MN^2 \n",
      "Epoch 78/200, Train Loss: 0.0095, Test Loss: 0.0106, Test MAER: 2.8772 %, Test MAE: 28.3566 MN, Test MSE: 5972.9044 MN^2 -> MODEL SAVED\n",
      "Epoch 79/200, Train Loss: 0.0094, Test Loss: 0.0110, Test MAER: 2.9480 %, Test MAE: 29.0385 MN, Test MSE: 5958.6046 MN^2 \n",
      "Epoch 80/200, Train Loss: 0.0093, Test Loss: 0.0104, Test MAER: 2.8618 %, Test MAE: 28.1205 MN, Test MSE: 5860.4169 MN^2 -> MODEL SAVED\n",
      "Epoch 81/200, Train Loss: 0.0092, Test Loss: 0.0109, Test MAER: 2.9583 %, Test MAE: 29.0464 MN, Test MSE: 5867.0311 MN^2 \n",
      "Epoch 82/200, Train Loss: 0.0091, Test Loss: 0.0103, Test MAER: 2.8509 %, Test MAE: 27.8651 MN, Test MSE: 5745.3995 MN^2 -> MODEL SAVED\n",
      "Epoch 83/200, Train Loss: 0.0091, Test Loss: 0.0109, Test MAER: 2.9545 %, Test MAE: 28.9065 MN, Test MSE: 5743.9817 MN^2 \n",
      "Epoch 84/200, Train Loss: 0.0089, Test Loss: 0.0101, Test MAER: 2.7921 %, Test MAE: 27.3286 MN, Test MSE: 5627.4030 MN^2 -> MODEL SAVED\n",
      "Epoch 85/200, Train Loss: 0.0089, Test Loss: 0.0099, Test MAER: 2.7761 %, Test MAE: 27.2391 MN, Test MSE: 5543.0061 MN^2 -> MODEL SAVED\n",
      "Epoch 86/200, Train Loss: 0.0087, Test Loss: 0.0100, Test MAER: 2.7936 %, Test MAE: 27.4482 MN, Test MSE: 5507.3925 MN^2 \n",
      "Epoch 87/200, Train Loss: 0.0086, Test Loss: 0.0097, Test MAER: 2.7587 %, Test MAE: 27.0417 MN, Test MSE: 5439.7469 MN^2 -> MODEL SAVED\n",
      "Epoch 88/200, Train Loss: 0.0086, Test Loss: 0.0099, Test MAER: 2.7755 %, Test MAE: 27.2588 MN, Test MSE: 5420.0570 MN^2 \n",
      "Epoch 89/200, Train Loss: 0.0085, Test Loss: 0.0095, Test MAER: 2.7356 %, Test MAE: 26.7912 MN, Test MSE: 5343.4461 MN^2 -> MODEL SAVED\n",
      "Epoch 90/200, Train Loss: 0.0084, Test Loss: 0.0097, Test MAER: 2.7620 %, Test MAE: 27.0586 MN, Test MSE: 5312.0231 MN^2 \n",
      "Epoch 91/200, Train Loss: 0.0083, Test Loss: 0.0094, Test MAER: 2.7148 %, Test MAE: 26.5219 MN, Test MSE: 5239.0013 MN^2 -> MODEL SAVED\n",
      "Epoch 92/200, Train Loss: 0.0083, Test Loss: 0.0096, Test MAER: 2.7561 %, Test MAE: 26.9582 MN, Test MSE: 5222.3295 MN^2 \n",
      "Epoch 93/200, Train Loss: 0.0082, Test Loss: 0.0092, Test MAER: 2.6926 %, Test MAE: 26.2353 MN, Test MSE: 5131.0732 MN^2 -> MODEL SAVED\n",
      "Epoch 94/200, Train Loss: 0.0081, Test Loss: 0.0096, Test MAER: 2.7561 %, Test MAE: 26.8707 MN, Test MSE: 5135.0493 MN^2 \n",
      "Epoch 95/200, Train Loss: 0.0080, Test Loss: 0.0090, Test MAER: 2.6803 %, Test MAE: 26.0237 MN, Test MSE: 5039.5567 MN^2 -> MODEL SAVED\n",
      "Epoch 96/200, Train Loss: 0.0080, Test Loss: 0.0097, Test MAER: 2.7892 %, Test MAE: 27.0658 MN, Test MSE: 5072.8790 MN^2 \n",
      "Epoch 97/200, Train Loss: 0.0079, Test Loss: 0.0090, Test MAER: 2.6405 %, Test MAE: 25.6833 MN, Test MSE: 4971.6260 MN^2 -> MODEL SAVED\n",
      "Epoch 98/200, Train Loss: 0.0079, Test Loss: 0.0088, Test MAER: 2.6407 %, Test MAE: 25.7666 MN, Test MSE: 4922.8227 MN^2 -> MODEL SAVED\n",
      "Epoch 99/200, Train Loss: 0.0077, Test Loss: 0.0089, Test MAER: 2.6559 %, Test MAE: 25.8961 MN, Test MSE: 4880.0533 MN^2 \n",
      "Epoch 100/200, Train Loss: 0.0077, Test Loss: 0.0087, Test MAER: 2.6274 %, Test MAE: 25.5918 MN, Test MSE: 4824.2736 MN^2 -> MODEL SAVED\n",
      "Epoch 101/200, Train Loss: 0.0076, Test Loss: 0.0088, Test MAER: 2.6644 %, Test MAE: 25.8747 MN, Test MSE: 4785.8041 MN^2 \n",
      "Epoch 102/200, Train Loss: 0.0075, Test Loss: 0.0085, Test MAER: 2.6159 %, Test MAE: 25.4370 MN, Test MSE: 4732.9208 MN^2 -> MODEL SAVED\n",
      "Epoch 103/200, Train Loss: 0.0075, Test Loss: 0.0087, Test MAER: 2.6760 %, Test MAE: 25.8924 MN, Test MSE: 4712.6029 MN^2 \n",
      "Epoch 104/200, Train Loss: 0.0074, Test Loss: 0.0083, Test MAER: 2.6019 %, Test MAE: 25.2255 MN, Test MSE: 4635.1390 MN^2 -> MODEL SAVED\n",
      "Epoch 105/200, Train Loss: 0.0074, Test Loss: 0.0087, Test MAER: 2.7091 %, Test MAE: 26.0853 MN, Test MSE: 4642.1669 MN^2 \n",
      "Epoch 106/200, Train Loss: 0.0073, Test Loss: 0.0081, Test MAER: 2.5896 %, Test MAE: 25.0370 MN, Test MSE: 4549.5706 MN^2 -> MODEL SAVED\n",
      "Epoch 107/200, Train Loss: 0.0073, Test Loss: 0.0088, Test MAER: 2.7650 %, Test MAE: 26.3876 MN, Test MSE: 4577.5512 MN^2 \n",
      "Epoch 108/200, Train Loss: 0.0072, Test Loss: 0.0081, Test MAER: 2.5949 %, Test MAE: 24.8991 MN, Test MSE: 4478.5327 MN^2 -> MODEL SAVED\n",
      "Epoch 109/200, Train Loss: 0.0072, Test Loss: 0.0092, Test MAER: 2.9153 %, Test MAE: 27.3994 MN, Test MSE: 4560.5657 MN^2 \n",
      "Epoch 110/200, Train Loss: 0.0072, Test Loss: 0.0080, Test MAER: 2.5260 %, Test MAE: 24.4569 MN, Test MSE: 4415.9677 MN^2 -> MODEL SAVED\n",
      "Epoch 111/200, Train Loss: 0.0071, Test Loss: 0.0078, Test MAER: 2.5486 %, Test MAE: 24.6319 MN, Test MSE: 4365.3122 MN^2 -> MODEL SAVED\n",
      "Epoch 112/200, Train Loss: 0.0070, Test Loss: 0.0078, Test MAER: 2.5824 %, Test MAE: 24.8301 MN, Test MSE: 4328.1845 MN^2 -> MODEL SAVED\n",
      "Epoch 113/200, Train Loss: 0.0069, Test Loss: 0.0079, Test MAER: 2.6113 %, Test MAE: 24.9947 MN, Test MSE: 4294.4688 MN^2 \n",
      "Epoch 114/200, Train Loss: 0.0068, Test Loss: 0.0076, Test MAER: 2.5551 %, Test MAE: 24.5195 MN, Test MSE: 4236.6051 MN^2 -> MODEL SAVED\n",
      "Epoch 115/200, Train Loss: 0.0068, Test Loss: 0.0077, Test MAER: 2.6044 %, Test MAE: 24.8192 MN, Test MSE: 4207.5697 MN^2 \n",
      "Epoch 116/200, Train Loss: 0.0067, Test Loss: 0.0075, Test MAER: 2.5557 %, Test MAE: 24.3832 MN, Test MSE: 4151.1635 MN^2 -> MODEL SAVED\n",
      "Epoch 117/200, Train Loss: 0.0067, Test Loss: 0.0076, Test MAER: 2.6113 %, Test MAE: 24.7406 MN, Test MSE: 4130.4042 MN^2 \n",
      "Epoch 118/200, Train Loss: 0.0066, Test Loss: 0.0073, Test MAER: 2.5378 %, Test MAE: 24.1276 MN, Test MSE: 4060.3121 MN^2 -> MODEL SAVED\n",
      "Epoch 119/200, Train Loss: 0.0066, Test Loss: 0.0076, Test MAER: 2.6334 %, Test MAE: 24.7929 MN, Test MSE: 4064.6638 MN^2 \n",
      "Epoch 120/200, Train Loss: 0.0066, Test Loss: 0.0071, Test MAER: 2.5346 %, Test MAE: 23.9715 MN, Test MSE: 3984.4904 MN^2 -> MODEL SAVED\n",
      "Epoch 121/200, Train Loss: 0.0065, Test Loss: 0.0078, Test MAER: 2.6941 %, Test MAE: 25.0722 MN, Test MSE: 4018.6944 MN^2 \n",
      "Epoch 122/200, Train Loss: 0.0065, Test Loss: 0.0071, Test MAER: 2.5344 %, Test MAE: 23.8415 MN, Test MSE: 3917.6109 MN^2 -> MODEL SAVED\n",
      "Epoch 123/200, Train Loss: 0.0065, Test Loss: 0.0079, Test MAER: 2.7775 %, Test MAE: 25.5756 MN, Test MSE: 3989.7154 MN^2 \n",
      "Epoch 124/200, Train Loss: 0.0065, Test Loss: 0.0070, Test MAER: 2.4699 %, Test MAE: 23.3813 MN, Test MSE: 3845.3467 MN^2 -> MODEL SAVED\n",
      "Epoch 125/200, Train Loss: 0.0064, Test Loss: 0.0069, Test MAER: 2.5332 %, Test MAE: 23.7916 MN, Test MSE: 3839.3934 MN^2 -> MODEL SAVED\n",
      "Epoch 126/200, Train Loss: 0.0063, Test Loss: 0.0073, Test MAER: 2.6337 %, Test MAE: 24.4721 MN, Test MSE: 3845.7154 MN^2 \n",
      "Epoch 127/200, Train Loss: 0.0063, Test Loss: 0.0068, Test MAER: 2.5195 %, Test MAE: 23.6238 MN, Test MSE: 3771.9083 MN^2 -> MODEL SAVED\n",
      "Epoch 128/200, Train Loss: 0.0063, Test Loss: 0.0073, Test MAER: 2.6520 %, Test MAE: 24.5259 MN, Test MSE: 3810.4972 MN^2 \n",
      "Epoch 129/200, Train Loss: 0.0062, Test Loss: 0.0067, Test MAER: 2.5208 %, Test MAE: 23.5669 MN, Test MSE: 3723.5476 MN^2 -> MODEL SAVED\n",
      "Epoch 130/200, Train Loss: 0.0062, Test Loss: 0.0076, Test MAER: 2.7510 %, Test MAE: 25.1953 MN, Test MSE: 3817.3645 MN^2 \n",
      "Epoch 131/200, Train Loss: 0.0062, Test Loss: 0.0067, Test MAER: 2.4599 %, Test MAE: 23.1859 MN, Test MSE: 3669.0634 MN^2 -> MODEL SAVED\n",
      "Epoch 132/200, Train Loss: 0.0061, Test Loss: 0.0067, Test MAER: 2.5304 %, Test MAE: 23.6388 MN, Test MSE: 3672.4834 MN^2 -> MODEL SAVED\n",
      "Epoch 133/200, Train Loss: 0.0061, Test Loss: 0.0070, Test MAER: 2.6292 %, Test MAE: 24.3474 MN, Test MSE: 3702.8718 MN^2 \n",
      "Epoch 134/200, Train Loss: 0.0060, Test Loss: 0.0066, Test MAER: 2.5317 %, Test MAE: 23.6301 MN, Test MSE: 3632.9380 MN^2 -> MODEL SAVED\n",
      "Epoch 135/200, Train Loss: 0.0060, Test Loss: 0.0071, Test MAER: 2.6622 %, Test MAE: 24.5696 MN, Test MSE: 3684.5821 MN^2 \n",
      "Epoch 136/200, Train Loss: 0.0060, Test Loss: 0.0065, Test MAER: 2.5221 %, Test MAE: 23.5166 MN, Test MSE: 3585.9063 MN^2 -> MODEL SAVED\n",
      "Epoch 137/200, Train Loss: 0.0060, Test Loss: 0.0072, Test MAER: 2.6953 %, Test MAE: 24.7513 MN, Test MSE: 3669.5272 MN^2 \n",
      "Epoch 138/200, Train Loss: 0.0060, Test Loss: 0.0065, Test MAER: 2.5250 %, Test MAE: 23.4675 MN, Test MSE: 3550.6027 MN^2 -> MODEL SAVED\n",
      "Epoch 139/200, Train Loss: 0.0060, Test Loss: 0.0074, Test MAER: 2.7726 %, Test MAE: 25.3056 MN, Test MSE: 3677.7301 MN^2 \n",
      "Epoch 140/200, Train Loss: 0.0060, Test Loss: 0.0064, Test MAER: 2.4391 %, Test MAE: 22.9701 MN, Test MSE: 3498.5126 MN^2 -> MODEL SAVED\n",
      "Epoch 141/200, Train Loss: 0.0059, Test Loss: 0.0064, Test MAER: 2.5392 %, Test MAE: 23.6299 MN, Test MSE: 3511.2208 MN^2 \n",
      "Epoch 142/200, Train Loss: 0.0058, Test Loss: 0.0068, Test MAER: 2.6461 %, Test MAE: 24.4124 MN, Test MSE: 3552.4633 MN^2 \n",
      "Epoch 143/200, Train Loss: 0.0058, Test Loss: 0.0064, Test MAER: 2.5423 %, Test MAE: 23.6272 MN, Test MSE: 3481.5477 MN^2 -> MODEL SAVED\n",
      "Epoch 144/200, Train Loss: 0.0058, Test Loss: 0.0068, Test MAER: 2.6525 %, Test MAE: 24.4302 MN, Test MSE: 3529.1670 MN^2 \n",
      "Epoch 145/200, Train Loss: 0.0058, Test Loss: 0.0063, Test MAER: 2.5237 %, Test MAE: 23.4741 MN, Test MSE: 3440.9581 MN^2 -> MODEL SAVED\n",
      "Epoch 146/200, Train Loss: 0.0057, Test Loss: 0.0069, Test MAER: 2.7036 %, Test MAE: 24.7992 MN, Test MSE: 3530.5952 MN^2 \n",
      "Epoch 147/200, Train Loss: 0.0057, Test Loss: 0.0062, Test MAER: 2.5071 %, Test MAE: 23.2884 MN, Test MSE: 3390.4702 MN^2 -> MODEL SAVED\n",
      "Epoch 148/200, Train Loss: 0.0057, Test Loss: 0.0071, Test MAER: 2.7643 %, Test MAE: 25.2247 MN, Test MSE: 3534.0697 MN^2 \n",
      "Epoch 149/200, Train Loss: 0.0057, Test Loss: 0.0061, Test MAER: 2.4179 %, Test MAE: 22.7210 MN, Test MSE: 3333.3915 MN^2 -> MODEL SAVED\n",
      "Epoch 150/200, Train Loss: 0.0056, Test Loss: 0.0062, Test MAER: 2.5234 %, Test MAE: 23.4932 MN, Test MSE: 3363.9838 MN^2 \n",
      "Epoch 151/200, Train Loss: 0.0056, Test Loss: 0.0065, Test MAER: 2.6092 %, Test MAE: 24.1392 MN, Test MSE: 3400.7176 MN^2 \n",
      "Epoch 152/200, Train Loss: 0.0056, Test Loss: 0.0061, Test MAER: 2.5202 %, Test MAE: 23.4467 MN, Test MSE: 3335.5890 MN^2 \n",
      "Epoch 153/200, Train Loss: 0.0055, Test Loss: 0.0065, Test MAER: 2.6243 %, Test MAE: 24.2654 MN, Test MSE: 3385.6495 MN^2 \n",
      "Epoch 154/200, Train Loss: 0.0055, Test Loss: 0.0060, Test MAER: 2.4859 %, Test MAE: 23.1456 MN, Test MSE: 3281.0092 MN^2 -> MODEL SAVED\n",
      "Epoch 155/200, Train Loss: 0.0055, Test Loss: 0.0067, Test MAER: 2.6737 %, Test MAE: 24.5904 MN, Test MSE: 3389.4654 MN^2 \n",
      "Epoch 156/200, Train Loss: 0.0055, Test Loss: 0.0059, Test MAER: 2.4729 %, Test MAE: 22.9837 MN, Test MSE: 3244.7153 MN^2 -> MODEL SAVED\n",
      "Epoch 157/200, Train Loss: 0.0055, Test Loss: 0.0069, Test MAER: 2.7495 %, Test MAE: 25.1770 MN, Test MSE: 3400.3331 MN^2 \n",
      "Epoch 158/200, Train Loss: 0.0055, Test Loss: 0.0059, Test MAER: 2.4392 %, Test MAE: 22.6561 MN, Test MSE: 3199.9829 MN^2 \n",
      "Epoch 159/200, Train Loss: 0.0056, Test Loss: 0.0077, Test MAER: 2.9350 %, Test MAE: 26.4554 MN, Test MSE: 3531.9911 MN^2 \n",
      "Epoch 160/200, Train Loss: 0.0056, Test Loss: 0.0058, Test MAER: 2.3263 %, Test MAE: 21.9761 MN, Test MSE: 3139.2048 MN^2 -> MODEL SAVED\n",
      "Epoch 161/200, Train Loss: 0.0054, Test Loss: 0.0058, Test MAER: 2.3800 %, Test MAE: 22.3760 MN, Test MSE: 3151.0602 MN^2 \n",
      "Epoch 162/200, Train Loss: 0.0054, Test Loss: 0.0058, Test MAER: 2.4503 %, Test MAE: 22.8435 MN, Test MSE: 3175.3886 MN^2 \n",
      "Epoch 163/200, Train Loss: 0.0053, Test Loss: 0.0061, Test MAER: 2.5473 %, Test MAE: 23.5422 MN, Test MSE: 3215.3469 MN^2 \n",
      "Epoch 164/200, Train Loss: 0.0053, Test Loss: 0.0058, Test MAER: 2.4342 %, Test MAE: 22.7078 MN, Test MSE: 3139.4547 MN^2 -> MODEL SAVED\n",
      "Epoch 165/200, Train Loss: 0.0053, Test Loss: 0.0061, Test MAER: 2.5754 %, Test MAE: 23.7509 MN, Test MSE: 3202.8639 MN^2 \n",
      "Epoch 166/200, Train Loss: 0.0053, Test Loss: 0.0056, Test MAER: 2.3945 %, Test MAE: 22.3663 MN, Test MSE: 3085.5247 MN^2 -> MODEL SAVED\n",
      "Epoch 167/200, Train Loss: 0.0053, Test Loss: 0.0063, Test MAER: 2.6103 %, Test MAE: 23.9819 MN, Test MSE: 3197.9874 MN^2 \n",
      "Epoch 168/200, Train Loss: 0.0052, Test Loss: 0.0056, Test MAER: 2.3814 %, Test MAE: 22.2341 MN, Test MSE: 3055.1391 MN^2 -> MODEL SAVED\n",
      "Epoch 169/200, Train Loss: 0.0052, Test Loss: 0.0064, Test MAER: 2.6341 %, Test MAE: 24.1175 MN, Test MSE: 3202.6950 MN^2 \n",
      "Epoch 170/200, Train Loss: 0.0052, Test Loss: 0.0055, Test MAER: 2.3659 %, Test MAE: 22.0606 MN, Test MSE: 3022.9391 MN^2 -> MODEL SAVED\n",
      "Epoch 171/200, Train Loss: 0.0052, Test Loss: 0.0064, Test MAER: 2.6492 %, Test MAE: 24.2441 MN, Test MSE: 3184.0614 MN^2 \n",
      "Epoch 172/200, Train Loss: 0.0052, Test Loss: 0.0054, Test MAER: 2.3514 %, Test MAE: 21.9409 MN, Test MSE: 2999.1552 MN^2 -> MODEL SAVED\n",
      "Epoch 173/200, Train Loss: 0.0052, Test Loss: 0.0065, Test MAER: 2.6555 %, Test MAE: 24.3017 MN, Test MSE: 3181.8005 MN^2 \n",
      "Epoch 174/200, Train Loss: 0.0052, Test Loss: 0.0054, Test MAER: 2.3488 %, Test MAE: 21.8201 MN, Test MSE: 2962.3376 MN^2 -> MODEL SAVED\n",
      "Epoch 175/200, Train Loss: 0.0052, Test Loss: 0.0067, Test MAER: 2.7449 %, Test MAE: 24.9346 MN, Test MSE: 3222.1418 MN^2 \n",
      "Epoch 176/200, Train Loss: 0.0052, Test Loss: 0.0054, Test MAER: 2.2576 %, Test MAE: 21.3349 MN, Test MSE: 2921.3463 MN^2 -> MODEL SAVED\n",
      "Epoch 177/200, Train Loss: 0.0051, Test Loss: 0.0054, Test MAER: 2.3569 %, Test MAE: 22.0493 MN, Test MSE: 2950.1456 MN^2 \n",
      "Epoch 178/200, Train Loss: 0.0050, Test Loss: 0.0058, Test MAER: 2.4888 %, Test MAE: 23.1100 MN, Test MSE: 3031.3073 MN^2 \n",
      "Epoch 179/200, Train Loss: 0.0050, Test Loss: 0.0054, Test MAER: 2.3518 %, Test MAE: 22.0298 MN, Test MSE: 2938.4899 MN^2 \n",
      "Epoch 180/200, Train Loss: 0.0050, Test Loss: 0.0058, Test MAER: 2.4668 %, Test MAE: 22.9031 MN, Test MSE: 2999.7027 MN^2 \n",
      "Epoch 181/200, Train Loss: 0.0050, Test Loss: 0.0053, Test MAER: 2.3341 %, Test MAE: 21.8764 MN, Test MSE: 2904.9319 MN^2 -> MODEL SAVED\n",
      "Epoch 182/200, Train Loss: 0.0050, Test Loss: 0.0058, Test MAER: 2.4872 %, Test MAE: 23.0430 MN, Test MSE: 2992.6424 MN^2 \n",
      "Epoch 183/200, Train Loss: 0.0050, Test Loss: 0.0053, Test MAER: 2.3211 %, Test MAE: 21.7935 MN, Test MSE: 2888.2473 MN^2 -> MODEL SAVED\n",
      "Epoch 184/200, Train Loss: 0.0050, Test Loss: 0.0060, Test MAER: 2.5602 %, Test MAE: 23.5833 MN, Test MSE: 3002.1327 MN^2 \n",
      "Epoch 185/200, Train Loss: 0.0049, Test Loss: 0.0051, Test MAER: 2.2695 %, Test MAE: 21.2384 MN, Test MSE: 2815.6447 MN^2 -> MODEL SAVED\n",
      "Epoch 186/200, Train Loss: 0.0050, Test Loss: 0.0065, Test MAER: 2.6903 %, Test MAE: 24.5785 MN, Test MSE: 3082.9392 MN^2 \n",
      "Epoch 187/200, Train Loss: 0.0050, Test Loss: 0.0051, Test MAER: 2.2568 %, Test MAE: 21.1761 MN, Test MSE: 2795.3874 MN^2 \n",
      "Epoch 188/200, Train Loss: 0.0050, Test Loss: 0.0063, Test MAER: 2.6519 %, Test MAE: 24.3489 MN, Test MSE: 3047.2248 MN^2 \n",
      "Epoch 189/200, Train Loss: 0.0049, Test Loss: 0.0051, Test MAER: 2.2776 %, Test MAE: 21.2991 MN, Test MSE: 2787.6809 MN^2 -> MODEL SAVED\n",
      "Epoch 190/200, Train Loss: 0.0050, Test Loss: 0.0065, Test MAER: 2.6986 %, Test MAE: 24.7540 MN, Test MSE: 3037.2905 MN^2 \n",
      "Epoch 191/200, Train Loss: 0.0049, Test Loss: 0.0051, Test MAER: 2.1883 %, Test MAE: 20.8001 MN, Test MSE: 2749.8999 MN^2 -> MODEL SAVED\n",
      "Epoch 192/200, Train Loss: 0.0048, Test Loss: 0.0051, Test MAER: 2.3029 %, Test MAE: 21.7183 MN, Test MSE: 2789.7496 MN^2 \n",
      "Epoch 193/200, Train Loss: 0.0048, Test Loss: 0.0055, Test MAER: 2.4320 %, Test MAE: 22.7207 MN, Test MSE: 2855.1424 MN^2 \n",
      "Epoch 194/200, Train Loss: 0.0047, Test Loss: 0.0051, Test MAER: 2.2894 %, Test MAE: 21.6065 MN, Test MSE: 2764.6459 MN^2 \n",
      "Epoch 195/200, Train Loss: 0.0048, Test Loss: 0.0057, Test MAER: 2.4919 %, Test MAE: 23.1579 MN, Test MSE: 2865.5944 MN^2 \n",
      "Epoch 196/200, Train Loss: 0.0047, Test Loss: 0.0050, Test MAER: 2.2684 %, Test MAE: 21.4888 MN, Test MSE: 2739.8761 MN^2 -> MODEL SAVED\n",
      "Epoch 197/200, Train Loss: 0.0048, Test Loss: 0.0057, Test MAER: 2.5009 %, Test MAE: 23.3055 MN, Test MSE: 2861.0200 MN^2 \n",
      "Epoch 198/200, Train Loss: 0.0047, Test Loss: 0.0050, Test MAER: 2.2640 %, Test MAE: 21.4457 MN, Test MSE: 2713.2210 MN^2 -> MODEL SAVED\n",
      "Epoch 199/200, Train Loss: 0.0047, Test Loss: 0.0057, Test MAER: 2.4872 %, Test MAE: 23.1747 MN, Test MSE: 2834.3378 MN^2 \n",
      "Epoch 200/200, Train Loss: 0.0047, Test Loss: 0.0049, Test MAER: 2.2448 %, Test MAE: 21.2949 MN, Test MSE: 2691.1084 MN^2 -> MODEL SAVED\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T16:28:24.268096Z",
     "start_time": "2025-01-20T16:09:10.657148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = 'cantilever'\n",
    "writer = SummaryWriter(f\"./runs/final/{dataset_name}/FEI_MLP/4/50/ReLU/lr_0.0004\")\n",
    "save_path = f\"./runs/final/{dataset_name}/FEI_MLP/4/50/ReLU/lr_0.0004/SAVE\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model_beam = main_final(writer, _train_ds[dataset_name], _test_ds[dataset_name], connectivity[dataset_name], support[dataset_name], hidden_size=50, n_hidden=4, activation=nn.ReLU,\n",
    "                        activation_params={}, learning_rate=0.0004, n_epochs=200, save_path=save_path)\n",
    "writer.close()"
   ],
   "id": "96ba72bf30d79c01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 1.7966, Test Loss: 1.3356, Test MAER: 87.5679 %, Test MAE: 588.8691 MN, Test MSE: 545909.1587 MN^2 \n",
      "Epoch 2/200, Train Loss: 0.8347, Test Loss: 0.4419, Test MAER: 44.0312 %, Test MAE: 327.0296 MN, Test MSE: 219770.1872 MN^2 \n",
      "Epoch 3/200, Train Loss: 0.3267, Test Loss: 0.2414, Test MAER: 33.1893 %, Test MAE: 241.1810 MN, Test MSE: 128136.5067 MN^2 \n",
      "Epoch 4/200, Train Loss: 0.2018, Test Loss: 0.1702, Test MAER: 26.1439 %, Test MAE: 197.4001 MN, Test MSE: 93019.8182 MN^2 \n",
      "Epoch 5/200, Train Loss: 0.1503, Test Loss: 0.1304, Test MAER: 21.9692 %, Test MAE: 168.9178 MN, Test MSE: 73218.7702 MN^2 \n",
      "Epoch 6/200, Train Loss: 0.1171, Test Loss: 0.1034, Test MAER: 18.6759 %, Test MAE: 144.5013 MN, Test MSE: 59087.9315 MN^2 \n",
      "Epoch 7/200, Train Loss: 0.0933, Test Loss: 0.0837, Test MAER: 16.2726 %, Test MAE: 124.4734 MN, Test MSE: 48352.1425 MN^2 \n",
      "Epoch 8/200, Train Loss: 0.0768, Test Loss: 0.0693, Test MAER: 14.7933 %, Test MAE: 110.1348 MN, Test MSE: 40226.7248 MN^2 \n",
      "Epoch 9/200, Train Loss: 0.0644, Test Loss: 0.0595, Test MAER: 13.3812 %, Test MAE: 98.8987 MN, Test MSE: 34735.0674 MN^2 \n",
      "Epoch 10/200, Train Loss: 0.0563, Test Loss: 0.0522, Test MAER: 12.1856 %, Test MAE: 89.6860 MN, Test MSE: 30650.6340 MN^2 \n",
      "Epoch 11/200, Train Loss: 0.0498, Test Loss: 0.0464, Test MAER: 11.1920 %, Test MAE: 82.3816 MN, Test MSE: 27324.8073 MN^2 \n",
      "Epoch 12/200, Train Loss: 0.0449, Test Loss: 0.0420, Test MAER: 10.3771 %, Test MAE: 76.7671 MN, Test MSE: 24873.2558 MN^2 \n",
      "Epoch 13/200, Train Loss: 0.0406, Test Loss: 0.0384, Test MAER: 9.7817 %, Test MAE: 72.6834 MN, Test MSE: 22822.0257 MN^2 \n",
      "Epoch 14/200, Train Loss: 0.0383, Test Loss: 0.0361, Test MAER: 9.1652 %, Test MAE: 69.0588 MN, Test MSE: 21480.1684 MN^2 \n",
      "Epoch 15/200, Train Loss: 0.0348, Test Loss: 0.0335, Test MAER: 8.6687 %, Test MAE: 66.2306 MN, Test MSE: 20049.1558 MN^2 \n",
      "Epoch 16/200, Train Loss: 0.0324, Test Loss: 0.0315, Test MAER: 8.2235 %, Test MAE: 63.6061 MN, Test MSE: 18944.4086 MN^2 \n",
      "Epoch 17/200, Train Loss: 0.0305, Test Loss: 0.0298, Test MAER: 7.8743 %, Test MAE: 61.4518 MN, Test MSE: 17973.7469 MN^2 \n",
      "Epoch 18/200, Train Loss: 0.0288, Test Loss: 0.0283, Test MAER: 7.5450 %, Test MAE: 59.4617 MN, Test MSE: 17131.4540 MN^2 \n",
      "Epoch 19/200, Train Loss: 0.0273, Test Loss: 0.0271, Test MAER: 7.2876 %, Test MAE: 57.7947 MN, Test MSE: 16438.2511 MN^2 \n",
      "Epoch 20/200, Train Loss: 0.0260, Test Loss: 0.0262, Test MAER: 7.1164 %, Test MAE: 56.7150 MN, Test MSE: 15895.3959 MN^2 \n",
      "Epoch 21/200, Train Loss: 0.0250, Test Loss: 0.0254, Test MAER: 6.9453 %, Test MAE: 55.6439 MN, Test MSE: 15384.4817 MN^2 \n",
      "Epoch 22/200, Train Loss: 0.0241, Test Loss: 0.0244, Test MAER: 6.7677 %, Test MAE: 54.2588 MN, Test MSE: 14794.0108 MN^2 \n",
      "Epoch 23/200, Train Loss: 0.0233, Test Loss: 0.0237, Test MAER: 6.7002 %, Test MAE: 53.6547 MN, Test MSE: 14350.4203 MN^2 \n",
      "Epoch 24/200, Train Loss: 0.0229, Test Loss: 0.0236, Test MAER: 6.5913 %, Test MAE: 53.2035 MN, Test MSE: 14213.6343 MN^2 \n",
      "Epoch 25/200, Train Loss: 0.0217, Test Loss: 0.0222, Test MAER: 6.3385 %, Test MAE: 51.2210 MN, Test MSE: 13539.9170 MN^2 \n",
      "Epoch 26/200, Train Loss: 0.0212, Test Loss: 0.0216, Test MAER: 6.2251 %, Test MAE: 50.4348 MN, Test MSE: 13167.6203 MN^2 \n",
      "Epoch 27/200, Train Loss: 0.0206, Test Loss: 0.0212, Test MAER: 6.0754 %, Test MAE: 49.5881 MN, Test MSE: 12905.6072 MN^2 \n",
      "Epoch 28/200, Train Loss: 0.0200, Test Loss: 0.0207, Test MAER: 5.9998 %, Test MAE: 48.9554 MN, Test MSE: 12614.2882 MN^2 \n",
      "Epoch 29/200, Train Loss: 0.0194, Test Loss: 0.0202, Test MAER: 5.9181 %, Test MAE: 48.3497 MN, Test MSE: 12335.4949 MN^2 \n",
      "Epoch 30/200, Train Loss: 0.0193, Test Loss: 0.0204, Test MAER: 6.0233 %, Test MAE: 48.7447 MN, Test MSE: 12166.7045 MN^2 \n",
      "Epoch 31/200, Train Loss: 0.0188, Test Loss: 0.0194, Test MAER: 5.7577 %, Test MAE: 47.1679 MN, Test MSE: 11788.1893 MN^2 \n",
      "Epoch 32/200, Train Loss: 0.0187, Test Loss: 0.0188, Test MAER: 5.5852 %, Test MAE: 46.2370 MN, Test MSE: 11524.0906 MN^2 \n",
      "Epoch 33/200, Train Loss: 0.0179, Test Loss: 0.0184, Test MAER: 5.4781 %, Test MAE: 45.5036 MN, Test MSE: 11309.5896 MN^2 \n",
      "Epoch 34/200, Train Loss: 0.0176, Test Loss: 0.0182, Test MAER: 5.4178 %, Test MAE: 45.0990 MN, Test MSE: 11151.6393 MN^2 \n",
      "Epoch 35/200, Train Loss: 0.0172, Test Loss: 0.0179, Test MAER: 5.3548 %, Test MAE: 44.7034 MN, Test MSE: 11002.4548 MN^2 \n",
      "Epoch 36/200, Train Loss: 0.0169, Test Loss: 0.0177, Test MAER: 5.3215 %, Test MAE: 44.5942 MN, Test MSE: 10850.2110 MN^2 \n",
      "Epoch 37/200, Train Loss: 0.0166, Test Loss: 0.0176, Test MAER: 5.2929 %, Test MAE: 44.3175 MN, Test MSE: 10755.3791 MN^2 \n",
      "Epoch 38/200, Train Loss: 0.0166, Test Loss: 0.0177, Test MAER: 5.4094 %, Test MAE: 44.8944 MN, Test MSE: 10622.0231 MN^2 \n",
      "Epoch 39/200, Train Loss: 0.0170, Test Loss: 0.0172, Test MAER: 5.1584 %, Test MAE: 43.7189 MN, Test MSE: 10560.9077 MN^2 \n",
      "Epoch 40/200, Train Loss: 0.0160, Test Loss: 0.0164, Test MAER: 5.0408 %, Test MAE: 42.6069 MN, Test MSE: 10145.7068 MN^2 \n",
      "Epoch 41/200, Train Loss: 0.0156, Test Loss: 0.0161, Test MAER: 5.0107 %, Test MAE: 42.1842 MN, Test MSE: 9902.2520 MN^2 \n",
      "Epoch 42/200, Train Loss: 0.0154, Test Loss: 0.0160, Test MAER: 4.9864 %, Test MAE: 41.9749 MN, Test MSE: 9855.2097 MN^2 \n",
      "Epoch 43/200, Train Loss: 0.0151, Test Loss: 0.0157, Test MAER: 4.9008 %, Test MAE: 41.4865 MN, Test MSE: 9722.6621 MN^2 \n",
      "Epoch 44/200, Train Loss: 0.0148, Test Loss: 0.0155, Test MAER: 4.8946 %, Test MAE: 41.2933 MN, Test MSE: 9549.8246 MN^2 \n",
      "Epoch 45/200, Train Loss: 0.0146, Test Loss: 0.0154, Test MAER: 4.8763 %, Test MAE: 41.0739 MN, Test MSE: 9456.3927 MN^2 \n",
      "Epoch 46/200, Train Loss: 0.0144, Test Loss: 0.0152, Test MAER: 4.8415 %, Test MAE: 40.7976 MN, Test MSE: 9336.9913 MN^2 \n",
      "Epoch 47/200, Train Loss: 0.0142, Test Loss: 0.0151, Test MAER: 4.7936 %, Test MAE: 40.5428 MN, Test MSE: 9268.3835 MN^2 \n",
      "Epoch 48/200, Train Loss: 0.0140, Test Loss: 0.0147, Test MAER: 4.7569 %, Test MAE: 40.1001 MN, Test MSE: 9049.8053 MN^2 \n",
      "Epoch 49/200, Train Loss: 0.0139, Test Loss: 0.0145, Test MAER: 4.7070 %, Test MAE: 39.8578 MN, Test MSE: 8969.8236 MN^2 \n",
      "Epoch 50/200, Train Loss: 0.0137, Test Loss: 0.0145, Test MAER: 4.6349 %, Test MAE: 39.6353 MN, Test MSE: 8944.9084 MN^2 \n",
      "Epoch 51/200, Train Loss: 0.0135, Test Loss: 0.0140, Test MAER: 4.6112 %, Test MAE: 39.1176 MN, Test MSE: 8670.8887 MN^2 \n",
      "Epoch 52/200, Train Loss: 0.0134, Test Loss: 0.0139, Test MAER: 4.5543 %, Test MAE: 38.8553 MN, Test MSE: 8595.8100 MN^2 -> MODEL SAVED\n",
      "Epoch 53/200, Train Loss: 0.0132, Test Loss: 0.0137, Test MAER: 4.5002 %, Test MAE: 38.5691 MN, Test MSE: 8504.8865 MN^2 -> MODEL SAVED\n",
      "Epoch 54/200, Train Loss: 0.0132, Test Loss: 0.0137, Test MAER: 4.4774 %, Test MAE: 38.5663 MN, Test MSE: 8493.9633 MN^2 -> MODEL SAVED\n",
      "Epoch 55/200, Train Loss: 0.0130, Test Loss: 0.0133, Test MAER: 4.4507 %, Test MAE: 37.9197 MN, Test MSE: 8245.2648 MN^2 -> MODEL SAVED\n",
      "Epoch 56/200, Train Loss: 0.0129, Test Loss: 0.0133, Test MAER: 4.4310 %, Test MAE: 37.8078 MN, Test MSE: 8192.4807 MN^2 -> MODEL SAVED\n",
      "Epoch 57/200, Train Loss: 0.0128, Test Loss: 0.0131, Test MAER: 4.3817 %, Test MAE: 37.4811 MN, Test MSE: 8112.0420 MN^2 -> MODEL SAVED\n",
      "Epoch 58/200, Train Loss: 0.0125, Test Loss: 0.0129, Test MAER: 4.3295 %, Test MAE: 37.1090 MN, Test MSE: 7973.1180 MN^2 -> MODEL SAVED\n",
      "Epoch 59/200, Train Loss: 0.0124, Test Loss: 0.0127, Test MAER: 4.3068 %, Test MAE: 36.8384 MN, Test MSE: 7818.3751 MN^2 -> MODEL SAVED\n",
      "Epoch 60/200, Train Loss: 0.0124, Test Loss: 0.0126, Test MAER: 4.2783 %, Test MAE: 36.7550 MN, Test MSE: 7800.8468 MN^2 -> MODEL SAVED\n",
      "Epoch 61/200, Train Loss: 0.0122, Test Loss: 0.0124, Test MAER: 4.2621 %, Test MAE: 36.4295 MN, Test MSE: 7652.8181 MN^2 -> MODEL SAVED\n",
      "Epoch 62/200, Train Loss: 0.0122, Test Loss: 0.0124, Test MAER: 4.2305 %, Test MAE: 36.5072 MN, Test MSE: 7697.9756 MN^2 \n",
      "Epoch 63/200, Train Loss: 0.0120, Test Loss: 0.0121, Test MAER: 4.2416 %, Test MAE: 36.0884 MN, Test MSE: 7459.6477 MN^2 -> MODEL SAVED\n",
      "Epoch 64/200, Train Loss: 0.0121, Test Loss: 0.0123, Test MAER: 4.2059 %, Test MAE: 36.1864 MN, Test MSE: 7552.3225 MN^2 \n",
      "Epoch 65/200, Train Loss: 0.0121, Test Loss: 0.0122, Test MAER: 4.1758 %, Test MAE: 36.1589 MN, Test MSE: 7541.4958 MN^2 \n",
      "Epoch 66/200, Train Loss: 0.0119, Test Loss: 0.0121, Test MAER: 4.1336 %, Test MAE: 35.8388 MN, Test MSE: 7458.4378 MN^2 -> MODEL SAVED\n",
      "Epoch 67/200, Train Loss: 0.0118, Test Loss: 0.0121, Test MAER: 4.1148 %, Test MAE: 35.8024 MN, Test MSE: 7457.2947 MN^2 -> MODEL SAVED\n",
      "Epoch 68/200, Train Loss: 0.0117, Test Loss: 0.0120, Test MAER: 4.1246 %, Test MAE: 35.8373 MN, Test MSE: 7438.8413 MN^2 -> MODEL SAVED\n",
      "Epoch 69/200, Train Loss: 0.0117, Test Loss: 0.0120, Test MAER: 4.0910 %, Test MAE: 35.7502 MN, Test MSE: 7420.0346 MN^2 -> MODEL SAVED\n",
      "Epoch 70/200, Train Loss: 0.0117, Test Loss: 0.0120, Test MAER: 4.0580 %, Test MAE: 35.5326 MN, Test MSE: 7382.9587 MN^2 -> MODEL SAVED\n",
      "Epoch 71/200, Train Loss: 0.0117, Test Loss: 0.0120, Test MAER: 4.0441 %, Test MAE: 35.5386 MN, Test MSE: 7394.4285 MN^2 \n",
      "Epoch 72/200, Train Loss: 0.0116, Test Loss: 0.0118, Test MAER: 3.9953 %, Test MAE: 35.1255 MN, Test MSE: 7281.4920 MN^2 -> MODEL SAVED\n",
      "Epoch 73/200, Train Loss: 0.0115, Test Loss: 0.0117, Test MAER: 3.9823 %, Test MAE: 35.0598 MN, Test MSE: 7244.7465 MN^2 -> MODEL SAVED\n",
      "Epoch 74/200, Train Loss: 0.0115, Test Loss: 0.0116, Test MAER: 3.9454 %, Test MAE: 34.6272 MN, Test MSE: 7154.9249 MN^2 -> MODEL SAVED\n",
      "Epoch 75/200, Train Loss: 0.0114, Test Loss: 0.0114, Test MAER: 3.9184 %, Test MAE: 34.4156 MN, Test MSE: 7086.3631 MN^2 -> MODEL SAVED\n",
      "Epoch 76/200, Train Loss: 0.0115, Test Loss: 0.0114, Test MAER: 3.9580 %, Test MAE: 34.6037 MN, Test MSE: 7039.9786 MN^2 -> MODEL SAVED\n",
      "Epoch 77/200, Train Loss: 0.0116, Test Loss: 0.0112, Test MAER: 3.8926 %, Test MAE: 34.0250 MN, Test MSE: 6910.0695 MN^2 -> MODEL SAVED\n",
      "Epoch 78/200, Train Loss: 0.0114, Test Loss: 0.0110, Test MAER: 3.8549 %, Test MAE: 33.6536 MN, Test MSE: 6801.9785 MN^2 -> MODEL SAVED\n",
      "Epoch 79/200, Train Loss: 0.0113, Test Loss: 0.0108, Test MAER: 3.8504 %, Test MAE: 33.3410 MN, Test MSE: 6644.0854 MN^2 -> MODEL SAVED\n",
      "Epoch 80/200, Train Loss: 0.0113, Test Loss: 0.0106, Test MAER: 3.8223 %, Test MAE: 33.0641 MN, Test MSE: 6551.0703 MN^2 -> MODEL SAVED\n",
      "Epoch 81/200, Train Loss: 0.0113, Test Loss: 0.0104, Test MAER: 3.7931 %, Test MAE: 32.6384 MN, Test MSE: 6407.5170 MN^2 -> MODEL SAVED\n",
      "Epoch 82/200, Train Loss: 0.0113, Test Loss: 0.0101, Test MAER: 3.7935 %, Test MAE: 32.3328 MN, Test MSE: 6246.6516 MN^2 -> MODEL SAVED\n",
      "Epoch 83/200, Train Loss: 0.0113, Test Loss: 0.0101, Test MAER: 3.7463 %, Test MAE: 32.1399 MN, Test MSE: 6222.4978 MN^2 -> MODEL SAVED\n",
      "Epoch 84/200, Train Loss: 0.0111, Test Loss: 0.0100, Test MAER: 3.7176 %, Test MAE: 31.9705 MN, Test MSE: 6176.9477 MN^2 -> MODEL SAVED\n",
      "Epoch 85/200, Train Loss: 0.0111, Test Loss: 0.0099, Test MAER: 3.6973 %, Test MAE: 31.7789 MN, Test MSE: 6144.2625 MN^2 -> MODEL SAVED\n",
      "Epoch 86/200, Train Loss: 0.0110, Test Loss: 0.0097, Test MAER: 3.6667 %, Test MAE: 31.3863 MN, Test MSE: 6019.7269 MN^2 -> MODEL SAVED\n",
      "Epoch 87/200, Train Loss: 0.0109, Test Loss: 0.0097, Test MAER: 3.6472 %, Test MAE: 31.2619 MN, Test MSE: 5984.3173 MN^2 -> MODEL SAVED\n",
      "Epoch 88/200, Train Loss: 0.0110, Test Loss: 0.0096, Test MAER: 3.6290 %, Test MAE: 31.1337 MN, Test MSE: 5927.4753 MN^2 -> MODEL SAVED\n",
      "Epoch 89/200, Train Loss: 0.0111, Test Loss: 0.0094, Test MAER: 3.6136 %, Test MAE: 30.8577 MN, Test MSE: 5820.5422 MN^2 -> MODEL SAVED\n",
      "Epoch 90/200, Train Loss: 0.0111, Test Loss: 0.0091, Test MAER: 3.6380 %, Test MAE: 30.5493 MN, Test MSE: 5621.3807 MN^2 -> MODEL SAVED\n",
      "Epoch 91/200, Train Loss: 0.0110, Test Loss: 0.0089, Test MAER: 3.6587 %, Test MAE: 30.3708 MN, Test MSE: 5489.6517 MN^2 -> MODEL SAVED\n",
      "Epoch 92/200, Train Loss: 0.0111, Test Loss: 0.0089, Test MAER: 3.6813 %, Test MAE: 30.3757 MN, Test MSE: 5408.6419 MN^2 -> MODEL SAVED\n",
      "Epoch 93/200, Train Loss: 0.0109, Test Loss: 0.0091, Test MAER: 3.9501 %, Test MAE: 31.7004 MN, Test MSE: 5519.1674 MN^2 \n",
      "Epoch 94/200, Train Loss: 0.0107, Test Loss: 0.0091, Test MAER: 4.0046 %, Test MAE: 31.8406 MN, Test MSE: 5497.0375 MN^2 \n",
      "Epoch 95/200, Train Loss: 0.0108, Test Loss: 0.0091, Test MAER: 3.9779 %, Test MAE: 31.8034 MN, Test MSE: 5543.5660 MN^2 \n",
      "Epoch 96/200, Train Loss: 0.0106, Test Loss: 0.0092, Test MAER: 4.0259 %, Test MAE: 32.1922 MN, Test MSE: 5592.2618 MN^2 \n",
      "Epoch 97/200, Train Loss: 0.0104, Test Loss: 0.0093, Test MAER: 3.9909 %, Test MAE: 32.1095 MN, Test MSE: 5638.4402 MN^2 \n",
      "Epoch 98/200, Train Loss: 0.0104, Test Loss: 0.0093, Test MAER: 4.0280 %, Test MAE: 32.3414 MN, Test MSE: 5655.5092 MN^2 \n",
      "Epoch 99/200, Train Loss: 0.0102, Test Loss: 0.0091, Test MAER: 3.9472 %, Test MAE: 31.6635 MN, Test MSE: 5560.9842 MN^2 \n",
      "Epoch 100/200, Train Loss: 0.0102, Test Loss: 0.0093, Test MAER: 4.0041 %, Test MAE: 32.1373 MN, Test MSE: 5635.5998 MN^2 \n",
      "Epoch 101/200, Train Loss: 0.0101, Test Loss: 0.0091, Test MAER: 3.9629 %, Test MAE: 31.7140 MN, Test MSE: 5543.9953 MN^2 \n",
      "Epoch 102/200, Train Loss: 0.0101, Test Loss: 0.0092, Test MAER: 3.9764 %, Test MAE: 31.9265 MN, Test MSE: 5588.4843 MN^2 \n",
      "Epoch 103/200, Train Loss: 0.0100, Test Loss: 0.0092, Test MAER: 3.9846 %, Test MAE: 32.0059 MN, Test MSE: 5631.1826 MN^2 \n",
      "Epoch 104/200, Train Loss: 0.0100, Test Loss: 0.0091, Test MAER: 3.9763 %, Test MAE: 31.9490 MN, Test MSE: 5585.6878 MN^2 \n",
      "Epoch 105/200, Train Loss: 0.0100, Test Loss: 0.0093, Test MAER: 4.0044 %, Test MAE: 32.2832 MN, Test MSE: 5671.6294 MN^2 \n",
      "Epoch 106/200, Train Loss: 0.0099, Test Loss: 0.0091, Test MAER: 3.9431 %, Test MAE: 31.7364 MN, Test MSE: 5531.7951 MN^2 \n",
      "Epoch 107/200, Train Loss: 0.0098, Test Loss: 0.0092, Test MAER: 3.9833 %, Test MAE: 32.0352 MN, Test MSE: 5624.1366 MN^2 \n",
      "Epoch 108/200, Train Loss: 0.0099, Test Loss: 0.0091, Test MAER: 3.9965 %, Test MAE: 32.0229 MN, Test MSE: 5585.2633 MN^2 \n",
      "Epoch 109/200, Train Loss: 0.0098, Test Loss: 0.0092, Test MAER: 4.0029 %, Test MAE: 32.1321 MN, Test MSE: 5625.1027 MN^2 \n",
      "Epoch 110/200, Train Loss: 0.0098, Test Loss: 0.0092, Test MAER: 4.0014 %, Test MAE: 32.1028 MN, Test MSE: 5661.4773 MN^2 \n",
      "Epoch 111/200, Train Loss: 0.0098, Test Loss: 0.0092, Test MAER: 4.0135 %, Test MAE: 32.0938 MN, Test MSE: 5644.7177 MN^2 \n",
      "Epoch 112/200, Train Loss: 0.0097, Test Loss: 0.0093, Test MAER: 4.0268 %, Test MAE: 32.2206 MN, Test MSE: 5680.2898 MN^2 \n",
      "Epoch 113/200, Train Loss: 0.0097, Test Loss: 0.0092, Test MAER: 4.0284 %, Test MAE: 32.1353 MN, Test MSE: 5651.9467 MN^2 \n",
      "Epoch 114/200, Train Loss: 0.0097, Test Loss: 0.0092, Test MAER: 4.0010 %, Test MAE: 32.0181 MN, Test MSE: 5627.8011 MN^2 \n",
      "Epoch 115/200, Train Loss: 0.0097, Test Loss: 0.0090, Test MAER: 3.9711 %, Test MAE: 31.7022 MN, Test MSE: 5541.0728 MN^2 \n",
      "Epoch 116/200, Train Loss: 0.0095, Test Loss: 0.0091, Test MAER: 3.9719 %, Test MAE: 31.8352 MN, Test MSE: 5610.0995 MN^2 \n",
      "Epoch 117/200, Train Loss: 0.0096, Test Loss: 0.0091, Test MAER: 4.0150 %, Test MAE: 31.8921 MN, Test MSE: 5591.7133 MN^2 \n",
      "Epoch 118/200, Train Loss: 0.0095, Test Loss: 0.0091, Test MAER: 3.9717 %, Test MAE: 31.6726 MN, Test MSE: 5568.2581 MN^2 \n",
      "Epoch 119/200, Train Loss: 0.0096, Test Loss: 0.0090, Test MAER: 3.9346 %, Test MAE: 31.4998 MN, Test MSE: 5532.1156 MN^2 \n",
      "Epoch 120/200, Train Loss: 0.0095, Test Loss: 0.0089, Test MAER: 3.8959 %, Test MAE: 31.2913 MN, Test MSE: 5462.4587 MN^2 \n",
      "Epoch 121/200, Train Loss: 0.0094, Test Loss: 0.0088, Test MAER: 3.8583 %, Test MAE: 31.0735 MN, Test MSE: 5419.6316 MN^2 -> MODEL SAVED\n",
      "Epoch 122/200, Train Loss: 0.0094, Test Loss: 0.0087, Test MAER: 3.8227 %, Test MAE: 30.6543 MN, Test MSE: 5355.8863 MN^2 -> MODEL SAVED\n",
      "Epoch 123/200, Train Loss: 0.0094, Test Loss: 0.0086, Test MAER: 3.8047 %, Test MAE: 30.5223 MN, Test MSE: 5313.0403 MN^2 -> MODEL SAVED\n",
      "Epoch 124/200, Train Loss: 0.0093, Test Loss: 0.0087, Test MAER: 3.8051 %, Test MAE: 30.5676 MN, Test MSE: 5351.6257 MN^2 \n",
      "Epoch 125/200, Train Loss: 0.0093, Test Loss: 0.0085, Test MAER: 3.7701 %, Test MAE: 30.1775 MN, Test MSE: 5221.9726 MN^2 -> MODEL SAVED\n",
      "Epoch 126/200, Train Loss: 0.0093, Test Loss: 0.0085, Test MAER: 3.7617 %, Test MAE: 30.2658 MN, Test MSE: 5244.4885 MN^2 \n",
      "Epoch 127/200, Train Loss: 0.0093, Test Loss: 0.0084, Test MAER: 3.7098 %, Test MAE: 29.8049 MN, Test MSE: 5137.8416 MN^2 -> MODEL SAVED\n",
      "Epoch 128/200, Train Loss: 0.0092, Test Loss: 0.0085, Test MAER: 3.7066 %, Test MAE: 29.9970 MN, Test MSE: 5214.1692 MN^2 \n",
      "Epoch 129/200, Train Loss: 0.0093, Test Loss: 0.0084, Test MAER: 3.6700 %, Test MAE: 29.7054 MN, Test MSE: 5156.0220 MN^2 \n",
      "Epoch 130/200, Train Loss: 0.0091, Test Loss: 0.0084, Test MAER: 3.6919 %, Test MAE: 29.9041 MN, Test MSE: 5181.0759 MN^2 \n",
      "Epoch 131/200, Train Loss: 0.0092, Test Loss: 0.0084, Test MAER: 3.6748 %, Test MAE: 29.7885 MN, Test MSE: 5139.0831 MN^2 -> MODEL SAVED\n",
      "Epoch 132/200, Train Loss: 0.0091, Test Loss: 0.0083, Test MAER: 3.6373 %, Test MAE: 29.5179 MN, Test MSE: 5103.1382 MN^2 -> MODEL SAVED\n",
      "Epoch 133/200, Train Loss: 0.0092, Test Loss: 0.0083, Test MAER: 3.6801 %, Test MAE: 29.5765 MN, Test MSE: 5102.5633 MN^2 -> MODEL SAVED\n",
      "Epoch 134/200, Train Loss: 0.0091, Test Loss: 0.0082, Test MAER: 3.5900 %, Test MAE: 29.1819 MN, Test MSE: 5020.0137 MN^2 -> MODEL SAVED\n",
      "Epoch 135/200, Train Loss: 0.0090, Test Loss: 0.0082, Test MAER: 3.6071 %, Test MAE: 29.2457 MN, Test MSE: 5025.0333 MN^2 \n",
      "Epoch 136/200, Train Loss: 0.0090, Test Loss: 0.0080, Test MAER: 3.5417 %, Test MAE: 28.8173 MN, Test MSE: 4912.4274 MN^2 -> MODEL SAVED\n",
      "Epoch 137/200, Train Loss: 0.0090, Test Loss: 0.0080, Test MAER: 3.5533 %, Test MAE: 28.8508 MN, Test MSE: 4914.4375 MN^2 -> MODEL SAVED\n",
      "Epoch 138/200, Train Loss: 0.0089, Test Loss: 0.0080, Test MAER: 3.5356 %, Test MAE: 28.8485 MN, Test MSE: 4932.2873 MN^2 \n",
      "Epoch 139/200, Train Loss: 0.0089, Test Loss: 0.0078, Test MAER: 3.4827 %, Test MAE: 28.4655 MN, Test MSE: 4825.2146 MN^2 -> MODEL SAVED\n",
      "Epoch 140/200, Train Loss: 0.0089, Test Loss: 0.0079, Test MAER: 3.5028 %, Test MAE: 28.5771 MN, Test MSE: 4868.0525 MN^2 \n",
      "Epoch 141/200, Train Loss: 0.0088, Test Loss: 0.0078, Test MAER: 3.4773 %, Test MAE: 28.4103 MN, Test MSE: 4839.3972 MN^2 \n",
      "Epoch 142/200, Train Loss: 0.0089, Test Loss: 0.0078, Test MAER: 3.4361 %, Test MAE: 28.1807 MN, Test MSE: 4780.5218 MN^2 -> MODEL SAVED\n",
      "Epoch 143/200, Train Loss: 0.0088, Test Loss: 0.0077, Test MAER: 3.4479 %, Test MAE: 28.0841 MN, Test MSE: 4748.4645 MN^2 -> MODEL SAVED\n",
      "Epoch 144/200, Train Loss: 0.0088, Test Loss: 0.0078, Test MAER: 3.3899 %, Test MAE: 28.0535 MN, Test MSE: 4787.3405 MN^2 \n",
      "Epoch 145/200, Train Loss: 0.0089, Test Loss: 0.0077, Test MAER: 3.4689 %, Test MAE: 28.1300 MN, Test MSE: 4724.3541 MN^2 -> MODEL SAVED\n",
      "Epoch 146/200, Train Loss: 0.0088, Test Loss: 0.0077, Test MAER: 3.3915 %, Test MAE: 27.9553 MN, Test MSE: 4774.6010 MN^2 \n",
      "Epoch 147/200, Train Loss: 0.0088, Test Loss: 0.0077, Test MAER: 3.4131 %, Test MAE: 27.9645 MN, Test MSE: 4718.8106 MN^2 -> MODEL SAVED\n",
      "Epoch 148/200, Train Loss: 0.0087, Test Loss: 0.0078, Test MAER: 3.3831 %, Test MAE: 28.0715 MN, Test MSE: 4802.8807 MN^2 \n",
      "Epoch 149/200, Train Loss: 0.0088, Test Loss: 0.0076, Test MAER: 3.4197 %, Test MAE: 27.8726 MN, Test MSE: 4679.5276 MN^2 -> MODEL SAVED\n",
      "Epoch 150/200, Train Loss: 0.0087, Test Loss: 0.0078, Test MAER: 3.3674 %, Test MAE: 28.0642 MN, Test MSE: 4826.9132 MN^2 \n",
      "Epoch 151/200, Train Loss: 0.0087, Test Loss: 0.0078, Test MAER: 3.3887 %, Test MAE: 28.0402 MN, Test MSE: 4802.4871 MN^2 \n",
      "Epoch 152/200, Train Loss: 0.0086, Test Loss: 0.0079, Test MAER: 3.3818 %, Test MAE: 28.2205 MN, Test MSE: 4880.7166 MN^2 \n",
      "Epoch 153/200, Train Loss: 0.0087, Test Loss: 0.0078, Test MAER: 3.4153 %, Test MAE: 28.1677 MN, Test MSE: 4817.4013 MN^2 \n",
      "Epoch 154/200, Train Loss: 0.0087, Test Loss: 0.0080, Test MAER: 3.4200 %, Test MAE: 28.5483 MN, Test MSE: 4949.0953 MN^2 \n",
      "Epoch 155/200, Train Loss: 0.0086, Test Loss: 0.0079, Test MAER: 3.4712 %, Test MAE: 28.5894 MN, Test MSE: 4883.5634 MN^2 \n",
      "Epoch 156/200, Train Loss: 0.0086, Test Loss: 0.0077, Test MAER: 3.4995 %, Test MAE: 28.4291 MN, Test MSE: 4755.9716 MN^2 \n",
      "Epoch 157/200, Train Loss: 0.0085, Test Loss: 0.0076, Test MAER: 3.3899 %, Test MAE: 27.7200 MN, Test MSE: 4695.5101 MN^2 \n",
      "Epoch 158/200, Train Loss: 0.0084, Test Loss: 0.0076, Test MAER: 3.3723 %, Test MAE: 27.7431 MN, Test MSE: 4692.3684 MN^2 -> MODEL SAVED\n",
      "Epoch 159/200, Train Loss: 0.0084, Test Loss: 0.0076, Test MAER: 3.3347 %, Test MAE: 27.5605 MN, Test MSE: 4717.0293 MN^2 \n",
      "Epoch 160/200, Train Loss: 0.0085, Test Loss: 0.0075, Test MAER: 3.3266 %, Test MAE: 27.3349 MN, Test MSE: 4646.5795 MN^2 -> MODEL SAVED\n",
      "Epoch 161/200, Train Loss: 0.0084, Test Loss: 0.0075, Test MAER: 3.2825 %, Test MAE: 27.3683 MN, Test MSE: 4682.7528 MN^2 \n",
      "Epoch 162/200, Train Loss: 0.0084, Test Loss: 0.0076, Test MAER: 3.3071 %, Test MAE: 27.5211 MN, Test MSE: 4730.7872 MN^2 \n",
      "Epoch 163/200, Train Loss: 0.0085, Test Loss: 0.0076, Test MAER: 3.2644 %, Test MAE: 27.2899 MN, Test MSE: 4698.0221 MN^2 \n",
      "Epoch 164/200, Train Loss: 0.0083, Test Loss: 0.0074, Test MAER: 3.2832 %, Test MAE: 27.1087 MN, Test MSE: 4598.8322 MN^2 -> MODEL SAVED\n",
      "Epoch 165/200, Train Loss: 0.0083, Test Loss: 0.0076, Test MAER: 3.3096 %, Test MAE: 27.4521 MN, Test MSE: 4716.2480 MN^2 \n",
      "Epoch 166/200, Train Loss: 0.0084, Test Loss: 0.0075, Test MAER: 3.3050 %, Test MAE: 27.1874 MN, Test MSE: 4639.8869 MN^2 \n",
      "Epoch 167/200, Train Loss: 0.0083, Test Loss: 0.0074, Test MAER: 3.2779 %, Test MAE: 27.0109 MN, Test MSE: 4590.1055 MN^2 -> MODEL SAVED\n",
      "Epoch 168/200, Train Loss: 0.0084, Test Loss: 0.0075, Test MAER: 3.2504 %, Test MAE: 27.1151 MN, Test MSE: 4663.7199 MN^2 \n",
      "Epoch 169/200, Train Loss: 0.0083, Test Loss: 0.0073, Test MAER: 3.1627 %, Test MAE: 26.5608 MN, Test MSE: 4560.8332 MN^2 -> MODEL SAVED\n",
      "Epoch 170/200, Train Loss: 0.0087, Test Loss: 0.0081, Test MAER: 3.3315 %, Test MAE: 28.1277 MN, Test MSE: 4952.7407 MN^2 \n",
      "Epoch 171/200, Train Loss: 0.0085, Test Loss: 0.0074, Test MAER: 3.2677 %, Test MAE: 26.9852 MN, Test MSE: 4530.1034 MN^2 \n",
      "Epoch 172/200, Train Loss: 0.0070, Test Loss: 0.0065, Test MAER: 3.0632 %, Test MAE: 25.7409 MN, Test MSE: 4001.4268 MN^2 -> MODEL SAVED\n",
      "Epoch 173/200, Train Loss: 0.0067, Test Loss: 0.0065, Test MAER: 3.0902 %, Test MAE: 25.8605 MN, Test MSE: 4004.1704 MN^2 \n",
      "Epoch 174/200, Train Loss: 0.0067, Test Loss: 0.0065, Test MAER: 3.0984 %, Test MAE: 25.9461 MN, Test MSE: 3965.9073 MN^2 -> MODEL SAVED\n",
      "Epoch 175/200, Train Loss: 0.0066, Test Loss: 0.0065, Test MAER: 3.1036 %, Test MAE: 26.0269 MN, Test MSE: 4003.0641 MN^2 \n",
      "Epoch 176/200, Train Loss: 0.0066, Test Loss: 0.0065, Test MAER: 3.0970 %, Test MAE: 25.9812 MN, Test MSE: 3994.0020 MN^2 \n",
      "Epoch 177/200, Train Loss: 0.0066, Test Loss: 0.0064, Test MAER: 3.0795 %, Test MAE: 25.8307 MN, Test MSE: 3952.6895 MN^2 -> MODEL SAVED\n",
      "Epoch 178/200, Train Loss: 0.0066, Test Loss: 0.0063, Test MAER: 3.0618 %, Test MAE: 25.6944 MN, Test MSE: 3913.8355 MN^2 -> MODEL SAVED\n",
      "Epoch 179/200, Train Loss: 0.0066, Test Loss: 0.0062, Test MAER: 3.0444 %, Test MAE: 25.4894 MN, Test MSE: 3851.2182 MN^2 -> MODEL SAVED\n",
      "Epoch 180/200, Train Loss: 0.0066, Test Loss: 0.0061, Test MAER: 3.0180 %, Test MAE: 25.2311 MN, Test MSE: 3773.3643 MN^2 -> MODEL SAVED\n",
      "Epoch 181/200, Train Loss: 0.0066, Test Loss: 0.0061, Test MAER: 3.0058 %, Test MAE: 25.1337 MN, Test MSE: 3736.7841 MN^2 -> MODEL SAVED\n",
      "Epoch 182/200, Train Loss: 0.0066, Test Loss: 0.0059, Test MAER: 2.9817 %, Test MAE: 24.8400 MN, Test MSE: 3643.6275 MN^2 -> MODEL SAVED\n",
      "Epoch 183/200, Train Loss: 0.0066, Test Loss: 0.0059, Test MAER: 2.9667 %, Test MAE: 24.6770 MN, Test MSE: 3618.0522 MN^2 -> MODEL SAVED\n",
      "Epoch 184/200, Train Loss: 0.0066, Test Loss: 0.0059, Test MAER: 2.9661 %, Test MAE: 24.7234 MN, Test MSE: 3625.0560 MN^2 \n",
      "Epoch 185/200, Train Loss: 0.0066, Test Loss: 0.0058, Test MAER: 2.9426 %, Test MAE: 24.4577 MN, Test MSE: 3542.6712 MN^2 -> MODEL SAVED\n",
      "Epoch 186/200, Train Loss: 0.0066, Test Loss: 0.0057, Test MAER: 2.9305 %, Test MAE: 24.3279 MN, Test MSE: 3503.5131 MN^2 -> MODEL SAVED\n",
      "Epoch 187/200, Train Loss: 0.0060, Test Loss: 0.0056, Test MAER: 2.9640 %, Test MAE: 24.9859 MN, Test MSE: 3389.1519 MN^2 -> MODEL SAVED\n",
      "Epoch 188/200, Train Loss: 0.0061, Test Loss: 0.0056, Test MAER: 2.9707 %, Test MAE: 25.1047 MN, Test MSE: 3410.8815 MN^2 \n",
      "Epoch 189/200, Train Loss: 0.0060, Test Loss: 0.0056, Test MAER: 2.9681 %, Test MAE: 25.0541 MN, Test MSE: 3391.1598 MN^2 \n",
      "Epoch 190/200, Train Loss: 0.0060, Test Loss: 0.0056, Test MAER: 2.9653 %, Test MAE: 24.9914 MN, Test MSE: 3371.1448 MN^2 -> MODEL SAVED\n",
      "Epoch 191/200, Train Loss: 0.0060, Test Loss: 0.0055, Test MAER: 2.9583 %, Test MAE: 24.8800 MN, Test MSE: 3331.8899 MN^2 -> MODEL SAVED\n",
      "Epoch 192/200, Train Loss: 0.0059, Test Loss: 0.0055, Test MAER: 2.9509 %, Test MAE: 24.7598 MN, Test MSE: 3302.5613 MN^2 -> MODEL SAVED\n",
      "Epoch 193/200, Train Loss: 0.0059, Test Loss: 0.0054, Test MAER: 2.9351 %, Test MAE: 24.5981 MN, Test MSE: 3269.3415 MN^2 -> MODEL SAVED\n",
      "Epoch 194/200, Train Loss: 0.0059, Test Loss: 0.0053, Test MAER: 2.9328 %, Test MAE: 24.5069 MN, Test MSE: 3238.6668 MN^2 -> MODEL SAVED\n",
      "Epoch 195/200, Train Loss: 0.0059, Test Loss: 0.0053, Test MAER: 2.9120 %, Test MAE: 24.3023 MN, Test MSE: 3203.3132 MN^2 -> MODEL SAVED\n",
      "Epoch 196/200, Train Loss: 0.0059, Test Loss: 0.0052, Test MAER: 2.8974 %, Test MAE: 24.1157 MN, Test MSE: 3169.8154 MN^2 -> MODEL SAVED\n",
      "Epoch 197/200, Train Loss: 0.0059, Test Loss: 0.0052, Test MAER: 2.8928 %, Test MAE: 23.9630 MN, Test MSE: 3125.2934 MN^2 -> MODEL SAVED\n",
      "Epoch 198/200, Train Loss: 0.0059, Test Loss: 0.0051, Test MAER: 2.8720 %, Test MAE: 23.7265 MN, Test MSE: 3083.7052 MN^2 -> MODEL SAVED\n",
      "Epoch 199/200, Train Loss: 0.0058, Test Loss: 0.0050, Test MAER: 2.8666 %, Test MAE: 23.6162 MN, Test MSE: 3056.2471 MN^2 -> MODEL SAVED\n",
      "Epoch 200/200, Train Loss: 0.0059, Test Loss: 0.0050, Test MAER: 2.8499 %, Test MAE: 23.4095 MN, Test MSE: 3020.3008 MN^2 -> MODEL SAVED\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f321c71d9decf975"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
