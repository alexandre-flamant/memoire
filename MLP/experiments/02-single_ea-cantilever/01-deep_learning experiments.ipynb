{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:26:51.781666Z",
     "start_time": "2025-02-21T12:26:51.777363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pwd\n",
    "%cd ../.."
   ],
   "id": "58fd7167621b4c64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aflamant/Documents/courses/2024-2025/meÃÅmoire/03-code/memoire/MLP\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:26:53.341803Z",
     "start_time": "2025-02-21T12:26:51.847603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RANDOM_STATE = 42\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torchmetrics.regression as R\n",
    "\n",
    "from MLP.dataset import TenBarsCantileverTrussSingleEADataset\n",
    "\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from MLP.models.architecture import MultiLayerPerceptron\n",
    "from MLP.models.processing import StandardScaler\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")"
   ],
   "id": "f073667fcbebe50c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load the data\n",
   "id": "6567808ced4fe2b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_path = \"./data/dataset/cantilever/data.hdf5\"\n",
    "_ds = TenBarsCantileverTrussSingleEADataset(data_path)\n",
    "\n",
    "ds = _ds[np.random.choice(np.arange(len(_ds)), 50000, replace=False)]\n",
    "in_dim = ds[0][0].__len__()\n",
    "out_dim = ds[0][1].__len__()\n",
    "\n",
    "print(f\"Dataset size: {len(ds)}\")\n",
    "print(f\"  Sample dimension: {in_dim}\")\n",
    "print(f\"  Target dimension: {out_dim}\")"
   ],
   "id": "5502a7293587797",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Training and Validation routine",
   "id": "93760e7333b6f0e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(model, train_ds, val_ds, lr, n_epochs, batch_size, verbose=True, plot=False):\n",
    "    model = model.to(device)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    x_scaler = StandardScaler(in_dim).to(device)\n",
    "    y_scaler = StandardScaler(out_dim).to(device)\n",
    "    for x, y, _, _, _ in train_dl:\n",
    "        x_scaler.partial_fit(x.to(device))\n",
    "        y_scaler.partial_fit(y.to(device))\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_MSE = []\n",
    "    val_MSE = []\n",
    "    train_MAPE = []\n",
    "    val_MAPE = []\n",
    "    train_R2 = []\n",
    "    val_R2 = []\n",
    "    train_D2 = []\n",
    "    val_D2 = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss_epoch = []\n",
    "        train_MSE_epoch = []\n",
    "        train_MAPE_epoch = []\n",
    "        train_R2_epoch = []\n",
    "        train_D2_epoch = []\n",
    "\n",
    "        for batch in train_dl:\n",
    "            x, y, _, _, _ = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            x = x_scaler.transform(x)\n",
    "            y = y_scaler.transform(y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y, y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_unscaled = y_scaler.inverse_transform(y).cpu().detach()\n",
    "            y_pred_unscaled = y_scaler.inverse_transform(y_pred).cpu().detach()\n",
    "\n",
    "            train_loss_epoch.append(loss.item())\n",
    "            train_MSE_epoch.append(F.mse_loss(y_pred, y).item())\n",
    "            train_MAPE_epoch.append(MAPE(y_unscaled, y_pred_unscaled))\n",
    "            train_D2_epoch.append(D2(y_unscaled, y_pred_unscaled))\n",
    "            train_R2_epoch.append(R2(y_unscaled, y_pred_unscaled))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_epoch = []\n",
    "        val_MSE_epoch = []\n",
    "        val_MAPE_epoch = []\n",
    "        val_R2_epoch = []\n",
    "        val_D2_epoch = []\n",
    "        for batch in val_dl:\n",
    "            x, y, _, _, _ = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            x = x_scaler.transform(x)\n",
    "            y = y_scaler.transform(y)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            y_unscaled = y_scaler.inverse_transform(y).cpu().detach()\n",
    "            y_pred_unscaled = y_scaler.inverse_transform(y_pred).cpu().detach()\n",
    "\n",
    "            val_loss_epoch.append(loss.item())\n",
    "            val_MSE_epoch.append(F.mse_loss(y_pred, y).item())\n",
    "            val_MAPE_epoch.append(MAPE(y_unscaled, y_pred_unscaled))\n",
    "            val_D2_epoch.append(D2(y_unscaled, y_pred_unscaled))\n",
    "            val_R2_epoch.append(R2(y_unscaled, y_pred_unscaled))\n",
    "\n",
    "        # Logging\n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": np.mean(train_loss_epoch),\n",
    "            \"train_mse\": np.mean(train_MSE_epoch),\n",
    "            \"train_mape\": np.mean(train_MAPE_epoch),\n",
    "            \"train_r2\": np.mean(train_R2_epoch),\n",
    "            \"train_d2\": np.mean(train_D2_epoch),\n",
    "\n",
    "            \"val_loss\": np.mean(val_loss_epoch),\n",
    "            \"val_mse\": np.mean(val_MSE_epoch),\n",
    "            \"val_mape\": np.mean(val_MAPE_epoch),\n",
    "            \"val_r2\": np.mean(val_R2_epoch),\n",
    "            \"val_d2\": np.mean(val_D2_epoch),\n",
    "        }, step=epoch)\n",
    "\n",
    "        train_losses.append(np.mean(train_loss_epoch))\n",
    "        val_losses.append(np.mean(val_loss_epoch))\n",
    "        train_MSE.append(np.mean(train_MSE_epoch))\n",
    "        val_MSE.append(np.mean(val_MSE_epoch))\n",
    "        train_MAPE.append(np.mean(train_MAPE_epoch))\n",
    "        val_MAPE.append(np.mean(val_MAPE_epoch))\n",
    "        train_R2.append(np.mean(train_R2_epoch))\n",
    "        val_R2.append(np.mean(val_R2_epoch))\n",
    "        train_D2.append(np.mean(train_D2_epoch))\n",
    "        val_D2.append(np.mean(val_D2_epoch))\n",
    "\n",
    "        if verbose and (epoch + 1) % 25 == 0:\n",
    "            print(f\"[Epoch] {epoch + 1:{len(str(n_epochs))}d}/{n_epochs:d}\", end='  ')\n",
    "            print(f\"TRAIN\", end='   ')\n",
    "            print(f\"Loss: {np.mean(train_loss_epoch):1.4f}\", end='   ')\n",
    "            print(f\"MSE: {np.mean(train_MSE_epoch):1.4f}\", end='   ')\n",
    "            print(f\"MAPE: {np.mean(train_MAPE_epoch):1.4f}\", end='   ')\n",
    "            print(f\"R2: {np.mean(train_R2_epoch): 1.4f}\", end='   ')\n",
    "            print(f\"D2: {np.mean(train_D2_epoch): 1.4f}\", end='')\n",
    "            print(\"  ||  \", end='')\n",
    "            print(f\"VALIDATION\", end='   ')\n",
    "            print(f\"Loss: {np.mean(val_loss_epoch):1.4f}\", end='   ')\n",
    "            print(f\"MSE: {np.mean(val_MSE_epoch):1.4f}\", end='   ')\n",
    "            print(f\"MAPE: {np.mean(val_MAPE_epoch):1.4f}\", end='   ')\n",
    "            print(f\"R2: {np.mean(val_R2_epoch): 1.4f}\", end='   ')\n",
    "            print(f\"D2: {np.mean(val_D2_epoch): 1.4f}\")\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(24, 8))\n",
    "        axs[0].set_title(\"Loss\")\n",
    "        axs[0].plot(train_losses, label='Training')\n",
    "        axs[0].plot(val_losses, label='Validation')\n",
    "        axs[0].set_yscale('log')\n",
    "        axs[3].set_xlabel(\"Epoch\")\n",
    "        axs[3].legend()\n",
    "\n",
    "        axs[1].set_title(\"MAPE\")\n",
    "        axs[1].plot(train_MAPE, label='Training')\n",
    "        axs[1].plot(val_MAPE, label='Validation')\n",
    "        axs[1].set_yscale('log')\n",
    "        axs[3].set_xlabel(\"Epoch\")\n",
    "        axs[3].legend()\n",
    "\n",
    "        axs[2].set_title(\"R2\")\n",
    "        axs[2].plot(train_R2, label='Training')\n",
    "        axs[2].plot(val_R2, label='Validation')\n",
    "        axs[2].set_yscale('function', functions=(lambda x: 10 ** x, lambda x: np.log10(x)))\n",
    "        axs[2].set_ylim(0, 1.0)\n",
    "        axs[3].set_xlabel(\"Epoch\")\n",
    "        axs[3].legend()\n",
    "\n",
    "        axs[3].set_title(\"D2\")\n",
    "        axs[3].plot(train_D2, label='Training')\n",
    "        axs[3].plot(val_D2, label='Validation')\n",
    "        axs[3].set_yscale('function', functions=(lambda x: 10 ** x, lambda x: np.log10(x)))\n",
    "        axs[3].set_ylim(0, 1.0)\n",
    "        axs[3].set_xlabel(\"Epoch\")\n",
    "        axs[3].legend()\n",
    "\n",
    "    signature = mlflow.models.infer_signature(x.cpu().detach().numpy(), model(x).cpu().detach().numpy())\n",
    "    model_info = mlflow.pytorch.log_model(\n",
    "        pytorch_model=model,\n",
    "        input_example=x.cpu().detach().numpy(),\n",
    "        artifact_path='model',\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"run_type\", \"kfold\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_MSE': train_MSE,\n",
    "        'val_MSE': val_MSE,\n",
    "        'train_MAPE': train_MAPE,\n",
    "        'val_MAPE': val_MAPE,\n",
    "        'train_R2': train_R2,\n",
    "        'val_R2': val_R2,\n",
    "        'train_D2': train_D2,\n",
    "        'val_D2': val_D2,\n",
    "    }"
   ],
   "id": "48a74fff01e9b052",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " # 3. Hyperparameter tuning\n",
    "Considering a 10 bar cantilever dataset we want to predict the EA of the bars which is assumed to be a single common value.\n",
    "The model is an MLP here are the parameters:\n",
    "- Activation function\n",
    "- Learning rate\n",
    "- Number of layers\n",
    "- Number of neurons per layer"
   ],
   "id": "180279513a44999"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Capacity training\n",
    "Capacity is assumed to be the number of trainable parameters. It is a function of the number of hidden layers and the number of neurons per layers.\n",
    "The more capacity the model has the more accurate it can be, but it also increase the risk of overfitting."
   ],
   "id": "a781b69519825d85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(\"MLP_Cantilever_Capacity_50K\")\n",
    "\n",
    "n_neurons_values = [25, 30, 35, 40]\n",
    "n_layers_values = [2, 3, 4]\n",
    "\n",
    "for n_neurons in n_neurons_values:\n",
    "    for n_layers in n_layers_values:\n",
    "        for fold, (train_idx, val_idx) in enumerate(KFold(n_splits=5, shuffle=True).split(ds)):\n",
    "            with mlflow.start_run(run_name=f\"{n_neurons}x{n_layers}_[{fold}]\"):\n",
    "                train_ds, val_ds = Subset(ds, train_idx), Subset(ds, val_idx)\n",
    "                model = MultiLayerPerceptron(in_dim, out_dim, n_neurons, n_layers, nn.ReLU)\n",
    "\n",
    "                model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "                n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "                mlflow.log_params({'n_neurons': n_neurons, 'n_layers': n_layers, 'lr': 1e-4,\n",
    "                                   'activation': 'ReLU', \"capacity\": n_params})\n",
    "\n",
    "                results = train(model=model,\n",
    "                                train_ds=train_ds,\n",
    "                                val_ds=val_ds,\n",
    "                                batch_size=2048,\n",
    "                                lr=4e-4,\n",
    "                                n_epochs=1000,\n",
    "                                verbose=True)\n"
   ],
   "id": "b7c2b30f2c99a982",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will then postprocess all these folds to extract the average measures",
   "id": "348606e9c017ffa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(\"MLP_Cantilever_Capacity\")\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(\"MLP_Cantilever_Capacity\")\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "runs_df = runs_df[runs_df['tags.health'].isnull()]\n",
    "all_run_names = runs_df[\"tags.mlflow.runName\"].dropna().tolist()\n",
    "\n",
    "n_neurons_values = [10, 15, 20, 25, 30, 35, 40]\n",
    "n_layers_values = [1, 2, 3, 4, 5]\n",
    "for fdfdf in n_neurons_values:\n",
    "    for n_layers in n_layers_values:\n",
    "        run_names = sorted([name for name in all_run_names if re.match(f\"{fdfdf}x{n_layers}_\\[\\d*\\]\", name)])\n",
    "        kfold_runs = runs_df[runs_df[\"tags.mlflow.runName\"].isin(run_names)]\n",
    "\n",
    "        if kfold_runs.shape[0] == 0: continue\n",
    "\n",
    "        # Extract train and validation loss\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_MSE = []\n",
    "        val_MSE = []\n",
    "        train_MAPE = []\n",
    "        val_MAPE = []\n",
    "        train_R2 = []\n",
    "        val_R2 = []\n",
    "        train_D2 = []\n",
    "        val_D2 = []\n",
    "\n",
    "        for _, run in kfold_runs.iterrows():\n",
    "            run_id = run[\"run_id\"]\n",
    "            client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "            # Get metric history\n",
    "            train_loss_hist = client.get_metric_history(run_id, \"train_loss\")\n",
    "            val_loss_hist = client.get_metric_history(run_id, \"val_loss\")\n",
    "            train_MSE_hist = client.get_metric_history(run_id, \"train_mse\")\n",
    "            val_MSE_hist = client.get_metric_history(run_id, \"val_mse\")\n",
    "            train_MAPE_hist = client.get_metric_history(run_id, \"train_mape\")\n",
    "            val_MAPE_hist = client.get_metric_history(run_id, \"val_mape\")\n",
    "            train_R2_hist = client.get_metric_history(run_id, \"train_r2\")\n",
    "            val_R2_hist = client.get_metric_history(run_id, \"val_r2\")\n",
    "            train_D2_hist = client.get_metric_history(run_id, \"train_d2\")\n",
    "            val_D2_hist = client.get_metric_history(run_id, \"val_d2\")\n",
    "\n",
    "            # Convert to lists of loss values per epoch\n",
    "            train_losses.append([m.value for m in train_loss_hist])\n",
    "            val_losses.append([m.value for m in val_loss_hist])\n",
    "\n",
    "            train_MSE.append([m.value for m in train_MSE_hist])\n",
    "            val_MSE.append([m.value for m in val_MSE_hist])\n",
    "\n",
    "            train_MAPE.append([m.value for m in train_MAPE_hist])\n",
    "            val_MAPE.append([m.value for m in val_MAPE_hist])\n",
    "\n",
    "            train_R2.append([m.value for m in train_R2_hist])\n",
    "            val_R2.append([m.value for m in val_R2_hist])\n",
    "\n",
    "            train_D2.append([m.value for m in train_D2_hist])\n",
    "            val_D2.append([m.value for m in val_D2_hist])\n",
    "\n",
    "            # Post process fold results\n",
    "            client.log_param(run_id, \"capacity\",\n",
    "                             ((in_dim * fdfdf) + fdfdf) +\n",
    "                             (n_layers - 1) * ((fdfdf * fdfdf) + fdfdf) +\n",
    "                             ((fdfdf * out_dim) + 1))\n",
    "\n",
    "            mlflow.log_metrics(\n",
    "                {\n",
    "                    'best_train_loss': np.min(train_losses),\n",
    "                    'best_val_loss': np.min(val_losses),\n",
    "                    'best_train_mse': np.min(train_MSE),\n",
    "                    'best_val_mse': np.min(val_MSE),\n",
    "                    'best_train_mape': np.min(train_MAPE),\n",
    "                    'best_val_mape': np.min(val_MAPE),\n",
    "                    'best_train_r2': np.max(train_R2),\n",
    "                    'best_val_r2': np.max(val_R2),\n",
    "                    'best_train_d2': np.max(train_D2),\n",
    "                    'best_val_d2': np.max(val_D2),\n",
    "                },\n",
    "                run_id=run_id)\n",
    "\n",
    "            client.set_tag(run_id, \"run_type\", \"KFold\")\n",
    "\n",
    "        # Aggregate fold results\n",
    "        train_losses = np.mean(train_losses, axis=0)\n",
    "        val_losses = np.mean(val_losses, axis=0)\n",
    "        train_MSE = np.mean(train_MSE, axis=0)\n",
    "        val_MSE = np.mean(val_MSE, axis=0)\n",
    "        train_MAPE = np.mean(train_MAPE, axis=0)\n",
    "        val_MAPE = np.mean(val_MAPE, axis=0)\n",
    "        train_R2 = np.mean(train_R2, axis=0)\n",
    "        val_R2 = np.mean(val_R2, axis=0)\n",
    "        train_D2 = np.mean(train_D2, axis=0)\n",
    "        val_D2 = np.mean(val_D2, axis=0)\n",
    "\n",
    "        best_train_loss = min(train_losses)\n",
    "        best_val_loss = min(val_losses)\n",
    "        best_train_MSE = min(train_MSE)\n",
    "        best_val_MSE = min(val_MSE)\n",
    "        best_train_MAPE = min(train_MAPE)\n",
    "        best_val_MAPE = min(val_MAPE)\n",
    "        best_train_R2 = max(train_R2)\n",
    "        best_val_R2 = max(val_R2)\n",
    "        best_train_D2 = max(train_D2)\n",
    "        best_val_D2 = max(val_D2)\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{fdfdf}x{n_layers}\"):\n",
    "            mlflow.log_params({'n_neurons': fdfdf, 'n_layers': n_layers, 'lr': 1e-4,\n",
    "                               'activation': 'ReLU', 'capacity': ((in_dim * fdfdf) + fdfdf) +\n",
    "                                                                 (n_layers - 1) * (\n",
    "                                                                         (fdfdf * fdfdf) + fdfdf) +\n",
    "                                                                 ((fdfdf * out_dim) + 1)})\n",
    "            mlflow.log_metrics({\n",
    "                \"best_train_loss\": best_train_loss,\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "                \"best_train_mse\": best_train_MSE,\n",
    "                \"best_val_mse\": best_val_MSE,\n",
    "                \"best_train_mape\": best_train_MAPE,\n",
    "                \"best_val_mape\": best_val_MAPE,\n",
    "                \"best_train_r2\": best_train_R2,\n",
    "                \"best_val_r2\": best_val_R2,\n",
    "                \"best_train_d2\": best_train_D2,\n",
    "                \"best_val_d2\": best_val_D2,\n",
    "            })\n",
    "\n",
    "            for i in range(len(train_losses)):\n",
    "                mlflow.log_metrics({\n",
    "                    \"train loss\": train_losses[i],\n",
    "                    \"val_loss\": val_losses[i],\n",
    "                    \"train_mse\": train_MSE[i],\n",
    "                    \"val_mse\": val_MSE[i],\n",
    "                    \"train_mape\": train_MAPE[i],\n",
    "                    \"val_mape\": val_MAPE[i],\n",
    "                    \"train_r2\": train_R2[i],\n",
    "                    \"val_r2\": val_R2[i],\n",
    "                    \"train_d2\": train_D2[i],\n",
    "                    \"val_d2\": val_D2[i],\n",
    "                }, step=i)\n",
    "\n",
    "            mlflow.set_tag(\"run_type\", \"KFold average\")"
   ],
   "id": "2e013b936adc4c4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Set up MLflow tracking\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(\"MLP_Cantilever_Activation_Function_50K\")\n",
    "\n",
    "# Hyperparameter values\n",
    "n_neurons_values = [30, 35, 40]\n",
    "n_layers_values = [3, 4]\n",
    "activations_values = [nn.LeakyReLU, nn.LeakyReLU, nn.LeakyReLU]\n",
    "activation_params_values = [{\"negative_slope\": 1.}, {\"negative_slope\": 5e-1}, {\"negative_slope\": 2.5e-1}]\n",
    "\n",
    "# Thread pool for parallel execution\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:  # Adjust max_workers as needed\n",
    "    futures = []\n",
    "\n",
    "    for activation, activation_params in zip(activations_values, activation_params_values):\n",
    "        for fdfdf in n_neurons_values:\n",
    "            for n_layers in n_layers_values:\n",
    "                for fold, (train_idx, val_idx) in enumerate(KFold(n_splits=5, shuffle=True).split(ds)):\n",
    "                    # Define run name\n",
    "                    param_str = f\"[{activation_params}]\" if activation_params is not None else \"[]\"\n",
    "                    run_name = f\"{activation.__name__}{param_str}_{fdfdf}x{n_layers}_[{fold}]\"\n",
    "\n",
    "\n",
    "                    # Start MLflow run\n",
    "                    def routine(run_name, train_idx, val_idx, n_neurons, n_layers, activation, activation_params):\n",
    "                        with mlflow.start_run(run_name=run_name):\n",
    "                            train_ds, val_ds = Subset(ds, train_idx), Subset(ds, val_idx)\n",
    "                            model = MultiLayerPerceptron(in_dim, out_dim, n_neurons, n_layers, activation,\n",
    "                                                         activation_params)\n",
    "\n",
    "                            # Compute model parameters\n",
    "                            model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "                            n_params = sum(np.prod(p.size()) for p in model_parameters)\n",
    "\n",
    "                            # Log experiment parameters\n",
    "                            mlflow.log_params({\n",
    "                                'n_neurons': n_neurons, 'n_layers': n_layers, 'lr': 1e-4,\n",
    "                                'activation': activation.__name__, \"capacity\": n_params\n",
    "                            })\n",
    "\n",
    "                            train(model=model, train_ds=train_ds, val_ds=val_ds,\n",
    "                                  batch_size=2048, lr=4e-4, n_epochs=1_000, verbose=False)\n",
    "\n",
    "\n",
    "                    # Submit training job to thread pool\n",
    "                    future = executor.submit(routine, run_name=run_name, train_idx=train_idx, val_idx=val_idx,\n",
    "                                             n_neurons=fdfdf, n_layers=n_layers,\n",
    "                                             activation=activation, activation_params=activation_params)\n",
    "                    futures.append(future)\n",
    "\n",
    "    # Ensure all threads complete execution\n",
    "    for future in futures:\n",
    "        future.result()"
   ],
   "id": "96f5b28204f644ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "run_name[:run_name.index(\"[\")]",
   "id": "9d846a3d3d59c2b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "experiment = mlflow.get_experiment_by_name(\"MLP_Cantilever_Activation_Function_50K\")\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "runs_df = runs_df[runs_df['tags.health'].isnull()]\n",
    "all_run_names = runs_df[\"tags.mlflow.runName\"].dropna().tolist()\n",
    "\n",
    "# Group runs\n",
    "run_names_grouped = []\n",
    "activations_values = [a.__name__ for a in [nn.LeakyReLU, nn.Sigmoid, nn.ReLU, nn.Tanh]]\n",
    "for activation in activations_values:\n",
    "    run_names = sorted([name for name in all_run_names if re.match(f\"{activation}.*\", name)])\n",
    "\n",
    "    params_list = set()\n",
    "    for run_name in run_names:\n",
    "        try:\n",
    "            params = run_name[run_name.index('{'):run_name.index('}') + 1]\n",
    "        except ValueError:\n",
    "            params = \"\"\n",
    "\n",
    "        params_list.add(params)\n",
    "\n",
    "    for params in params_list:\n",
    "        run_names_grouped.append([name for name in run_names if params in name])\n",
    "\n",
    "# Update\n",
    "for run_names in run_names_grouped:\n",
    "    kfold_runs = runs_df[runs_df[\"tags.mlflow.runName\"].isin(run_names)]\n",
    "    if kfold_runs.shape[0] == 0: continue\n",
    "\n",
    "    # Extract train and validation loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_MSE = []\n",
    "    val_MSE = []\n",
    "    train_MAPE = []\n",
    "    val_MAPE = []\n",
    "    train_R2 = []\n",
    "    val_R2 = []\n",
    "    train_D2 = []\n",
    "    val_D2 = []\n",
    "\n",
    "    for _, run in kfold_runs.iterrows():\n",
    "        run_id = run[\"run_id\"]\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "        # Get metric history\n",
    "        train_loss_hist = client.get_metric_history(run_id, \"train_loss\")\n",
    "        val_loss_hist = client.get_metric_history(run_id, \"val_loss\")\n",
    "        train_MSE_hist = client.get_metric_history(run_id, \"train_mse\")\n",
    "        val_MSE_hist = client.get_metric_history(run_id, \"val_mse\")\n",
    "        train_MAPE_hist = client.get_metric_history(run_id, \"train_mape\")\n",
    "        val_MAPE_hist = client.get_metric_history(run_id, \"val_mape\")\n",
    "        train_R2_hist = client.get_metric_history(run_id, \"train_r2\")\n",
    "        val_R2_hist = client.get_metric_history(run_id, \"val_r2\")\n",
    "        train_D2_hist = client.get_metric_history(run_id, \"train_d2\")\n",
    "        val_D2_hist = client.get_metric_history(run_id, \"val_d2\")\n",
    "\n",
    "        # Convert to lists of loss values per epoch\n",
    "        train_losses.append([m.value for m in train_loss_hist])\n",
    "        val_losses.append([m.value for m in val_loss_hist])\n",
    "\n",
    "        train_MSE.append([m.value for m in train_MSE_hist])\n",
    "        val_MSE.append([m.value for m in val_MSE_hist])\n",
    "\n",
    "        train_MAPE.append([m.value for m in train_MAPE_hist])\n",
    "        val_MAPE.append([m.value for m in val_MAPE_hist])\n",
    "\n",
    "        train_R2.append([m.value for m in train_R2_hist])\n",
    "        val_R2.append([m.value for m in val_R2_hist])\n",
    "\n",
    "        train_D2.append([m.value for m in train_D2_hist])\n",
    "        val_D2.append([m.value for m in val_D2_hist])\n",
    "\n",
    "        # Post process fold results\n",
    "\n",
    "        run_name = run[\"tags.mlflow.runName\"]\n",
    "        try:\n",
    "            params = run_name[run_name.index('{'):run_name.index('}') + 1]\n",
    "            params = ast.literal_eval(params)\n",
    "        except:\n",
    "            params = dict()\n",
    "\n",
    "        for k, v in params.items():\n",
    "            client.log_param(run_id, k, v)\n",
    "\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                'best_train_loss': np.min(train_losses),\n",
    "                'best_val_loss': np.min(val_losses),\n",
    "                'best_train_mse': np.min(train_MSE),\n",
    "                'best_val_mse': np.min(val_MSE),\n",
    "                'best_train_mape': np.min(train_MAPE),\n",
    "                'best_val_mape': np.min(val_MAPE),\n",
    "                'best_train_r2': np.max(train_R2),\n",
    "                'best_val_r2': np.max(val_R2),\n",
    "                'best_train_d2': np.max(train_D2),\n",
    "                'best_val_d2': np.max(val_D2),\n",
    "            },\n",
    "            run_id=run_id)\n",
    "\n",
    "        client.set_tag(run_id, \"run_type\", \"KFold\")\n",
    "\n",
    "    # Aggregate fold results\n",
    "    train_losses = np.mean(train_losses, axis=0)\n",
    "    val_losses = np.mean(val_losses, axis=0)\n",
    "    train_MSE = np.mean(train_MSE, axis=0)\n",
    "    val_MSE = np.mean(val_MSE, axis=0)\n",
    "    train_MAPE = np.mean(train_MAPE, axis=0)\n",
    "    val_MAPE = np.mean(val_MAPE, axis=0)\n",
    "    train_R2 = np.mean(train_R2, axis=0)\n",
    "    val_R2 = np.mean(val_R2, axis=0)\n",
    "    train_D2 = np.mean(train_D2, axis=0)\n",
    "    val_D2 = np.mean(val_D2, axis=0)\n",
    "\n",
    "    best_train_loss = min(train_losses)\n",
    "    best_val_loss = min(val_losses)\n",
    "    best_train_MSE = min(train_MSE)\n",
    "    best_val_MSE = min(val_MSE)\n",
    "    best_train_MAPE = min(train_MAPE)\n",
    "    best_val_MAPE = min(val_MAPE)\n",
    "    best_train_R2 = max(train_R2)\n",
    "    best_val_R2 = max(val_R2)\n",
    "    best_train_D2 = max(train_D2)\n",
    "    best_val_D2 = max(val_D2)\n",
    "\n",
    "    mlflow.set_experiment(\"MLP_Cantilever_Activation_Function_50K\")\n",
    "    with mlflow.start_run(run_name=run_name[:run_name.index(\"]\") + 1]):\n",
    "        mlflow.log_params({'lr': 1e-4,\n",
    "                           'activation': run_name[:run_name.index(\"[\")]})\n",
    "        for k, v in params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        mlflow.log_metrics({\n",
    "            \"best_train_loss\": best_train_loss,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"best_train_mse\": best_train_MSE,\n",
    "            \"best_val_mse\": best_val_MSE,\n",
    "            \"best_train_mape\": best_train_MAPE,\n",
    "            \"best_val_mape\": best_val_MAPE,\n",
    "            \"best_train_r2\": best_train_R2,\n",
    "            \"best_val_r2\": best_val_R2,\n",
    "            \"best_train_d2\": best_train_D2,\n",
    "            \"best_val_d2\": best_val_D2,\n",
    "        })\n",
    "\n",
    "        for i in range(len(train_losses)):\n",
    "            mlflow.log_metrics({\n",
    "                \"train loss\": train_losses[i],\n",
    "                \"val_loss\": val_losses[i],\n",
    "                \"train_mse\": train_MSE[i],\n",
    "                \"val_mse\": val_MSE[i],\n",
    "                \"train_mape\": train_MAPE[i],\n",
    "                \"val_mape\": val_MAPE[i],\n",
    "                \"train_r2\": train_R2[i],\n",
    "                \"val_r2\": val_R2[i],\n",
    "                \"train_d2\": train_D2[i],\n",
    "                \"val_d2\": val_D2[i],\n",
    "            }, step=i)\n",
    "\n",
    "        mlflow.set_tag(\"run_type\", \"KFold average\")\n"
   ],
   "id": "604522bc8ba57397",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Set up MLflow tracking\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(\"MLP_Cantilever_Learning_Rate_50K\")\n",
    "\n",
    "# Hyperparameter values\n",
    "n_neurons_values = [30, 35, 40]\n",
    "n_layers_values = [3, 4]\n",
    "activations_values = [nn.Tanh, nn.LeakyReLU]\n",
    "activation_params_values = [{}, {\"negative_slope\": 1.e-3}]\n",
    "\n",
    "lrs = np.logspace(-6, -3, 4)\n",
    "\n",
    "# Thread pool for parallel execution\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:  # Adjust max_workers as needed\n",
    "    futures = []\n",
    "\n",
    "    for lr in lrs:\n",
    "        for activation, activation_params in zip(activations_values, activation_params_values):\n",
    "            for n_neurons in n_neurons_values:\n",
    "                for n_layers in n_layers_values:\n",
    "                    for fold, (train_idx, val_idx) in enumerate(KFold(n_splits=5, shuffle=True).split(ds)):\n",
    "                        # Define run name\n",
    "                        param_str = f\"[{activation_params}]\"\n",
    "                        run_name = f\"{lr}_{activation.__name__}{param_str}_{n_neurons}x{n_layers}_[{fold}]\"\n",
    "\n",
    "\n",
    "                        # Start MLflow run\n",
    "                        def routine(run_name, train_idx, val_idx, n_neurons, n_layers, activation, activation_params):\n",
    "                            with mlflow.start_run(run_name=run_name):\n",
    "                                train_ds, val_ds = Subset(ds, train_idx), Subset(ds, val_idx)\n",
    "                                model = MultiLayerPerceptron(in_dim, out_dim, n_neurons, n_layers, activation,\n",
    "                                                             activation_params)\n",
    "\n",
    "                                # Compute model parameters\n",
    "                                model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "                                n_params = sum(np.prod(p.size()) for p in model_parameters)\n",
    "\n",
    "                                # Log experiment parameters\n",
    "                                mlflow.log_params({\n",
    "                                    'n_neurons': n_neurons, 'n_layers': n_layers, 'lr': lr,\n",
    "                                    'activation': activation.__name__, \"capacity\": n_params\n",
    "                                })\n",
    "\n",
    "                                for k, v in activation_params.items():\n",
    "                                    mlflow.log_param(k, v)\n",
    "\n",
    "                                train(model=model, train_ds=train_ds, val_ds=val_ds,\n",
    "                                      batch_size=2048, lr=lr, n_epochs=1_000, verbose=False)\n",
    "\n",
    "\n",
    "                        # Submit training job to thread pool\n",
    "                        future = executor.submit(routine, run_name=run_name, train_idx=train_idx, val_idx=val_idx,\n",
    "                                                 n_neurons=n_neurons, n_layers=n_layers,\n",
    "                                                 activation=activation, activation_params=activation_params)\n",
    "                        futures.append(future)\n",
    "\n",
    "        # Ensure all threads complete execution\n",
    "    for future in futures:\n",
    "        future.result()"
   ],
   "id": "8780a81d975a9d60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "experiment = mlflow.get_experiment_by_name(\"MLP_Cantilever_Learning_Rate_50K\")\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "runs_df = runs_df[runs_df['tags.health'].isnull()]\n",
    "all_run_names = runs_df[\"tags.mlflow.runName\"].dropna().tolist()\n",
    "\n",
    "# Group runs\n",
    "lrs = set()\n",
    "for run_name in all_run_names:\n",
    "    lrs.add(run_name.split('_')[0])\n",
    "\n",
    "run_names_grouped = []\n",
    "for lr in lrs:\n",
    "    group = []\n",
    "    for run_name in all_run_names:\n",
    "        if run_name.split('_')[0] == lr:\n",
    "            group.append(run_name)\n",
    "    run_names_grouped.append(group)\n",
    "\n",
    "# Update\n",
    "for run_names in run_names_grouped:\n",
    "    kfold_runs = runs_df[runs_df[\"tags.mlflow.runName\"].isin(run_names)]\n",
    "    if kfold_runs.shape[0] == 0: continue\n",
    "\n",
    "    # Extract train and validation loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_MSE = []\n",
    "    val_MSE = []\n",
    "    train_MAPE = []\n",
    "    val_MAPE = []\n",
    "    train_R2 = []\n",
    "    val_R2 = []\n",
    "    train_D2 = []\n",
    "    val_D2 = []\n",
    "\n",
    "    for _, run in kfold_runs.iterrows():\n",
    "        run_id = run[\"run_id\"]\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "        # Get metric history\n",
    "        train_loss_hist = client.get_metric_history(run_id, \"train_loss\")\n",
    "        val_loss_hist = client.get_metric_history(run_id, \"val_loss\")\n",
    "        train_MSE_hist = client.get_metric_history(run_id, \"train_mse\")\n",
    "        val_MSE_hist = client.get_metric_history(run_id, \"val_mse\")\n",
    "        train_MAPE_hist = client.get_metric_history(run_id, \"train_mape\")\n",
    "        val_MAPE_hist = client.get_metric_history(run_id, \"val_mape\")\n",
    "        train_R2_hist = client.get_metric_history(run_id, \"train_r2\")\n",
    "        val_R2_hist = client.get_metric_history(run_id, \"val_r2\")\n",
    "        train_D2_hist = client.get_metric_history(run_id, \"train_d2\")\n",
    "        val_D2_hist = client.get_metric_history(run_id, \"val_d2\")\n",
    "\n",
    "        # Convert to lists of loss values per epoch\n",
    "        train_losses.append([m.value for m in train_loss_hist])\n",
    "        val_losses.append([m.value for m in val_loss_hist])\n",
    "\n",
    "        train_MSE.append([m.value for m in train_MSE_hist])\n",
    "        val_MSE.append([m.value for m in val_MSE_hist])\n",
    "\n",
    "        train_MAPE.append([m.value for m in train_MAPE_hist])\n",
    "        val_MAPE.append([m.value for m in val_MAPE_hist])\n",
    "\n",
    "        train_R2.append([m.value for m in train_R2_hist])\n",
    "        val_R2.append([m.value for m in val_R2_hist])\n",
    "\n",
    "        train_D2.append([m.value for m in train_D2_hist])\n",
    "        val_D2.append([m.value for m in val_D2_hist])\n",
    "\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                'best_train_loss': np.min(train_losses),\n",
    "                'best_val_loss': np.min(val_losses),\n",
    "                'best_train_mse': np.min(train_MSE),\n",
    "                'best_val_mse': np.min(val_MSE),\n",
    "                'best_train_mape': np.min(train_MAPE),\n",
    "                'best_val_mape': np.min(val_MAPE),\n",
    "                'best_train_r2': np.max(train_R2),\n",
    "                'best_val_r2': np.max(val_R2),\n",
    "                'best_train_d2': np.max(train_D2),\n",
    "                'best_val_d2': np.max(val_D2),\n",
    "            },\n",
    "            run_id=run_id)\n",
    "\n",
    "        client.set_tag(run_id, \"run_type\", \"KFold\")\n",
    "\n",
    "    # Aggregate fold results\n",
    "    train_losses = np.mean(train_losses, axis=0)\n",
    "    val_losses = np.mean(val_losses, axis=0)\n",
    "    train_MSE = np.mean(train_MSE, axis=0)\n",
    "    val_MSE = np.mean(val_MSE, axis=0)\n",
    "    train_MAPE = np.mean(train_MAPE, axis=0)\n",
    "    val_MAPE = np.mean(val_MAPE, axis=0)\n",
    "    train_R2 = np.mean(train_R2, axis=0)\n",
    "    val_R2 = np.mean(val_R2, axis=0)\n",
    "    train_D2 = np.mean(train_D2, axis=0)\n",
    "    val_D2 = np.mean(val_D2, axis=0)\n",
    "\n",
    "    best_train_loss = min(train_losses)\n",
    "    best_val_loss = min(val_losses)\n",
    "    best_train_MSE = min(train_MSE)\n",
    "    best_val_MSE = min(val_MSE)\n",
    "    best_train_MAPE = min(train_MAPE)\n",
    "    best_val_MAPE = min(val_MAPE)\n",
    "    best_train_R2 = max(train_R2)\n",
    "    best_val_R2 = max(val_R2)\n",
    "    best_train_D2 = max(train_D2)\n",
    "    best_val_D2 = max(val_D2)\n",
    "\n",
    "    mlflow.set_experiment(\"MLP_Cantilever_Learning_Rate_50K\")\n",
    "    lr = ast.literal_eval(kfold_runs.iterrows().__next__()[1][\"tags.mlflow.runName\"].split(\"_\")[0])\n",
    "    with mlflow.start_run(run_name=str(lr)):\n",
    "        mlflow.log_params({'lr': lr})\n",
    "\n",
    "        mlflow.log_metrics({\n",
    "            \"best_train_loss\": best_train_loss,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"best_train_mse\": best_train_MSE,\n",
    "            \"best_val_mse\": best_val_MSE,\n",
    "            \"best_train_mape\": best_train_MAPE,\n",
    "            \"best_val_mape\": best_val_MAPE,\n",
    "            \"best_train_r2\": best_train_R2,\n",
    "            \"best_val_r2\": best_val_R2,\n",
    "            \"best_train_d2\": best_train_D2,\n",
    "            \"best_val_d2\": best_val_D2,\n",
    "        })\n",
    "\n",
    "        for i in range(len(train_losses)):\n",
    "            mlflow.log_metrics({\n",
    "                \"train loss\": train_losses[i],\n",
    "                \"val_loss\": val_losses[i],\n",
    "                \"train_mse\": train_MSE[i],\n",
    "                \"val_mse\": val_MSE[i],\n",
    "                \"train_mape\": train_MAPE[i],\n",
    "                \"val_mape\": val_MAPE[i],\n",
    "                \"train_r2\": train_R2[i],\n",
    "                \"val_r2\": val_R2[i],\n",
    "                \"train_d2\": train_D2[i],\n",
    "                \"val_d2\": val_D2[i],\n",
    "            }, step=i)\n",
    "\n",
    "        mlflow.set_tag(\"run_type\", \"KFold average\")\n"
   ],
   "id": "78a1482104c177f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Training the foundation model\n",
    "We will train our model with the whole dataset to create a foundation model that will have learnt all the specifics of the problem."
   ],
   "id": "5f0fd2f63ce01e6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_path = \"./data/dataset/cantilever/data.hdf5\"\n",
    "_ds = TenBarsCantileverTrussSingleEADataset(data_path)\n",
    "\n",
    "ds = _ds\n",
    "train_ds, val_ds = random_split(ds, (len(ds) - 25_000, 25_000))\n",
    "\n",
    "in_dim = ds[0][0].__len__()\n",
    "out_dim = ds[0][1].__len__()\n",
    "\n",
    "print(f\"Dataset size: {len(ds)}\")\n",
    "print(f\"  Sample dimension: {in_dim}\")\n",
    "print(f\"  Target dimension: {out_dim}\")\n",
    "print()\n",
    "print(f\"Train dataset size: {len(train_ds)}\")\n",
    "print()\n",
    "print(f\"Validation dataset size: {len(val_ds)}\")"
   ],
   "id": "b7defd6db3180a34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(\"MLP_Cantilever_final\")\n",
    "\n",
    "CCC = R.ConcordanceCorrCoef().to(device)\n",
    "R2 = R.R2Score(multioutput='uniform_average').to(device)\n",
    "MAPE = R.MeanAbsolutePercentageError().to(device)\n",
    "MSE = R.MeanSquaredError().to(device)\n",
    "\n",
    "print(\"Starting training\")\n",
    "\n",
    "best_loss = np.inf\n",
    "best_val_loss = np.inf\n",
    "with mlflow.start_run():\n",
    "    LR = 1e-4\n",
    "    N_NEURONS = 40\n",
    "    N_LAYERS = 3\n",
    "    N_EPOCHS = 2_500\n",
    "    BATCH_SIZE = 2048\n",
    "    ACTIVATION = nn.Tanh\n",
    "    ACTIVATION_PARAMS = {}\n",
    "\n",
    "    device = torch.device(\n",
    "        'cuda' if torch.cuda.is_available()\n",
    "        else 'mps' if torch.backends.mps.is_available()\n",
    "        else 'cpu'\n",
    "    )\n",
    "\n",
    "    model = MultiLayerPerceptron(in_dim, out_dim, N_NEURONS, N_LAYERS, ACTIVATION,\n",
    "                                 ACTIVATION_PARAMS).to(device)\n",
    "\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    n_params = sum(np.prod(p.size()) for p in model_parameters)\n",
    "    mlflow.log_params({\n",
    "        'n_neurons': N_NEURONS, 'n_layers': N_LAYERS, 'lr': LR,\n",
    "        'activation': ACTIVATION.__name__, \"capacity\": n_params, \"n_epochs\": N_EPOCHS,\n",
    "    })\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    x_scaler = StandardScaler(in_dim).to(device)\n",
    "    y_scaler = StandardScaler(out_dim).to(device)\n",
    "    for x, y, _, _, _ in train_dl:\n",
    "        x_scaler.partial_fit(x.to(device))\n",
    "        y_scaler.partial_fit(y.to(device))\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_MSE = []\n",
    "    val_MSE = []\n",
    "    train_MAPE = []\n",
    "    val_MAPE = []\n",
    "    train_R2 = []\n",
    "    val_R2 = []\n",
    "    train_CCC = []\n",
    "    val_CCC = []\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss_epoch = []\n",
    "        train_MSE_epoch = []\n",
    "        train_MAPE_epoch = []\n",
    "        train_R2_epoch = []\n",
    "        train_CCC_epoch = []\n",
    "\n",
    "        for batch in train_dl:\n",
    "            x, y, _, _, _ = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            x = x_scaler.transform(x)\n",
    "            y = y_scaler.transform(y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y, y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_unscaled = y_scaler.inverse_transform(y).cpu().detach()\n",
    "            y_pred_unscaled = y_scaler.inverse_transform(y_pred).cpu().detach()\n",
    "\n",
    "            train_loss_epoch.append(loss.item())\n",
    "            train_MSE_epoch.append(MSE(y_pred_unscaled, y_unscaled).item())\n",
    "            train_MAPE_epoch.append(MAPE(y_pred_unscaled, y_unscaled).item())\n",
    "            train_CCC_epoch.append(CCC(y_pred_unscaled, y_unscaled).item())\n",
    "            train_R2_epoch.append(R2(y_pred_unscaled, y_unscaled).item())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_epoch = []\n",
    "        val_MSE_epoch = []\n",
    "        val_MAPE_epoch = []\n",
    "        val_R2_epoch = []\n",
    "        val_CCC_epoch = []\n",
    "        for batch in val_dl:\n",
    "            x, y, _, _, _ = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            x = x_scaler.transform(x)\n",
    "            y = y_scaler.transform(y)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            y_unscaled = y_scaler.inverse_transform(y).cpu().detach()\n",
    "            y_pred_unscaled = y_scaler.inverse_transform(y_pred).cpu().detach()\n",
    "\n",
    "            val_loss_epoch.append(loss.item())\n",
    "            val_MSE_epoch.append(MSE(y_pred_unscaled, y_unscaled).item())\n",
    "            val_MAPE_epoch.append(MAPE(y_pred_unscaled, y_unscaled).item())\n",
    "            val_CCC_epoch.append(CCC(y_pred_unscaled, y_unscaled).item())\n",
    "            val_R2_epoch.append(R2(y_pred_unscaled, y_unscaled).item())\n",
    "\n",
    "        mean_train_loss = np.mean(train_loss_epoch)\n",
    "        mean_train_MSE = np.mean(train_MSE_epoch)\n",
    "        mean_train_MAPE = np.mean(train_MAPE_epoch)\n",
    "        mean_train_R2 = np.mean(train_R2_epoch)\n",
    "        mean_train_CCC = np.mean(train_CCC_epoch)\n",
    "\n",
    "        mean_val_loss = np.mean(val_loss_epoch)\n",
    "        mean_val_MSE = np.mean(val_MSE_epoch)\n",
    "        mean_val_MAPE = np.mean(val_MAPE_epoch)\n",
    "        mean_val_R2 = np.mean(val_R2_epoch)\n",
    "        mean_val_CCC = np.mean(val_CCC_epoch)\n",
    "\n",
    "        # Logging\n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": mean_train_loss,\n",
    "            \"train_mse\": mean_train_MSE,\n",
    "            \"train_mape\": mean_train_MAPE,\n",
    "            \"train_r2\": mean_train_R2,\n",
    "            \"train_ccc\": mean_train_CCC,\n",
    "\n",
    "            \"val_loss\": mean_val_loss,\n",
    "            \"val_mse\": mean_val_MSE,\n",
    "            \"val_mape\": mean_val_MAPE,\n",
    "            \"val_r2\": mean_val_R2,\n",
    "            \"val_ccc\": mean_val_CCC,\n",
    "        }, step=epoch)\n",
    "\n",
    "        train_losses.append(mean_train_loss)\n",
    "        val_losses.append(mean_val_loss)\n",
    "        train_MSE.append(mean_train_MSE)\n",
    "        val_MSE.append(mean_val_MSE)\n",
    "        train_MAPE.append(mean_train_MAPE)\n",
    "        val_MAPE.append(mean_val_MAPE)\n",
    "        train_R2.append(mean_train_R2)\n",
    "        val_R2.append(mean_val_R2)\n",
    "        train_CCC.append(mean_train_CCC)\n",
    "        val_CCC.append(mean_val_CCC)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"[Epoch] {epoch + 1:{len(str(N_EPOCHS))}d}/{N_EPOCHS:d}\", end='  ')\n",
    "            print(f\"TRAIN\", end='   ')\n",
    "            print(f\"Loss: {mean_train_loss:1.4f}\", end='   ')\n",
    "            print(f\"MSE: {mean_train_MSE:1.4e}\", end='   ')\n",
    "            print(f\"MAPE: {mean_train_MAPE:1.4f}\", end='   ')\n",
    "            print(f\"R2: {mean_train_R2: 1.4f}\", end='   ')\n",
    "            print(f\"CCC: {mean_train_CCC: 1.4f}\", end='')\n",
    "            print(\" ## \", end='')\n",
    "            print(f\"VALIDATION\", end='   ')\n",
    "            print(f\"Loss: {mean_val_loss:1.4f}\", end='   ')\n",
    "            print(f\"MSE: {mean_val_MSE:1.4e}\", end='   ')\n",
    "            print(f\"MAPE: {mean_val_MAPE:1.4f}\", end='   ')\n",
    "            print(f\"R2: {mean_val_R2: 1.4f}\", end='   ')\n",
    "            print(f\"CCC: {mean_val_CCC: 1.4f}\")\n",
    "\n",
    "    signature = mlflow.models.infer_signature(x.cpu().detach().numpy(), model(x).cpu().detach().numpy())\n",
    "\n",
    "    # Log all models\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model,\n",
    "        input_example=x.cpu().detach().numpy(),\n",
    "        artifact_path='model',\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "    signature = mlflow.models.infer_signature(x.cpu().detach().numpy(), x_scaler.transform(x).cpu().detach().numpy())\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=x_scaler,\n",
    "        artifact_path='x_scaler',\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "    signature = mlflow.models.infer_signature(y_pred.cpu().detach().numpy(),\n",
    "                                              y_scaler.transform(y_pred).cpu().detach().numpy())\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=y_scaler,\n",
    "        artifact_path='y_scaler',\n",
    "        signature=signature,\n",
    "    )"
   ],
   "id": "4844ba1bf8bede59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Prediction on real data\n",
    "We will use as *real* data, data with shared multiplicative noise:\n",
    "$$\\varepsilon \\sim \\mathcal N \\left( \\mu = 1, \\sigma = 0.0025 \\right)$$\n",
    "\n",
    "Such that $\\hat x = x * \\varepsilon$ has 95% chance of being within +- 0.5% of the true value. Which is the same order of magnitude observed with HBM sensors.\n",
    "\n",
    "This noise will be applied to a set of data from which a subset will be extracted for fine-tuning. The noised features are:\n",
    "- Displacement\n",
    "- Bar strain\n",
    "- Bars forces"
   ],
   "id": "3d54db98c5c73fbe"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "from MLP.dataset.generator import TenBarsCantileverTrussGenerator\n",
    "from MLP.dataset import TenBarsCantileverTrussDataset\n",
    "\n",
    "g = TenBarsCantileverTrussGenerator(\"data/config/cantilever_concrete.yaml\")\n",
    "\n",
    "default_config = g.default_config\n",
    "\n",
    "default_config['__area__'] = {'distribution': 'constant', 'value': 0.01}\n",
    "default_config['__young__'] = {'distribution': 'constant', 'value': 30.0e9}\n",
    "default_config['length'] = {'distribution': 'constant', 'value': 5.}\n",
    "default_config['height'] = {'distribution': 'constant', 'value': 5.}\n",
    "default_config['P_y_4']['shared_with'] = 'load'\n",
    "default_config['P_y_5']['shared_with'] = 'load'\n",
    "default_config['load'] = {'distribution': 'constant', 'value': 100.0e3}\n",
    "\n",
    "configs = []\n",
    "for q in np.arange(0, 751.e3, 1.e3):\n",
    "    config = default_config.copy()\n",
    "    config['load'] = {'distribution': 'constant', 'value': q}\n",
    "    configs.append(config)\n",
    "\n",
    "results = []\n",
    "for config in configs:\n",
    "    results.append(g.__iter__(config, 1).__next__())"
   ],
   "id": "c6cea77e1184e7d1"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# Adding noise\n",
    "for result in results:\n",
    "    e = np.random.normal(1, 0.0025, size=result[\"nodes_displacement\"].shape)\n",
    "    result[\"nodes_displacement\"] *= e\n",
    "\n",
    "    e = np.random.normal(1, 0.0025, size=result[\"bars_elongation\"].shape)\n",
    "    result[\"bars_elongation\"] *= e\n",
    "    result[\"bars_strain\"] = result[\"bars_elongation\"]/result[\"bars_length_init\"]\n",
    "\n",
    "    e = np.random.normal(1, 0.0025, size=result[\"bars_force\"].shape)\n",
    "    result[\"bars_force\"] *= e"
   ],
   "id": "24d8bdb6a1be5a9e"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "g.save_from_iterator(results, directory=\"./data/dataset/real_cantilever\", max_size=len(results))",
   "id": "f6653dc2e4f408ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:43:50.232421Z",
     "start_time": "2025-02-21T12:43:50.206878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_ds = TenBarsCantileverTrussSingleEADataset(\"./data/dataset/real_cantilever/data.hdf5\")\n",
    "ref = _ds[750]\n",
    "ds = Subset(_ds, np.arange(750))\n",
    "train_ds = Subset(_ds, np.arange(0, 750, 5))\n",
    "val_ds = Subset(_ds, sorted(list(set(np.arange(751)) - set(np.arange(0, 750, 5)))))"
   ],
   "id": "47e1abe4a4ad126f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:43:51.697625Z",
     "start_time": "2025-02-21T12:43:51.694444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, x_scaler, model, y_scaler):\n",
    "        super().__init__()\n",
    "        self.x_scaler = x_scaler\n",
    "        self.model = model\n",
    "        self.y_scaler = y_scaler\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.x_scaler.transform(x)\n",
    "        x = self.model(x)\n",
    "        x = self.y_scaler.inverse_transform(x)\n",
    "        return x"
   ],
   "id": "26a99ac2e3ac4f9b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:43:53.058967Z",
     "start_time": "2025-02-21T12:43:53.040862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CCC = R.ConcordanceCorrCoef().to(device)\n",
    "R2 = R.R2Score(multioutput='uniform_average').to(device)\n",
    "MAPE = R.MeanAbsolutePercentageError().to(device)\n",
    "MSE = R.MeanSquaredError().to(device)"
   ],
   "id": "7d121ff6d2a4adeb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## a. Non fine-tuned prediction\n",
    "Scores using the foundation model for prediction"
   ],
   "id": "29dadd240cc35cd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:43:54.391009Z",
     "start_time": "2025-02-21T12:43:54.136164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "\n",
    "# Load the model\n",
    "artifact_dir = 'runs:/32ba2bcaf225416e82c55ae552dbb628'\n",
    "\n",
    "model = Model(mlflow.pytorch.load_model(f\"{artifact_dir}/x_scaler\"),\n",
    "              mlflow.pytorch.load_model(f\"{artifact_dir}/model\"),\n",
    "              mlflow.pytorch.load_model(f\"{artifact_dir}/y_scaler\"))"
   ],
   "id": "5d4bf08b6103e02e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62c576eca25748db8b255fe5f693fe10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1a90168fcb743cda50e9dd9afb9a2e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "535c6c686c7043f691e379886181b52d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:43:57.194919Z",
     "start_time": "2025-02-21T12:43:56.898822Z"
    }
   },
   "cell_type": "code",
   "source": "y_pred_init = model(ref[0].to(device))",
   "id": "346e8a0229fe5f99",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:43:57.243096Z",
     "start_time": "2025-02-21T12:43:57.197280Z"
    }
   },
   "cell_type": "code",
   "source": "error_init = MAPE(y_pred_init.reshape(1).to(device), ref[1].to(device))",
   "id": "9fff6093a813f2d4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:43:57.732020Z",
     "start_time": "2025-02-21T12:43:57.703578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Prediction: {y_pred_init.item() * 1e-6:.3f} MN\")\n",
    "print(f\"Expected: {ref[1].to(device).item() * 1e-6:.3f} MN\")\n",
    "print(f\"MAPE: {error_init * 100:.3f}%\")"
   ],
   "id": "ae071410eb067122",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 2517.327 MN\n",
      "Expected: 300.000 MN\n",
      "MAPE: 739.109%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## b. Fine-tuned model\n",
    "We will finetune the foundation model using the subset of real data as input\n"
   ],
   "id": "5dd04240f84aaa01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### I. Experiment on the size of fine-tuning set\n",
    "These experiments will help us define how many real example are needed for *sufficient* fine-tuning."
   ],
   "id": "a5058d47065dfffa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Without PINN",
   "id": "2ad2ad24581a628f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T12:50:41.486019Z",
     "start_time": "2025-02-21T12:50:41.473817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def finetune_no_pinn(train_ds, val_ds, train_size, n_epoch=100, verbose=False):\n",
    "    mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "    mlflow.set_experiment(\"MLP_Cantilever_finetuning\")\n",
    "\n",
    "    artifact_dir = 'runs:/32ba2bcaf225416e82c55ae552dbb628'\n",
    "    x_scaler = mlflow.pytorch.load_model(f\"{artifact_dir}/x_scaler\")\n",
    "    model = mlflow.pytorch.load_model(f\"{artifact_dir}/model\")\n",
    "    y_scaler = mlflow.pytorch.load_model(f\"{artifact_dir}/y_scaler\")\n",
    "\n",
    "    # CCC = R.ConcordanceCorrCoef().to(device)\n",
    "    # R2 = R.R2Score(multioutput='uniform_average').to(device)\n",
    "    MAPE = R.MeanAbsolutePercentageError().to(device)\n",
    "    # MSE = R.MeanSquaredError().to(device)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        LR = 1e-4\n",
    "        N_NEURONS = 40\n",
    "        N_LAYERS = 3\n",
    "        N_EPOCHS = n_epoch\n",
    "        BATCH_SIZE = min(8, train_size)\n",
    "        ACTIVATION = nn.Tanh\n",
    "\n",
    "        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        n_params = sum(np.prod(p.size()) for p in model_parameters)\n",
    "        mlflow.log_params({\n",
    "            'n_neurons': N_NEURONS, 'n_layers': N_LAYERS, 'lr': LR,\n",
    "            'activation': ACTIVATION.__name__, \"capacity\": n_params, \"n_epochs\": N_EPOCHS,\n",
    "            'FEI': False, 'train_size': train_size\n",
    "        })\n",
    "\n",
    "        train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        val_dl = DataLoader(val_ds, batch_size=len(val_ds), shuffle=True,)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        # train_MSE = []\n",
    "        # val_MSE = []\n",
    "        train_MAPE = []\n",
    "        val_MAPE = []\n",
    "        # train_R2 = []\n",
    "        # val_R2 = []\n",
    "        # train_CCC = []\n",
    "        # val_CCC = []\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            model.train()\n",
    "            train_loss_epoch = []\n",
    "            train_MSE_epoch = []\n",
    "            train_MAPE_epoch = []\n",
    "            train_R2_epoch = []\n",
    "            train_CCC_epoch = []\n",
    "\n",
    "            for batch in train_dl:\n",
    "                x, y, _, _, _ = batch\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                x = x_scaler.transform(x)\n",
    "                y = y_scaler.transform(y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y, y_pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                y_unscaled = y_scaler.inverse_transform(y).cpu().detach()\n",
    "                y_pred_unscaled = y_scaler.inverse_transform(y_pred).cpu().detach()\n",
    "\n",
    "                train_loss_epoch.append(loss.item())\n",
    "                # train_MSE_epoch.append(MSE(y_pred_unscaled, y_unscaled).item())\n",
    "                train_MAPE_epoch.append(MAPE(y_pred_unscaled, y_unscaled).item())\n",
    "                # train_CCC_epoch.append(CCC(y_pred_unscaled, y_unscaled).item())\n",
    "                # train_R2_epoch.append(R2(y_pred_unscaled, y_unscaled).item())\n",
    "\n",
    "            model.eval()\n",
    "            val_loss_epoch = []\n",
    "            # val_MSE_epoch = []\n",
    "            val_MAPE_epoch = []\n",
    "            # val_R2_epoch = []\n",
    "            # val_CCC_epoch = []\n",
    "            for batch in val_dl:\n",
    "                x, y, _, _, _ = batch\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                x = x_scaler.transform(x)\n",
    "                y = y_scaler.transform(y)\n",
    "\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "\n",
    "                y_unscaled = y_scaler.inverse_transform(y).cpu().detach()\n",
    "                y_pred_unscaled = y_scaler.inverse_transform(y_pred).cpu().detach()\n",
    "\n",
    "                val_loss_epoch.append(loss.item())\n",
    "                #  val_MSE_epoch.append(MSE(y_pred_unscaled, y_unscaled).item())\n",
    "                val_MAPE_epoch.append(MAPE(y_pred_unscaled, y_unscaled).item())\n",
    "                # val_CCC_epoch.append(CCC(y_pred_unscaled, y_unscaled).item())\n",
    "                # val_R2_epoch.append(R2(y_pred_unscaled, y_unscaled).item())\n",
    "\n",
    "            mean_train_loss = np.mean(train_loss_epoch)\n",
    "            # mean_train_MSE = np.mean(train_MSE_epoch)\n",
    "            mean_train_MAPE = np.mean(train_MAPE_epoch)\n",
    "            # mean_train_R2 = np.mean(train_R2_epoch)\n",
    "            # mean_train_CCC = np.mean(train_CCC_epoch)\n",
    "\n",
    "            mean_val_loss = np.mean(val_loss_epoch)\n",
    "            # mean_val_MSE = np.mean(val_MSE_epoch)\n",
    "            mean_val_MAPE = np.mean(val_MAPE_epoch)\n",
    "            # mean_val_R2 = np.mean(val_R2_epoch)\n",
    "            # mean_val_CCC = np.mean(val_CCC_epoch)\n",
    "\n",
    "            # Reference structure score\n",
    "            x, y, _, _, _ = ref\n",
    "            x = x_scaler.inverse_transform(x.to(device))\n",
    "            y_pred = model(x)\n",
    "            y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "            reference_MAPE = MAPE(y_pred, y.to(device))\n",
    "\n",
    "            # Logging\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": mean_train_loss,\n",
    "                # \"train_mse\": mean_train_MSE,\n",
    "                \"train_mape\": mean_train_MAPE,\n",
    "                # \"train_r2\": mean_train_R2,\n",
    "                # \"train_ccc\": mean_train_CCC,\n",
    "\n",
    "                \"val_loss\": mean_val_loss,\n",
    "                # \"val_mse\": mean_val_MSE,\n",
    "                \"val_mape\": mean_val_MAPE,\n",
    "                # \"val_r2\": mean_val_R2,\n",
    "                # \"val_ccc\": mean_val_CCC,\n",
    "\n",
    "                \"reference_MAPE\": reference_MAPE,\n",
    "            }, step=epoch)\n",
    "\n",
    "            train_losses.append(mean_train_loss)\n",
    "            val_losses.append(mean_val_loss)\n",
    "            # train_MSE.append(mean_train_MSE)\n",
    "            # val_MSE.append(mean_val_MSE)\n",
    "            train_MAPE.append(mean_train_MAPE)\n",
    "            val_MAPE.append(mean_val_MAPE)\n",
    "            # train_R2.append(mean_train_R2)\n",
    "            # val_R2.append(mean_val_R2)\n",
    "            # train_CCC.append(mean_train_CCC)\n",
    "            # val_CCC.append(mean_val_CCC)\n",
    "\n",
    "            if (epoch + 1) % 100 == 0 and verbose:\n",
    "                print(f\"[Epoch] {epoch + 1:{len(str(N_EPOCHS))}d}/{N_EPOCHS:d}\", end='  ')\n",
    "                print(f\"TRAIN\", end='   ')\n",
    "                print(f\"Loss: {mean_train_loss:1.4f}\", end='   ')\n",
    "                # print(f\"MSE: {mean_train_MSE:1.4e}\", end='   ')\n",
    "                print(f\"MAPE: {mean_train_MAPE:1.4f}\", end='   ')\n",
    "                # print(f\"R2: {mean_train_R2: 1.4f}\", end='   ')\n",
    "                # print(f\"CCC: {mean_train_CCC: 1.4f}\", end='')\n",
    "                print(\" ## \", end='')\n",
    "                print(f\"VALIDATION\", end='   ')\n",
    "                print(f\"Loss: {mean_val_loss:1.4f}\", end='   ')\n",
    "                # print(f\"MSE: {mean_val_MSE:1.4e}\", end='   ')\n",
    "                print(f\"MAPE: {mean_val_MAPE:1.4f}\", end='   ')\n",
    "                # print(f\"R2: {mean_val_R2: 1.4f}\", end='   ')\n",
    "                # print(f\"CCC: {mean_val_CCC: 1.4f}\")"
   ],
   "id": "fde930edda682760",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "print(\"Start\")\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.WARNING)\n",
    "\n",
    "for i in range(0, len(train_ds)):\n",
    "    idx = np.linspace(0, len(train_ds)-1, i+1, dtype=int)\n",
    "    finetune_no_pinn(Subset(train_ds, idx), val_ds, i+1, n_epoch=100, verbose=False)"
   ],
   "id": "88b5ecd5e1864468",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### With PINN",
   "id": "bf331aa03af2263a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bca8c4bd40d754ff",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
