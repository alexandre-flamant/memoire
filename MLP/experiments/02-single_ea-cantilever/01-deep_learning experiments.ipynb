{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T17:10:15.081065Z",
     "start_time": "2025-02-12T17:10:15.077233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pwd\n",
    "%cd ../.."
   ],
   "id": "58fd7167621b4c64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aflamant/Documents/courses/2024-2025/meÃÅmoire/03-code/memoire/MLP\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T17:10:16.948625Z",
     "start_time": "2025-02-12T17:10:15.489297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RANDOM_STATE = 42\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (d2_absolute_error_score as D2,\n",
    "                             r2_score as R2,\n",
    "                             mean_absolute_percentage_error as MAPE)\n",
    "\n",
    "from dataset import TenBarsCantileverTrussSingleEADataset\n",
    "\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from models.architecture import MultiLayerPerceptron\n",
    "from models.processing import StandardScaler\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")"
   ],
   "id": "f073667fcbebe50c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load the data\n",
   "id": "6567808ced4fe2b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T17:10:20.530969Z",
     "start_time": "2025-02-12T17:10:17.611515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = \"./data/dataset/cantilever/data.hdf5\"\n",
    "_ds = TenBarsCantileverTrussSingleEADataset(data_path)\n",
    "\n",
    "ds = _ds[np.random.choice(np.arange(len(_ds)), 25000, replace=False)]\n",
    "in_dim = ds[0][0].__len__()\n",
    "out_dim = ds[0][1].__len__()\n",
    "\n",
    "print(f\"Dataset size: {len(ds)}\")\n",
    "print(f\"  Sample dimension: {in_dim}\")\n",
    "print(f\"  Target dimension: {out_dim}\")"
   ],
   "id": "5502a7293587797",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 25000\n",
      "  Sample dimension: 31\n",
      "  Target dimension: 1\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Training and Validation routine",
   "id": "93760e7333b6f0e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T21:01:43.185497Z",
     "start_time": "2025-02-12T21:01:43.170872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, train_ds, val_ds, lr, n_epochs, batch_size, verbose=True, plot=False):\n",
    "    model = model.to(device)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    x_scaler = StandardScaler(in_dim).to(device)\n",
    "    y_scaler = StandardScaler(out_dim).to(device)\n",
    "    for x, y, _, _, _ in train_dl:\n",
    "        x_scaler.partial_fit(x.to(device))\n",
    "        y_scaler.partial_fit(y.to(device))\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_MSE = []\n",
    "    val_MSE = []\n",
    "    train_MAPE = []\n",
    "    val_MAPE = []\n",
    "    train_R2 = []\n",
    "    val_R2 = []\n",
    "    train_D2 = []\n",
    "    val_D2 = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss_epoch = []\n",
    "        train_MSE_epoch = []\n",
    "        train_MAPE_epoch = []\n",
    "        train_R2_epoch = []\n",
    "        train_D2_epoch = []\n",
    "\n",
    "        for batch in train_dl:\n",
    "            x, y, _, _, _ = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            x = x_scaler.transform(x)\n",
    "            y = y_scaler.transform(y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y, y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_unscaled = y_scaler.inverse_transform(y).cpu().detach()\n",
    "            y_pred_unscaled = y_scaler.inverse_transform(y_pred).cpu().detach()\n",
    "\n",
    "            train_loss_epoch.append(loss.item())\n",
    "            train_MSE_epoch.append(F.mse_loss(y_pred, y).item())\n",
    "            train_MAPE_epoch.append(MAPE(y_unscaled, y_pred_unscaled))\n",
    "            train_D2_epoch.append(D2(y_unscaled, y_pred_unscaled))\n",
    "            train_R2_epoch.append(R2(y_unscaled, y_pred_unscaled))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_epoch = []\n",
    "        val_MSE_epoch = []\n",
    "        val_MAPE_epoch = []\n",
    "        val_R2_epoch = []\n",
    "        val_D2_epoch = []\n",
    "        for batch in val_dl:\n",
    "            x, y, _, _, _ = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            x = x_scaler.transform(x)\n",
    "            y = y_scaler.transform(y)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            y_unscaled = y_scaler.inverse_transform(y).cpu().detach()\n",
    "            y_pred_unscaled = y_scaler.inverse_transform(y_pred).cpu().detach()\n",
    "\n",
    "            val_loss_epoch.append(loss.item())\n",
    "            val_MSE_epoch.append(F.mse_loss(y_pred, y).item())\n",
    "            val_MAPE_epoch.append(MAPE(y_unscaled, y_pred_unscaled))\n",
    "            val_D2_epoch.append(D2(y_unscaled, y_pred_unscaled))\n",
    "            val_R2_epoch.append(R2(y_unscaled, y_pred_unscaled))\n",
    "\n",
    "        # Logging\n",
    "        mlflow.log_metric(\"train loss\", np.mean(train_loss_epoch), step=epoch)\n",
    "        mlflow.log_metric(\"train MSE\", np.mean(train_MSE_epoch), step=epoch)\n",
    "        mlflow.log_metric(\"train MAPE\", np.mean(train_MAPE_epoch), step=epoch)\n",
    "        mlflow.log_metric(\"train R2\", np.mean(train_R2_epoch), step=epoch)\n",
    "        mlflow.log_metric(\"train D2\", np.mean(train_D2_epoch), step=epoch)\n",
    "\n",
    "        mlflow.log_metric(\"val loss\", np.mean(val_loss_epoch), step=epoch)\n",
    "        mlflow.log_metric(\"val MSE\", np.mean(val_MSE_epoch), step=epoch)\n",
    "        mlflow.log_metric(\"val MAPE\", np.mean(val_MAPE_epoch), step=epoch)\n",
    "        mlflow.log_metric(\"val R2\", np.mean(val_R2_epoch), step=epoch)\n",
    "        mlflow.log_metric(\"val D2\", np.mean(val_D2_epoch), step=epoch)\n",
    "\n",
    "        train_losses.append(np.mean(train_loss_epoch))\n",
    "        val_losses.append(np.mean(val_loss_epoch))\n",
    "        train_MSE.append(np.mean(train_MSE_epoch))\n",
    "        val_MSE.append(np.mean(val_MSE_epoch))\n",
    "        train_MAPE.append(np.mean(train_MAPE_epoch))\n",
    "        val_MAPE.append(np.mean(val_MAPE_epoch))\n",
    "        train_R2.append(np.mean(train_R2_epoch))\n",
    "        val_R2.append(np.mean(val_R2_epoch))\n",
    "        train_D2.append(np.mean(train_D2_epoch))\n",
    "        val_D2.append(np.mean(val_D2_epoch))\n",
    "\n",
    "        if verbose and (epoch + 1) % 25 == 0:\n",
    "            print(f\"[Epoch] {epoch + 1:{len(str(n_epochs))}d}/{n_epochs:d}\", end='  ')\n",
    "            print(f\"TRAIN\", end='   ')\n",
    "            print(f\"Loss: {np.mean(train_loss_epoch):1.4f}\", end='   ')\n",
    "            print(f\"MSE: {np.mean(train_MSE_epoch):1.4f}\", end='   ')\n",
    "            print(f\"MAPE: {np.mean(train_MAPE_epoch):1.4f}\", end='   ')\n",
    "            print(f\"R2: {np.mean(train_R2_epoch): 1.4f}\", end='   ')\n",
    "            print(f\"D2: {np.mean(train_D2_epoch): 1.4f}\", end='')\n",
    "            print(\"  ||  \", end='')\n",
    "            print(f\"VALIDATION\", end='   ')\n",
    "            print(f\"Loss: {np.mean(val_loss_epoch):1.4f}\", end='   ')\n",
    "            print(f\"MSE: {np.mean(val_MSE_epoch):1.4f}\", end='   ')\n",
    "            print(f\"MAPE: {np.mean(val_MAPE_epoch):1.4f}\", end='   ')\n",
    "            print(f\"R2: {np.mean(val_R2_epoch): 1.4f}\", end='   ')\n",
    "            print(f\"D2: {np.mean(val_D2_epoch): 1.4f}\")\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(24, 8))\n",
    "        axs[0].set_title(\"Loss\")\n",
    "        axs[0].plot(train_losses, label='Training')\n",
    "        axs[0].plot(val_losses, label='Validation')\n",
    "        axs[0].set_yscale('log')\n",
    "        axs[3].set_xlabel(\"Epoch\")\n",
    "        axs[3].legend()\n",
    "\n",
    "        axs[1].set_title(\"MAPE\")\n",
    "        axs[1].plot(train_MAPE, label='Training')\n",
    "        axs[1].plot(val_MAPE, label='Validation')\n",
    "        axs[1].set_yscale('log')\n",
    "        axs[3].set_xlabel(\"Epoch\")\n",
    "        axs[3].legend()\n",
    "\n",
    "        axs[2].set_title(\"R2\")\n",
    "        axs[2].plot(train_R2, label='Training')\n",
    "        axs[2].plot(val_R2, label='Validation')\n",
    "        axs[2].set_yscale('function', functions=(lambda x: 10 ** x, lambda x: np.log10(x)))\n",
    "        axs[2].set_ylim(0, 1.0)\n",
    "        axs[3].set_xlabel(\"Epoch\")\n",
    "        axs[3].legend()\n",
    "\n",
    "        axs[3].set_title(\"D2\")\n",
    "        axs[3].plot(train_D2, label='Training')\n",
    "        axs[3].plot(val_D2, label='Validation')\n",
    "        axs[3].set_yscale('function', functions=(lambda x: 10 ** x, lambda x: np.log10(x)))\n",
    "        axs[3].set_ylim(0, 1.0)\n",
    "        axs[3].set_xlabel(\"Epoch\")\n",
    "        axs[3].legend()\n",
    "\n",
    "\n",
    "    signature = mlflow.models.infer_signature(x.cpu().detach().numpy(), model(x).cpu().detach().numpy())\n",
    "    model_info = mlflow.pytorch.log_model(\n",
    "        pytorch_model=model,\n",
    "        input_example=x.cpu().detach().numpy(),\n",
    "        artifact_path='model',\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    # mlflow.set_tag(\"motivation\", \"First test on MLFlow\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_MSE': train_MSE,\n",
    "        'val_MSE': val_MSE,\n",
    "        'train_MAPE': train_MAPE,\n",
    "        'val_MAPE': val_MAPE,\n",
    "        'train_R2': train_R2,\n",
    "        'val_R2': val_R2,\n",
    "        'train_D2': train_D2,\n",
    "        'val_D2': val_D2,\n",
    "    }"
   ],
   "id": "48a74fff01e9b052",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T21:02:13.757454Z",
     "start_time": "2025-02-12T21:01:58.216100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "#mlflow.set_experiment(\"deep_learning\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"seventh test\"):\n",
    "    ds_1, ds_2 = random_split(ds, (.8, .2))\n",
    "    results = train(model=MultiLayerPerceptron(in_dim, out_dim,\n",
    "                                               40, 3,\n",
    "                                               nn.ReLU),\n",
    "                    train_ds=ds_1,\n",
    "                    lr=4e-4,\n",
    "                    val_ds=ds_2,\n",
    "                    n_epochs=200,\n",
    "                    batch_size=2048,\n",
    "                    verbose=True)"
   ],
   "id": "74d5a1e1be494fe1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch]  25/200  TRAIN   Loss: 0.1326   MSE: 0.1326   MAPE: 0.2717   R2:  0.8675   D2:  0.7059  ||  VALIDATION   Loss: 0.1184   MSE: 0.1184   MAPE: 0.2736   R2:  0.8817   D2:  0.7171\n",
      "[Epoch]  50/200  TRAIN   Loss: 0.0661   MSE: 0.0661   MAPE: 0.1714   R2:  0.9338   D2:  0.8140  ||  VALIDATION   Loss: 0.0593   MSE: 0.0593   MAPE: 0.1785   R2:  0.9412   D2:  0.8234\n",
      "[Epoch]  75/200  TRAIN   Loss: 0.0436   MSE: 0.0436   MAPE: 0.1248   R2:  0.9564   D2:  0.8647  ||  VALIDATION   Loss: 0.0418   MSE: 0.0418   MAPE: 0.1322   R2:  0.9590   D2:  0.8701\n",
      "[Epoch] 100/200  TRAIN   Loss: 0.0325   MSE: 0.0325   MAPE: 0.0977   R2:  0.9675   D2:  0.8902  ||  VALIDATION   Loss: 0.0299   MSE: 0.0299   MAPE: 0.1032   R2:  0.9701   D2:  0.8935\n",
      "[Epoch] 125/200  TRAIN   Loss: 0.0257   MSE: 0.0257   MAPE: 0.0816   R2:  0.9743   D2:  0.9066  ||  VALIDATION   Loss: 0.0257   MSE: 0.0257   MAPE: 0.0846   R2:  0.9747   D2:  0.9097\n",
      "[Epoch] 150/200  TRAIN   Loss: 0.0215   MSE: 0.0215   MAPE: 0.0734   R2:  0.9785   D2:  0.9158  ||  VALIDATION   Loss: 0.0219   MSE: 0.0219   MAPE: 0.0780   R2:  0.9781   D2:  0.9178\n",
      "[Epoch] 175/200  TRAIN   Loss: 0.0184   MSE: 0.0184   MAPE: 0.0701   R2:  0.9816   D2:  0.9229  ||  VALIDATION   Loss: 0.0191   MSE: 0.0191   MAPE: 0.0714   R2:  0.9811   D2:  0.9238\n",
      "[Epoch] 200/200  TRAIN   Loss: 0.0172   MSE: 0.0172   MAPE: 0.0677   R2:  0.9827   D2:  0.9243  ||  VALIDATION   Loss: 0.0168   MSE: 0.0168   MAPE: 0.0760   R2:  0.9833   D2:  0.9250\n",
      "üèÉ View run seventh test at: http://127.0.0.1:5000/#/experiments/375959947083724615/runs/46063cbbda45466680379fd032078dfb\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/375959947083724615\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " # 2. Hyperparameter tuning\n",
    "Considering a 10 bar cantilever dataset we want to predict the EA of the bars which is assumed to be a single common value.\n",
    "The model is an MLP here are the parameters:\n",
    "- Activation function\n",
    "- Learning rate\n",
    "- Number of layers\n",
    "- Number of neurons per layer"
   ],
   "id": "180279513a44999"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(\"MLP_Cantilever_Capacity\")\n",
    "\n",
    "# Model capacity tuning\n",
    "n_neurons_values = [10, 15, 20, 25, 30, 35, 40]\n",
    "n_layers_values = [1, 2, 3, 4, 5]\n",
    "\n",
    "outer_cv = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "outer_configs = []\n",
    "outer_scores = []\n",
    "for outer_fold_id, (train_idx, test_idx) in enumerate(outer_cv.split(_ds)):\n",
    "    train_ds, test_ds = Subset(_ds, train_idx), Subset(_ds, test_idx)\n",
    "\n",
    "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE + outer_fold_id)\n",
    "    inner_scores = {}\n",
    "    for n_neurons in n_neurons_values:\n",
    "        for n_layers in n_layers_values:\n",
    "            inner_scores.setdefault((n_layers, n_neurons), [])\n",
    "            for inner_fold_id, (train_idx, val_idx) in inner_cv.split(train_ds):\n",
    "                train_ds, val_ds = Subset(train_ds, train_idx), Subset(train_ds, val_idx)\n",
    "\n",
    "                model = MultiLayerPerceptron(in_dim, out_dim,\n",
    "                                             n_layers, n_neurons,\n",
    "                                             nn.ReLU)\n",
    "\n",
    "                train(model, train_ds, val_ds)\n",
    "\n",
    "                inner_scores[(n_layers, n_neurons)].append(score)\n",
    "\n",
    "    best_score = np.inf\n",
    "    best_config = None\n",
    "    for config, scores in inner_scores.items():\n",
    "        if scores < best_score:\n",
    "            best_score = scores\n",
    "            best_config = config\n",
    "\n",
    "    model = MultiLayerPerceptron(in_dim, out_dim,\n",
    "                                 best_config[0], best_config[1],\n",
    "                                 nn.ReLU)\n",
    "\n",
    "    scores = train(model, train_ds, test_ds)\n",
    "\n",
    "    outer_configs.append(best_config)\n",
    "    outer_scores.append(score)"
   ],
   "id": "bc2d7655211f1194",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Training the foundation model\n",
    "We will train our model with the whole dataset to create a foundation model that will have learnt all the specifics of the problem."
   ],
   "id": "5f0fd2f63ce01e6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4844ba1bf8bede59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Prediction on real data\n",
    "We will use as *real* data, data with shared multiplicative noise:\n",
    "$$\\varepsilon \\sim \\mathcal N \\left( \\mu = 1, \\sigma = 0.0025 \\right)$$\n",
    "\n",
    "Such that $\\hat x = x * \\varepsilon$ has 95% chance of being within +- 0.5% of the true value. Which is the same order of magnitude observed with HBM sensors.\n",
    "\n",
    "This noise will be applied to a set of data from which a subset will be extracted for fine-tuning."
   ],
   "id": "3d54db98c5c73fbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## a. Non fine-tuned prediction\n",
    "Scores using the foundation model for prediction"
   ],
   "id": "29dadd240cc35cd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5d4bf08b6103e02e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "z## b. Fine-tuned model\n",
    "We will finetune the foundation model using the subset of real data as input\n"
   ],
   "id": "5dd04240f84aaa01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f19781d1424e6ab8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### I. Experiment on the size of fine-tuning set\n",
    "These experiments will help us define how many real example are needed for *sufficient* fine-tuning."
   ],
   "id": "a5058d47065dfffa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Without PINN",
   "id": "2ad2ad24581a628f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fde930edda682760",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### With PINN",
   "id": "bf331aa03af2263a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bca8c4bd40d754ff",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
